{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TaPS: Task Performance Suite","text":"<p>:warning: This repository is currently under development.</p> <p>TaPS is a standardized framework for evaluating task-based execution frameworks and data management systems using a suite a real and synthetic scientific applications.</p>"},{"location":"get-started/","title":"Quick Start","text":"<p>The Workflow Execution Benchmark Suite (WEBS) provides a set of standard computational workflows that can be executed with a variety of execution engines.</p>"},{"location":"get-started/#installation","title":"Installation","text":"<pre><code>git clone https://github.com/proxystore/taps\ncd taps\npython -m venv venv\n. venv/bin/activate\npip install -e .\n</code></pre> <p>Documentation on installing for local development is provided in Contributing.</p>"},{"location":"get-started/#usage","title":"Usage","text":"<pre><code>python -m webs.run {workflow-name} {args}\n</code></pre>"},{"location":"api/","title":"webs","text":"<code>webs/__init__.py</code> <p>Workflows benchmark package.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>webs</li> <li>webs.config</li> <li>webs.context</li> <li>webs.data<ul> <li>config</li> <li>file</li> <li>filter</li> <li>null</li> <li>proxy</li> <li>transform</li> </ul> </li> <li>webs.executor<ul> <li>config</li> <li>dag</li> <li>dask</li> <li>globus</li> <li>parsl</li> <li>python</li> <li>ray</li> <li>workflow</li> </ul> </li> <li>webs.logging</li> <li>webs.record</li> <li>webs.run<ul> <li>config</li> <li>main</li> </ul> </li> <li>webs.wf<ul> <li>cholesky<ul> <li>config</li> <li>workflow</li> </ul> </li> <li>docking<ul> <li>config</li> <li>train</li> <li>workflow</li> </ul> </li> <li>fedlearn<ul> <li>config</li> <li>modules</li> <li>tasks</li> <li>types</li> <li>utils</li> <li>workflow</li> </ul> </li> <li>mapreduce<ul> <li>config</li> <li>utils</li> <li>workflow</li> </ul> </li> <li>moldesign<ul> <li>chemfunctions</li> <li>config</li> <li>tasks</li> <li>workflow</li> </ul> </li> <li>montage<ul> <li>config</li> <li>workflow</li> </ul> </li> <li>synthetic<ul> <li>config</li> <li>utils</li> <li>workflow</li> </ul> </li> </ul> </li> <li>webs.workflow</li> </ul>"},{"location":"api/config/","title":"webs.config","text":"<code>webs/config.py</code>"},{"location":"api/config/#webs.config.Config","title":"Config","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base configuration model type.</p>"},{"location":"api/config/#webs.config.Config.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/context/","title":"webs.context","text":"<code>webs/context.py</code>"},{"location":"api/context/#webs.context.ContextManagerAddIn","title":"ContextManagerAddIn","text":"<pre><code>ContextManagerAddIn(\n    managers: (\n        Sequence[AbstractContextManager[Any] | None] | None\n    ) = None,\n)\n</code></pre> <p>Context manager add in class.</p> Source code in <code>webs/context.py</code> <pre><code>def __init__(\n    self,\n    managers: Sequence[AbstractContextManager[Any] | None] | None = None,\n) -&gt; None:\n    self._managers = [] if managers is None else managers\n</code></pre>"},{"location":"api/logging/","title":"webs.logging","text":"<code>webs/logging.py</code>"},{"location":"api/logging/#webs.logging.init_logging","title":"init_logging()","text":"<pre><code>init_logging(\n    logfile: Path | None = None,\n    level: int | str = logging.INFO,\n    logfile_level: int | str = logging.INFO,\n    force: bool = False,\n) -&gt; None\n</code></pre> <p>Initialize logging with custom formats.</p> <p>Adds a custom log levels RUN and WORK which are higher than INFO and lower than WARNING. RUN is used by the workflow benchmark harness and WORK is using within the workflows.</p> Usage <p>logger = init_logger(...) logger.log(RUN_LOG_LEVEL, 'message')</p> <p>Parameters:</p> <ul> <li> <code>logfile</code>             (<code>str</code>, default:                 <code>None</code> )         \u2013          <p>option filepath to write log to (default: None).</p> </li> <li> <code>level</code>             (<code>(int, str)</code>, default:                 <code>INFO</code> )         \u2013          <p>minimum logging level (default: INFO).</p> </li> <li> <code>logfile_level</code>             (<code>(int, str)</code>, default:                 <code>INFO</code> )         \u2013          <p>minimum logging level for the logfile (default: INFO).</p> </li> <li> <code>force</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>remove any existing handlers attached to the root handler. This option is useful to silencing the third-party package logging. Note: should not be set when running inside pytest (default: False).</p> </li> </ul> Source code in <code>webs/logging.py</code> <pre><code>def init_logging(\n    logfile: pathlib.Path | None = None,\n    level: int | str = logging.INFO,\n    logfile_level: int | str = logging.INFO,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"Initialize logging with custom formats.\n\n    Adds a custom log levels RUN and WORK which are higher than INFO and\n    lower than WARNING. RUN is used by the workflow benchmark harness\n    and WORK is using within the workflows.\n\n    Usage:\n        &gt;&gt;&gt; logger = init_logger(...)\n        &gt;&gt;&gt; logger.log(RUN_LOG_LEVEL, 'message')\n\n    Args:\n        logfile (str): option filepath to write log to (default: None).\n        level (int, str): minimum logging level (default: INFO).\n        logfile_level (int, str): minimum logging level for the logfile\n            (default: INFO).\n        force (bool): remove any existing handlers attached to the root\n            handler. This option is useful to silencing the third-party\n            package logging. Note: should not be set when running inside\n            pytest (default: False).\n    \"\"\"\n    logging.addLevelName(RUN_LOG_LEVEL, 'RUN')\n    logging.addLevelName(WORK_LOG_LEVEL, 'WORK')\n\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setLevel(level)\n\n    handlers: list[logging.Handler] = [stdout_handler]\n    if logfile is not None:\n        logfile.parent.mkdir(parents=True, exist_ok=True)\n        handler = logging.FileHandler(logfile)\n        handler.setLevel(logfile_level)\n        handlers.append(handler)\n\n    kwargs: dict[str, Any] = {}\n    if force:  # pragma: no cover\n        kwargs['force'] = force\n\n    logging.basicConfig(\n        format=(\n            '[%(asctime)s.%(msecs)03d] %(levelname)-5s (%(name)s) :: '\n            '%(message)s'\n        ),\n        datefmt='%Y-%m-%d %H:%M:%S',\n        level=logging.DEBUG,\n        handlers=handlers,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/record/","title":"webs.record","text":"<code>webs/record.py</code>"},{"location":"api/record/#webs.record.Record","title":"Record  <code>module-attribute</code>","text":"<pre><code>Record: TypeAlias = Dict[str, Any]\n</code></pre> <p>Record type.</p>"},{"location":"api/record/#webs.record.RecordLogger","title":"RecordLogger","text":"<p>             Bases: <code>Protocol</code></p> <p>Record logger protocol.</p>"},{"location":"api/record/#webs.record.RecordLogger.log","title":"log()","text":"<pre><code>log(record: Record) -&gt; None\n</code></pre> <p>Log a record.</p> Source code in <code>webs/record.py</code> <pre><code>def log(self, record: Record) -&gt; None:\n    \"\"\"Log a record.\"\"\"\n    ...\n</code></pre>"},{"location":"api/record/#webs.record.RecordLogger.close","title":"close()","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the logger.</p> Source code in <code>webs/record.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the logger.\"\"\"\n    ...\n</code></pre>"},{"location":"api/record/#webs.record.JSONRecordLogger","title":"JSONRecordLogger","text":"<pre><code>JSONRecordLogger(filepath: Path | str)\n</code></pre> <p>JSON lines record logger.</p> <p>Logs records as JSON strings per line to a file.</p> <p>Parameters:</p> <ul> <li> <code>filepath</code>             (<code>Path | str</code>)         \u2013          <p>Filepath to log to.</p> </li> </ul> Source code in <code>webs/record.py</code> <pre><code>def __init__(self, filepath: pathlib.Path | str) -&gt; None:\n    self._filepath = pathlib.Path(filepath)\n    self._handle = open(self._filepath, 'a')  # noqa: SIM115\n</code></pre>"},{"location":"api/record/#webs.record.JSONRecordLogger.log","title":"log()","text":"<pre><code>log(record: Record) -&gt; None\n</code></pre> <p>Log a record.</p> Source code in <code>webs/record.py</code> <pre><code>def log(self, record: Record) -&gt; None:\n    \"\"\"Log a record.\"\"\"\n    self._handle.write(json.dumps(record) + '\\n')\n</code></pre>"},{"location":"api/record/#webs.record.JSONRecordLogger.close","title":"close()","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the logger.</p> Source code in <code>webs/record.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the logger.\"\"\"\n    self._handle.close()\n</code></pre>"},{"location":"api/record/#webs.record.NullRecordLogger","title":"NullRecordLogger","text":"<p>Null/no-op record logger.</p>"},{"location":"api/record/#webs.record.NullRecordLogger.log","title":"log()","text":"<pre><code>log(record: Record) -&gt; None\n</code></pre> <p>Log a record.</p> Source code in <code>webs/record.py</code> <pre><code>def log(self, record: Record) -&gt; None:\n    \"\"\"Log a record.\"\"\"\n    return\n</code></pre>"},{"location":"api/record/#webs.record.NullRecordLogger.close","title":"close()","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the logger.</p> Source code in <code>webs/record.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the logger.\"\"\"\n    return\n</code></pre>"},{"location":"api/workflow/","title":"webs.workflow","text":"<code>webs/workflow.py</code>"},{"location":"api/workflow/#webs.workflow.Workflow","title":"Workflow","text":"<p>             Bases: <code>Protocol[WorkflowConfigT]</code></p> <p>Workflow protocol.</p> <p>Attributes:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the workflow.</p> </li> <li> <code>config_type</code>             (<code>type[WorkflowConfigT]</code>)         \u2013          <p>Workflow configuration type.</p> </li> </ul>"},{"location":"api/workflow/#webs.workflow.Workflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: WorkflowConfigT) -&gt; Self\n</code></pre> <p>Initialize a workflow instance from a config.</p> Source code in <code>webs/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: WorkflowConfigT) -&gt; Self:\n    \"\"\"Initialize a workflow instance from a config.\"\"\"\n    ...\n</code></pre>"},{"location":"api/workflow/#webs.workflow.Workflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> Source code in <code>webs/workflow.py</code> <pre><code>def run(\n    self,\n    executor: WorkflowExecutor,\n    run_dir: pathlib.Path,\n) -&gt; None:\n    \"\"\"Run the workflow.\"\"\"\n    ...\n</code></pre>"},{"location":"api/workflow/#webs.workflow.get_registered_workflow_names","title":"get_registered_workflow_names()","text":"<pre><code>get_registered_workflow_names() -&gt; tuple[str, ...]\n</code></pre> <p>Get the names of all registered workflows.</p> Source code in <code>webs/workflow.py</code> <pre><code>def get_registered_workflow_names() -&gt; tuple[str, ...]:\n    \"\"\"Get the names of all registered workflows.\"\"\"\n    return tuple(REGISTERED_WORKFLOWS.keys())\n</code></pre>"},{"location":"api/workflow/#webs.workflow.get_registered_workflow","title":"get_registered_workflow()","text":"<pre><code>get_registered_workflow(name: str) -&gt; type[Workflow[Any]]\n</code></pre> <p>Get a workflow implementation by name.</p> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the registered workflow.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type[Workflow[Any]]</code>         \u2013          <p>The <code>Workflow</code> implementation type.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>KeyError</code>           \u2013          <p>If <code>name</code> is not known.</p> </li> <li> <code>ImportError</code>           \u2013          <p>If the workflow cannot be imported.</p> </li> </ul> Source code in <code>webs/workflow.py</code> <pre><code>def get_registered_workflow(name: str) -&gt; type[Workflow[Any]]:\n    \"\"\"Get a workflow implementation by name.\n\n    Args:\n        name: Name of the registered workflow.\n\n    Returns:\n        The [`Workflow`][webs.workflow.Workflow] implementation type.\n\n    Raises:\n        KeyError: If `name` is not known.\n        ImportError: If the workflow cannot be imported.\n    \"\"\"\n    try:\n        path = REGISTERED_WORKFLOWS[name]\n    except KeyError as e:\n        raise KeyError(\n            f'A workflow named \"{name}\" has not been registered.',\n        ) from e\n\n    module_path, _, class_name = path.rpartition('.')\n\n    try:\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError) as e:\n        raise ImportError(\n            f'Failed to load the \"{name}\" workflow. (Tried to load '\n            f'{class_name} from {module_path}.)\\n\\n'\n            'If the above error is because another dependency was not found, '\n            'check the documentation for the specific workflow '\n            'for installation instructions or try reinstalling the package '\n            'with the corresponding extras option.\\n'\n            f'  $ pip install .[{name}]',\n        ) from e\n</code></pre>"},{"location":"api/data/","title":"webs.data","text":"<code>webs/data/__init__.py</code>"},{"location":"api/data/config/","title":"webs.data.config","text":"<code>webs/data/config.py</code>"},{"location":"api/data/config/#webs.data.config.FilterConfig","title":"FilterConfig","text":"<p>             Bases: <code>Config</code></p> <p>Filter configuration.</p>"},{"location":"api/data/config/#webs.data.config.FilterConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/data/config/#webs.data.config.FilterConfig.get_filter","title":"get_filter()","text":"<pre><code>get_filter() -&gt; Filter\n</code></pre> <p>Get a filter according to the config.</p> Source code in <code>webs/data/config.py</code> <pre><code>def get_filter(self) -&gt; Filter:\n    \"\"\"Get a filter according to the config.\"\"\"\n    if self.filter_type is None:\n        return NullFilter()\n    elif self.filter_type == 'object-size':\n        return ObjectSizeFilter(\n            min_bytes=self.filter_min_size,\n            max_bytes=self.filter_max_size,\n        )\n    elif self.filter_type == 'pickle-size':\n        return PickleSizeFilter(\n            min_bytes=self.filter_min_size,\n            max_bytes=self.filter_max_size,\n        )\n    else:\n        raise AssertionError(f'Unknown filter type {self.filter_type}.')\n</code></pre>"},{"location":"api/data/config/#webs.data.config.TransformerChoicesConfig","title":"TransformerChoicesConfig","text":"<p>             Bases: <code>Config</code></p> <p>Transformer choice configuration.</p>"},{"location":"api/data/config/#webs.data.config.TransformerChoicesConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/data/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    configs = get_registered()\n\n    group = parser.add_argument_group(cls.__name__)\n    group.add_argument(\n        '--transformer',\n        choices=sorted(configs.keys()),\n        default='null',\n        help='executor to use',\n    )\n\n    executor_type: str | None = None\n    if argv is not None and '--executor' in argv:  # pragma: no cover\n        executor_type = argv[argv.index('--executor') + 1]\n\n    for name, config_type in configs.items():\n        config_type.add_argument_group(\n            parser,\n            argv=argv,\n            required=name == executor_type,\n        )\n</code></pre>"},{"location":"api/data/file/","title":"webs.data.file","text":"<code>webs/data/file.py</code>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformerConfig","title":"PickleFileTransformerConfig","text":"<p>             Bases: <code>TransformerConfig</code></p> <p>Pickle file transformer config.</p>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformerConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformerConfig.get_transformer","title":"get_transformer()","text":"<pre><code>get_transformer() -&gt; PickleFileTransformer\n</code></pre> <p>Create a transformer instance from the config.</p> Source code in <code>webs/data/file.py</code> <pre><code>def get_transformer(self) -&gt; PickleFileTransformer:\n    \"\"\"Create a transformer instance from the config.\"\"\"\n    return PickleFileTransformer(self.file_dir)\n</code></pre>"},{"location":"api/data/file/#webs.data.file.Identifier","title":"Identifier","text":"<p>             Bases: <code>NamedTuple</code></p> <p>Object identifier.</p> <p>Attributes:</p> <ul> <li> <code>cache_dir</code>             (<code>Path</code>)         \u2013          <p>Object directory.</p> </li> <li> <code>obj_id</code>             (<code>UUID</code>)         \u2013          <p>Object ID.</p> </li> </ul>"},{"location":"api/data/file/#webs.data.file.Identifier.path","title":"path()","text":"<pre><code>path() -&gt; Path\n</code></pre> <p>Get path to the object.</p> Source code in <code>webs/data/file.py</code> <pre><code>def path(self) -&gt; pathlib.Path:\n    \"\"\"Get path to the object.\"\"\"\n    return self.cache_dir / str(self.obj_id)\n</code></pre>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformer","title":"PickleFileTransformer","text":"<pre><code>PickleFileTransformer(cache_dir: Path | str)\n</code></pre> <p>Pickle file object transformer.</p> <p>Parameters:</p> <ul> <li> <code>cache_dir</code>             (<code>Path | str</code>)         \u2013          <p>Directory to store pickled objects in.</p> </li> </ul> Source code in <code>webs/data/file.py</code> <pre><code>def __init__(\n    self,\n    cache_dir: pathlib.Path | str,\n) -&gt; None:\n    self.cache_dir = pathlib.Path(cache_dir).resolve()\n</code></pre>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformer.is_identifier","title":"is_identifier()","text":"<pre><code>is_identifier(obj: Any) -&gt; bool\n</code></pre> <p>Check if the object is an identifier instance.</p> Source code in <code>webs/data/file.py</code> <pre><code>def is_identifier(self, obj: Any) -&gt; bool:\n    \"\"\"Check if the object is an identifier instance.\"\"\"\n    return isinstance(obj, Identifier)\n</code></pre>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformer.transform","title":"transform()","text":"<pre><code>transform(obj: T) -&gt; Identifier\n</code></pre> <p>Transform the object into an identifier.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>             (<code>T</code>)         \u2013          <p>Object to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Identifier</code>         \u2013          <p>Identifier object that can be used to resolve <code>obj</code>.</p> </li> </ul> Source code in <code>webs/data/file.py</code> <pre><code>def transform(self, obj: T) -&gt; Identifier:\n    \"\"\"Transform the object into an identifier.\n\n    Args:\n        obj: Object to transform.\n\n    Returns:\n        Identifier object that can be used to resolve `obj`.\n    \"\"\"\n    identifier = Identifier(self.cache_dir, uuid.uuid4())\n    filepath = identifier.path()\n    filepath.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(filepath, 'wb', buffering=0) as f:\n        pickle.dump(obj, f)\n\n    return identifier\n</code></pre>"},{"location":"api/data/file/#webs.data.file.PickleFileTransformer.resolve","title":"resolve()","text":"<pre><code>resolve(identifier: Identifier) -&gt; Any\n</code></pre> <p>Resolve an object from an identifier.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>             (<code>Identifier</code>)         \u2013          <p>Identifier to an object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>         \u2013          <p>The resolved object.</p> </li> </ul> Source code in <code>webs/data/file.py</code> <pre><code>def resolve(self, identifier: Identifier) -&gt; Any:\n    \"\"\"Resolve an object from an identifier.\n\n    Args:\n        identifier: Identifier to an object.\n\n    Returns:\n        The resolved object.\n    \"\"\"\n    filepath = identifier.path()\n    with open(filepath, 'rb') as f:\n        obj = pickle.load(f)\n    return obj\n</code></pre>"},{"location":"api/data/filter/","title":"webs.data.filter","text":"<code>webs/data/filter.py</code>"},{"location":"api/data/filter/#webs.data.filter.Filter","title":"Filter","text":"<p>             Bases: <code>Protocol</code></p> <p>Filter protocol.</p>"},{"location":"api/data/filter/#webs.data.filter.Filter.__call__","title":"__call__()","text":"<pre><code>__call__(obj: Any) -&gt; bool\n</code></pre> <p>Check if an abject passes through the filter.</p> Source code in <code>webs/data/filter.py</code> <pre><code>def __call__(self, obj: Any) -&gt; bool:\n    \"\"\"Check if an abject passes through the filter.\"\"\"\n    ...\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.NullFilter","title":"NullFilter","text":"<p>Null filter that lets all objects pass through.</p>"},{"location":"api/data/filter/#webs.data.filter.NullFilter.__call__","title":"__call__()","text":"<pre><code>__call__(obj: Any) -&gt; bool\n</code></pre> <p>Check if an object passes through the filter.</p> Source code in <code>webs/data/filter.py</code> <pre><code>def __call__(self, obj: Any) -&gt; bool:\n    \"\"\"Check if an object passes through the filter.\"\"\"\n    return True\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.ObjectSizeFilter","title":"ObjectSizeFilter","text":"<pre><code>ObjectSizeFilter(\n    *, min_bytes: int = 0, max_bytes: float = math.inf\n)\n</code></pre> <p>Object size filter.</p> <p>Checks if the size of an object (computed using <code>sys.getsizeof()</code>) is greater than a minimum size and less than a maximum size.</p> Warning <p><code>sys.getsizeof()</code> does not count the size of objects referred to by the main object.</p> <p>Parameters:</p> <ul> <li> <code>min_bytes</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Minimum size threshold (inclusive) to pass through the filter.</p> </li> <li> <code>max_bytes</code>             (<code>float</code>, default:                 <code>inf</code> )         \u2013          <p>Maximum size threshold (inclusive) to pass through the filter.</p> </li> </ul> Source code in <code>webs/data/filter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    min_bytes: int = 0,\n    max_bytes: float = math.inf,\n) -&gt; None:\n    self.min_bytes = min_bytes\n    self.max_bytes = max_bytes\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.ObjectSizeFilter.__call__","title":"__call__()","text":"<pre><code>__call__(obj: Any) -&gt; bool\n</code></pre> <p>Check if an object passes through the filter.</p> Source code in <code>webs/data/filter.py</code> <pre><code>def __call__(self, obj: Any) -&gt; bool:\n    \"\"\"Check if an object passes through the filter.\"\"\"\n    size = sys.getsizeof(obj)\n    return self.min_bytes &lt;= size &lt;= self.max_bytes\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.ObjectTypeFilter","title":"ObjectTypeFilter","text":"<pre><code>ObjectTypeFilter(*types: type)\n</code></pre> <p>Object type filter.</p> <p>Checks if an object is of a certain type.</p> <p>Parameters:</p> <ul> <li> <code>types</code>             (<code>type</code>, default:                 <code>()</code> )         \u2013          <p>Types to check.</p> </li> </ul> Source code in <code>webs/data/filter.py</code> <pre><code>def __init__(self, *types: type) -&gt; None:\n    self.types = types\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.ObjectTypeFilter.__call__","title":"__call__()","text":"<pre><code>__call__(obj: Any) -&gt; bool\n</code></pre> <p>Check if an object passes through the filter.</p> Source code in <code>webs/data/filter.py</code> <pre><code>def __call__(self, obj: Any) -&gt; bool:\n    \"\"\"Check if an object passes through the filter.\"\"\"\n    return isinstance(obj, self.types)\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.PickleSizeFilter","title":"PickleSizeFilter","text":"<pre><code>PickleSizeFilter(\n    *, min_bytes: int = 0, max_bytes: float = math.inf\n)\n</code></pre> <p>Object size filter.</p> <p>Checks if the size of an object (computed using size of the pickled object) is greater than a minimum size and less than a maximum size.</p> Warning <p>Pickling large objects can take significant time.</p> <p>Parameters:</p> <ul> <li> <code>min_bytes</code>             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Minimum size threshold (inclusive) to pass through the filter.</p> </li> <li> <code>max_bytes</code>             (<code>float</code>, default:                 <code>inf</code> )         \u2013          <p>Maximum size threshold (inclusive) to pass through the filter.</p> </li> </ul> Source code in <code>webs/data/filter.py</code> <pre><code>def __init__(\n    self,\n    *,\n    min_bytes: int = 0,\n    max_bytes: float = math.inf,\n) -&gt; None:\n    self.min_bytes = min_bytes\n    self.max_bytes = max_bytes\n</code></pre>"},{"location":"api/data/filter/#webs.data.filter.PickleSizeFilter.__call__","title":"__call__()","text":"<pre><code>__call__(obj: Any) -&gt; bool\n</code></pre> <p>Check if an object passes through the filter.</p> Source code in <code>webs/data/filter.py</code> <pre><code>def __call__(self, obj: Any) -&gt; bool:\n    \"\"\"Check if an object passes through the filter.\"\"\"\n    size = len(pickle.dumps(obj))\n    return self.min_bytes &lt;= size &lt;= self.max_bytes\n</code></pre>"},{"location":"api/data/null/","title":"webs.data.null","text":"<code>webs/data/null.py</code>"},{"location":"api/data/null/#webs.data.null.NullTransformerConfig","title":"NullTransformerConfig","text":"<p>             Bases: <code>TransformerConfig</code></p> <p>Null transformer config.</p>"},{"location":"api/data/null/#webs.data.null.NullTransformerConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/data/null/#webs.data.null.NullTransformerConfig.get_transformer","title":"get_transformer()","text":"<pre><code>get_transformer() -&gt; NullTransformer\n</code></pre> <p>Create a transformer instance from the config.</p> Source code in <code>webs/data/null.py</code> <pre><code>def get_transformer(self) -&gt; NullTransformer:\n    \"\"\"Create a transformer instance from the config.\"\"\"\n    return NullTransformer()\n</code></pre>"},{"location":"api/data/null/#webs.data.null.NullTransformer","title":"NullTransformer","text":"<p>Null transformer that does no transformations.</p>"},{"location":"api/data/null/#webs.data.null.NullTransformer.is_identifier","title":"is_identifier()","text":"<pre><code>is_identifier(obj: Any) -&gt; bool\n</code></pre> <p>Check if the object is an identifier instance.</p> <p>Always <code>False</code> in this implementation.</p> Source code in <code>webs/data/null.py</code> <pre><code>def is_identifier(self, obj: Any) -&gt; bool:\n    \"\"\"Check if the object is an identifier instance.\n\n    Always `False` in this implementation.\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/data/null/#webs.data.null.NullTransformer.transform","title":"transform()","text":"<pre><code>transform(obj: T) -&gt; T\n</code></pre> <p>Transform the object into an identifier.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>             (<code>T</code>)         \u2013          <p>Object to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code>         \u2013          <p>Identifier object that can be usd to resolve <code>obj</code>.</p> </li> </ul> Source code in <code>webs/data/null.py</code> <pre><code>def transform(self, obj: T) -&gt; T:\n    \"\"\"Transform the object into an identifier.\n\n    Args:\n        obj: Object to transform.\n\n    Returns:\n        Identifier object that can be usd to resolve `obj`.\n    \"\"\"\n    return obj\n</code></pre>"},{"location":"api/data/null/#webs.data.null.NullTransformer.resolve","title":"resolve()","text":"<pre><code>resolve(identifier: Any) -&gt; NoReturn\n</code></pre> <p>Resolve an object from an identifier.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>             (<code>Any</code>)         \u2013          <p>Identifier to an object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>NoReturn</code>         \u2013          <p>The resolved object.</p> </li> </ul> Source code in <code>webs/data/null.py</code> <pre><code>def resolve(self, identifier: Any) -&gt; NoReturn:\n    \"\"\"Resolve an object from an identifier.\n\n    Args:\n        identifier: Identifier to an object.\n\n    Returns:\n        The resolved object.\n    \"\"\"\n    raise NotImplementedError(\n        f'{self.__class__.__name__} does not support identifiers',\n    )\n</code></pre>"},{"location":"api/data/proxy/","title":"webs.data.proxy","text":"<code>webs/data/proxy.py</code>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyFileTransformerConfig","title":"ProxyFileTransformerConfig","text":"<p>             Bases: <code>TransformerConfig</code></p> <p>Proxy file transformer config.</p>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyFileTransformerConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyFileTransformerConfig.get_transformer","title":"get_transformer()","text":"<pre><code>get_transformer() -&gt; ProxyTransformer\n</code></pre> <p>Create a transformer instance from the config.</p> Source code in <code>webs/data/proxy.py</code> <pre><code>def get_transformer(self) -&gt; ProxyTransformer:\n    \"\"\"Create a transformer instance from the config.\"\"\"\n    connector: Connector[Any]\n    if self.ps_type == 'file':\n        if self.ps_file_dir is None:  # pragma: no cover\n            raise ValueError(\n                'Option --ps-file-dir is required when --ps-type file.',\n            )\n        connector = FileConnector(self.ps_file_dir)\n    elif self.ps_type == 'redis':\n        if self.ps_redis_addr is None:\n            raise ValueError(  # pragma: no cover\n                'Option --ps-redis-addr is required when --ps-type redis.',\n            )\n        parts = self.ps_redis_addr.split(':')\n        host, port = parts[0], int(parts[1])\n        connector = RedisConnector(host, port)\n    else:\n        raise AssertionError(\n            f'Unknown proxy transformer type: {self.ps_type}.',\n        )\n\n    return ProxyTransformer(\n        store=Store(\n            'transformer',\n            connector=connector,\n            register=True,\n            populate_target=True,\n        ),\n        extract_target=self.ps_extract_target,\n    )\n</code></pre>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyTransformer","title":"ProxyTransformer","text":"<pre><code>ProxyTransformer(\n    store: Store[Any], *, extract_target: bool = False\n)\n</code></pre> <p>Proxy object transformer.</p> <p>Transforms objects into proxies which act as the identifier.</p> <p>Parameters:</p> <ul> <li> <code>store</code>             (<code>Store[Any]</code>)         \u2013          <p>Store instance to use for proxying objects.</p> </li> <li> <code>extract_target</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>When <code>True</code>, resolving an identifier (i.e., a proxy) will return the target object. Otherwise, the proxy is returned since a proxy can act as the target object.</p> </li> </ul> Source code in <code>webs/data/proxy.py</code> <pre><code>def __init__(\n    self,\n    store: Store[Any],\n    *,\n    extract_target: bool = False,\n) -&gt; None:\n    self.store = store\n    self.extract_target = extract_target\n</code></pre>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyTransformer.is_identifier","title":"is_identifier()","text":"<pre><code>is_identifier(obj: Any) -&gt; bool\n</code></pre> <p>Check if the object is an identifier instance.</p> Source code in <code>webs/data/proxy.py</code> <pre><code>def is_identifier(self, obj: Any) -&gt; bool:\n    \"\"\"Check if the object is an identifier instance.\"\"\"\n    return isinstance(obj, Proxy)\n</code></pre>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyTransformer.transform","title":"transform()","text":"<pre><code>transform(obj: T) -&gt; Proxy[T]\n</code></pre> <p>Transform the object into an identifier.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>             (<code>T</code>)         \u2013          <p>Object to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Proxy[T]</code>         \u2013          <p>Identifier object that can be used to resolve <code>obj</code>.</p> </li> </ul> Source code in <code>webs/data/proxy.py</code> <pre><code>def transform(self, obj: T) -&gt; Proxy[T]:\n    \"\"\"Transform the object into an identifier.\n\n    Args:\n        obj: Object to transform.\n\n    Returns:\n        Identifier object that can be used to resolve `obj`.\n    \"\"\"\n    return self.store.proxy(obj)\n</code></pre>"},{"location":"api/data/proxy/#webs.data.proxy.ProxyTransformer.resolve","title":"resolve()","text":"<pre><code>resolve(identifier: Proxy[T]) -&gt; T | Proxy[T]\n</code></pre> <p>Resolve an object from an identifier.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>             (<code>Proxy[T]</code>)         \u2013          <p>Identifier to an object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T | Proxy[T]</code>         \u2013          <p>The resolved object or a proxy of the resolved object depending             on the setting of <code>extract_target</code>.</p> </li> </ul> Source code in <code>webs/data/proxy.py</code> <pre><code>def resolve(self, identifier: Proxy[T]) -&gt; T | Proxy[T]:\n    \"\"\"Resolve an object from an identifier.\n\n    Args:\n        identifier: Identifier to an object.\n\n    Returns:\n        The resolved object or a proxy of the resolved object depending \\\n        on the setting of `extract_target`.\n    \"\"\"\n    return extract(identifier) if self.extract_target else identifier\n</code></pre>"},{"location":"api/data/transform/","title":"webs.data.transform","text":"<code>webs/data/transform.py</code>"},{"location":"api/data/transform/#webs.data.transform.TransformerConfig","title":"TransformerConfig","text":"<p>             Bases: <code>Config</code>, <code>ABC</code></p> <p>Data transformer configuration abstract base class.</p>"},{"location":"api/data/transform/#webs.data.transform.TransformerConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TransformerConfig.get_transformer","title":"get_transformer()  <code>abstractmethod</code>","text":"<pre><code>get_transformer() -&gt; Transformer[Any]\n</code></pre> <p>Create a transformer instance from the config.</p> Source code in <code>webs/data/transform.py</code> <pre><code>@abc.abstractmethod\ndef get_transformer(self) -&gt; Transformer[Any]:\n    \"\"\"Create a transformer instance from the config.\"\"\"\n    ...\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.Transformer","title":"Transformer","text":"<p>             Bases: <code>Protocol[IdentifierT]</code></p> <p>Object transformer protocol.</p>"},{"location":"api/data/transform/#webs.data.transform.Transformer.is_identifier","title":"is_identifier()","text":"<pre><code>is_identifier(obj: T) -&gt; bool\n</code></pre> <p>Check if the object is an identifier instance.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def is_identifier(self, obj: T) -&gt; bool:\n    \"\"\"Check if the object is an identifier instance.\"\"\"\n    ...\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.Transformer.transform","title":"transform()","text":"<pre><code>transform(obj: T) -&gt; IdentifierT\n</code></pre> <p>Transform the object into an identifier.</p> <p>Parameters:</p> <ul> <li> <code>obj</code>             (<code>T</code>)         \u2013          <p>Object to transform.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>IdentifierT</code>         \u2013          <p>Identifier object that can be used to resolve <code>obj</code>.</p> </li> </ul> Source code in <code>webs/data/transform.py</code> <pre><code>def transform(self, obj: T) -&gt; IdentifierT:\n    \"\"\"Transform the object into an identifier.\n\n    Args:\n        obj: Object to transform.\n\n    Returns:\n        Identifier object that can be used to resolve `obj`.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.Transformer.resolve","title":"resolve()","text":"<pre><code>resolve(identifier: IdentifierT) -&gt; Any\n</code></pre> <p>Resolve an object from an identifier.</p> <p>Parameters:</p> <ul> <li> <code>identifier</code>             (<code>IdentifierT</code>)         \u2013          <p>Identifier to an object.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>         \u2013          <p>The resolved object.</p> </li> </ul> Source code in <code>webs/data/transform.py</code> <pre><code>def resolve(self, identifier: IdentifierT) -&gt; Any:\n    \"\"\"Resolve an object from an identifier.\n\n    Args:\n        identifier: Identifier to an object.\n\n    Returns:\n        The resolved object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer","title":"TaskDataTransformer","text":"<pre><code>TaskDataTransformer(\n    transformer: Transformer[IdentifierT],\n    filter_: Filter | None = None,\n)\n</code></pre> <p>             Bases: <code>Generic[IdentifierT]</code></p> <p>Task data transformer.</p> <p>This class combines a simple object <code>Transformer</code> and a <code>Filter</code> into useful methods for transforming the positional arguments, keyword arguments, and results of tasks.</p> <p>Parameters:</p> <ul> <li> <code>transformer</code>             (<code>Transformer[IdentifierT]</code>)         \u2013          <p>Object transformer.</p> </li> <li> <code>filter_</code>             (<code>Filter | None</code>, default:                 <code>None</code> )         \u2013          <p>A filter which when called on an object returns <code>True</code> if the object should be transformed.</p> </li> </ul> Source code in <code>webs/data/transform.py</code> <pre><code>def __init__(\n    self,\n    transformer: Transformer[IdentifierT],\n    filter_: Filter | None = None,\n) -&gt; None:\n    self.transformer = transformer\n    self.filter_ = NullFilter() if filter_ is None else filter_\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer.transform","title":"transform()","text":"<pre><code>transform(obj: T) -&gt; T | IdentifierT\n</code></pre> <p>Transform an object.</p> <p>Transforms <code>obj</code> into an identifier if it passes the filter check. The identifier can later be used to resolve the object.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def transform(self, obj: T) -&gt; T | IdentifierT:\n    \"\"\"Transform an object.\n\n    Transforms `obj` into an identifier if it passes the filter check.\n    The identifier can later be used to resolve the object.\n    \"\"\"\n    if self.filter_(obj) and not isinstance(obj, Future):\n        return self.transformer.transform(obj)\n    else:\n        return obj\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer.transform_iterable","title":"transform_iterable()","text":"<pre><code>transform_iterable(\n    iterable: Iterable[T],\n) -&gt; tuple[T | IdentifierT, ...]\n</code></pre> <p>Transform each object in an iterable.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def transform_iterable(\n    self,\n    iterable: Iterable[T],\n) -&gt; tuple[T | IdentifierT, ...]:\n    \"\"\"Transform each object in an iterable.\"\"\"\n    return tuple(self.transform(obj) for obj in iterable)\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer.transform_mapping","title":"transform_mapping()","text":"<pre><code>transform_mapping(\n    mapping: Mapping[K, T]\n) -&gt; dict[K, Any]\n</code></pre> <p>Transform each value in a mapping.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def transform_mapping(self, mapping: Mapping[K, T]) -&gt; dict[K, Any]:\n    \"\"\"Transform each value in a mapping.\"\"\"\n    return {k: self.transform(v) for k, v in mapping.items()}\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer.resolve","title":"resolve()","text":"<pre><code>resolve(obj: Any) -&gt; Any\n</code></pre> <p>Resolve an object.</p> <p>Resolves the object if it is an identifier, otherwise returns the passed object.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def resolve(self, obj: Any) -&gt; Any:\n    \"\"\"Resolve an object.\n\n    Resolves the object if it is an identifier, otherwise returns the\n    passed object.\n    \"\"\"\n    if self.transformer.is_identifier(obj):\n        return self.transformer.resolve(obj)\n    else:\n        return obj\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer.resolve_iterable","title":"resolve_iterable()","text":"<pre><code>resolve_iterable(\n    iterable: Iterable[Any],\n) -&gt; tuple[Any, ...]\n</code></pre> <p>Resolve each object in an iterable.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def resolve_iterable(self, iterable: Iterable[Any]) -&gt; tuple[Any, ...]:\n    \"\"\"Resolve each object in an iterable.\"\"\"\n    return tuple(self.resolve(obj) for obj in iterable)\n</code></pre>"},{"location":"api/data/transform/#webs.data.transform.TaskDataTransformer.resolve_mapping","title":"resolve_mapping()","text":"<pre><code>resolve_mapping(mapping: Mapping[K, Any]) -&gt; dict[K, Any]\n</code></pre> <p>Resolve each value in a mapping.</p> Source code in <code>webs/data/transform.py</code> <pre><code>def resolve_mapping(self, mapping: Mapping[K, Any]) -&gt; dict[K, Any]:\n    \"\"\"Resolve each value in a mapping.\"\"\"\n    return {k: self.resolve(v) for k, v in mapping.items()}\n</code></pre>"},{"location":"api/executor/","title":"webs.executor","text":"<code>webs/executor/__init__.py</code>"},{"location":"api/executor/config/","title":"webs.executor.config","text":"<code>webs/executor/config.py</code>"},{"location":"api/executor/config/#webs.executor.config.ExecutorConfig","title":"ExecutorConfig","text":"<p>             Bases: <code>Config</code>, <code>ABC</code></p> <p>Executor configuration abstract base class.</p>"},{"location":"api/executor/config/#webs.executor.config.ExecutorConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/config/#webs.executor.config.ExecutorConfig.get_executor","title":"get_executor()  <code>abstractmethod</code>","text":"<pre><code>get_executor() -&gt; Executor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/config.py</code> <pre><code>@abc.abstractmethod\ndef get_executor(self) -&gt; Executor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    ...\n</code></pre>"},{"location":"api/executor/config/#webs.executor.config.ExecutorChoicesConfig","title":"ExecutorChoicesConfig","text":"<p>             Bases: <code>Config</code></p> <p>Executor choice configuration.</p>"},{"location":"api/executor/config/#webs.executor.config.ExecutorChoicesConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/executor/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    configs = get_registered()\n\n    group = parser.add_argument_group(cls.__name__)\n    group.add_argument(\n        '--executor',\n        choices=sorted(configs.keys()),\n        required=required,\n        help='executor to use',\n    )\n\n    executor_type: str | None = None\n    if argv is not None and '--executor' in argv:\n        executor_type = argv[argv.index('--executor') + 1]\n\n    for name, config_type in configs.items():\n        config_type.add_argument_group(\n            parser,\n            argv=argv,\n            required=name == executor_type,\n        )\n</code></pre>"},{"location":"api/executor/dag/","title":"webs.executor.dag","text":"<code>webs/executor/dag.py</code>"},{"location":"api/executor/dag/#webs.executor.dag.DAGExecutor","title":"DAGExecutor","text":"<pre><code>DAGExecutor(executor: Executor)\n</code></pre> <p>             Bases: <code>Executor</code></p> <p>Executor wrapper that adds DAG-like features.</p> <p>An <code>Executor</code> implementation that wraps another executor with logic for delaying task submission until all <code>Future</code> instances which are args or kwargs of a task have completed. In other words, child tasks will not be scheduled until the results of the child's parent tasks are available.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>Executor</code>)         \u2013          <p>Executor to wrap.</p> </li> </ul> Source code in <code>webs/executor/dag.py</code> <pre><code>def __init__(self, executor: Executor) -&gt; None:\n    self.executor = executor\n    self._tasks: dict[Future[Any], _Task[Any, Any]] = {}\n</code></pre>"},{"location":"api/executor/dag/#webs.executor.dag.DAGExecutor.submit","title":"submit()","text":"<pre><code>submit(\n    function: Callable[P, T],\n    /,\n    *args: args,\n    **kwargs: kwargs,\n) -&gt; Future[T]\n</code></pre> <p>Schedule the callable to be executed.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>Callable to execute.</p> </li> <li> <code>args</code>             (<code>args</code>, default:                 <code>()</code> )         \u2013          <p>Positional arguments.</p> </li> <li> <code>kwargs</code>             (<code>kwargs</code>, default:                 <code>{}</code> )         \u2013          <p>Keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Future[T]</code>         \u2013          <p><code>Future</code> object representing the             result of the execution of the callable.</p> </li> </ul> Source code in <code>webs/executor/dag.py</code> <pre><code>def submit(\n    self,\n    function: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[T]:\n    \"\"\"Schedule the callable to be executed.\n\n    Args:\n        function: Callable to execute.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n\n    Returns:\n        [`Future`][concurrent.futures.Future] object representing the \\\n        result of the execution of the callable.\n    \"\"\"\n    client_future: Future[T] = Future()\n    task = _Task(self.executor, function, args, kwargs, client_future)\n    self._tasks[client_future] = task\n    client_future.add_done_callback(self._task_future_callback)\n    return client_future\n</code></pre>"},{"location":"api/executor/dag/#webs.executor.dag.DAGExecutor.map","title":"map()","text":"<pre><code>map(\n    function: Callable[P, T],\n    *iterables: Iterable[args],\n    timeout: float | None = None,\n    chunksize: int = 1\n) -&gt; Iterator[T]\n</code></pre> <p>Map a function onto iterables of arguments.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>A callable that will take as many arguments as there are passed iterables.</p> </li> <li> <code>iterables</code>             (<code>Iterable[args]</code>, default:                 <code>()</code> )         \u2013          <p>Variable number of iterables.</p> </li> <li> <code>timeout</code>             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of seconds to wait. If None, then there is no limit on the wait time.</p> </li> <li> <code>chunksize</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>If greater than one, the iterables will be chopped into chunks of size chunksize and submitted to the executor. If set to one, the items in the list will be sent one at a time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[T]</code>         \u2013          <p>An iterator equivalent to: <code>map(func, *iterables)</code> but the calls             may be evaluated out-of-order.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>if chunksize is less than one.</p> </li> </ul> Source code in <code>webs/executor/dag.py</code> <pre><code>def map(\n    self,\n    function: Callable[P, T],\n    *iterables: Iterable[P.args],\n    timeout: float | None = None,\n    chunksize: int = 1,\n) -&gt; Iterator[T]:\n    \"\"\"Map a function onto iterables of arguments.\n\n    Args:\n        function: A callable that will take as many arguments as there are\n            passed iterables.\n        iterables: Variable number of iterables.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        chunksize: If greater than one, the iterables will be chopped into\n            chunks of size chunksize and submitted to the executor. If set\n            to one, the items in the list will be sent one at a time.\n\n    Returns:\n        An iterator equivalent to: `map(func, *iterables)` but the calls \\\n        may be evaluated out-of-order.\n\n    Raises:\n        ValueError: if chunksize is less than one.\n    \"\"\"\n    # Based on concurrent.futures.ProcessPoolExecutor.map()\n    # https://github.com/python/cpython/blob/37959e25cbbe1d207c660b5bc9583b9bd1403f1a/Lib/concurrent/futures/process.py\n    if chunksize &lt; 1:\n        raise ValueError('chunksize must be &gt;= 1.')\n\n    results = super().map(\n        functools.partial(_process_chunk, function),\n        _get_chunks(*iterables, chunksize=chunksize),\n        timeout=timeout,\n    )\n\n    def _result_iterator(\n        iterable: Iterator[list[T]],\n    ) -&gt; Generator[T, None, None]:\n        for element in iterable:\n            element.reverse()\n            while element:\n                yield element.pop()\n\n    return _result_iterator(results)\n</code></pre>"},{"location":"api/executor/dag/#webs.executor.dag.DAGExecutor.shutdown","title":"shutdown()","text":"<pre><code>shutdown(\n    wait: bool = True, *, cancel_futures: bool = False\n) -&gt; None\n</code></pre> <p>Shutdown the executor.</p> <p>Parameters:</p> <ul> <li> <code>wait</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Wait on all pending futures to complete.</p> </li> <li> <code>cancel_futures</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Cancel all pending futures that the executor has not started running. Only used in Python 3.9 and later.</p> </li> </ul> Source code in <code>webs/executor/dag.py</code> <pre><code>def shutdown(\n    self,\n    wait: bool = True,\n    *,\n    cancel_futures: bool = False,\n) -&gt; None:\n    \"\"\"Shutdown the executor.\n\n    Args:\n        wait: Wait on all pending futures to complete.\n        cancel_futures: Cancel all pending futures that the executor\n            has not started running. Only used in Python 3.9 and later.\n    \"\"\"\n    if sys.version_info &gt;= (3, 9):  # pragma: &gt;=3.9 cover\n        self.executor.shutdown(wait=wait, cancel_futures=cancel_futures)\n    else:  # pragma: &lt;3.9 cover\n        self.executor.shutdown(wait=wait)\n</code></pre>"},{"location":"api/executor/dask/","title":"webs.executor.dask","text":"<code>webs/executor/dask.py</code>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedExecutor","title":"DaskDistributedExecutor","text":"<pre><code>DaskDistributedExecutor(client: Client)\n</code></pre> <p>             Bases: <code>Executor</code></p> <p>Dask task execution engine.</p> <p>Parameters:</p> <ul> <li> <code>client</code>             (<code>Client</code>)         \u2013          <p>Dask distributed client.</p> </li> </ul> Source code in <code>webs/executor/dask.py</code> <pre><code>def __init__(self, client: Client) -&gt; None:\n    self.client = client\n</code></pre>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedExecutor.submit","title":"submit()","text":"<pre><code>submit(\n    function: Callable[P, T],\n    /,\n    *args: args,\n    **kwargs: kwargs,\n) -&gt; Future[T]\n</code></pre> <p>Schedule the callable to be executed.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>Callable to execute.</p> </li> <li> <code>args</code>             (<code>args</code>, default:                 <code>()</code> )         \u2013          <p>Positional arguments.</p> </li> <li> <code>kwargs</code>             (<code>kwargs</code>, default:                 <code>{}</code> )         \u2013          <p>Keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Future[T]</code>         \u2013          <p><code>Future</code>-like object representing             the result of the execution of the callable.</p> </li> </ul> Source code in <code>webs/executor/dask.py</code> <pre><code>def submit(\n    self,\n    function: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[T]:\n    \"\"\"Schedule the callable to be executed.\n\n    Args:\n        function: Callable to execute.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n\n    Returns:\n        [`Future`][concurrent.futures.Future]-like object representing \\\n        the result of the execution of the callable.\n    \"\"\"\n    return self.client.submit(function, *args, **kwargs)\n</code></pre>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedExecutor.map","title":"map()","text":"<pre><code>map(\n    function: Callable[P, T],\n    *iterables: Iterable[args],\n    timeout: float | None = None,\n    chunksize: int = 1\n) -&gt; Iterator[T]\n</code></pre> <p>Map a function onto iterables of arguments.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>A callable that will take as many arguments as there are passed iterables.</p> </li> <li> <code>iterables</code>             (<code>Iterable[args]</code>, default:                 <code>()</code> )         \u2013          <p>Variable number of iterables.</p> </li> <li> <code>timeout</code>             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of seconds to wait. If None, then there is no limit on the wait time.</p> </li> <li> <code>chunksize</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Sets the Dask batch size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[T]</code>         \u2013          <p>An iterator equivalent to: <code>map(func, *iterables)</code> but the calls             may be evaluated out-of-order.</p> </li> </ul> Source code in <code>webs/executor/dask.py</code> <pre><code>def map(\n    self,\n    function: Callable[P, T],\n    *iterables: Iterable[P.args],\n    timeout: float | None = None,\n    chunksize: int = 1,\n) -&gt; Iterator[T]:\n    \"\"\"Map a function onto iterables of arguments.\n\n    Args:\n        function: A callable that will take as many arguments as there are\n            passed iterables.\n        iterables: Variable number of iterables.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        chunksize: Sets the Dask batch size.\n\n    Returns:\n        An iterator equivalent to: `map(func, *iterables)` but the calls \\\n        may be evaluated out-of-order.\n    \"\"\"\n    # Based on the Parsl implementation.\n    # https://github.com/Parsl/parsl/blob/7fba7d634ccade76618ee397d3c951c5cbf2cd49/parsl/concurrent/__init__.py#L58\n    futures = self.client.map(function, *iterables, batch_size=chunksize)\n\n    def _result_iterator() -&gt; Generator[T, None, None]:\n        futures.reverse()\n        while futures:\n            yield futures.pop().result(timeout)\n\n    return _result_iterator()\n</code></pre>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedExecutor.shutdown","title":"shutdown()","text":"<pre><code>shutdown(\n    wait: bool = True, *, cancel_futures: bool = False\n) -&gt; None\n</code></pre> <p>Shutdown the client.</p> Source code in <code>webs/executor/dask.py</code> <pre><code>def shutdown(\n    self,\n    wait: bool = True,\n    *,\n    cancel_futures: bool = False,\n) -&gt; None:\n    \"\"\"Shutdown the client.\"\"\"\n    if DaskFuture._cb_executor is not None:\n        # Dask runs future callbacks in threads of a ThreadPoolExecutor\n        # that is a class attributed of Dask's future. Shutting down\n        # the client causes all futures to get cancelled, which can\n        # cause a currently executing callback to raise a CancelledError\n        # if the callback accesses the future's result.\n        DaskFuture._cb_executor.shutdown(wait=wait)\n        DaskFuture._cb_executor = None\n\n    # Note: wait and cancel_futures are not implemented.\n    self.client.close()\n</code></pre>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedConfig","title":"DaskDistributedConfig","text":"<p>             Bases: <code>ExecutorConfig</code></p> <p>Dask Distributed configuration.</p> <p>Attributes:</p> <ul> <li> <code>dask_scheduler_address</code>             (<code>Optional[str]</code>)         \u2013          <p>Dask scheduler address.</p> </li> <li> <code>dask_use_threads</code>             (<code>bool</code>)         \u2013          <p>Use threads rather than processes for local clusters.</p> </li> <li> <code>dask_workers</code>             (<code>Optional[int]</code>)         \u2013          <p>Number of Dask workers for local clusters.</p> </li> </ul>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/dask/#webs.executor.dask.DaskDistributedConfig.get_executor","title":"get_executor()","text":"<pre><code>get_executor() -&gt; DaskDistributedExecutor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/dask.py</code> <pre><code>def get_executor(self) -&gt; DaskDistributedExecutor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    if self.dask_scheduler_address is not None:\n        client = Client(self.dask_scheduler_address)\n    else:\n        dask.config.set(\n            {'distributed.worker.daemon': self.dask_daemon_workers},\n        )\n        client = Client(\n            n_workers=self.dask_workers,\n            processes=not self.dask_use_threads,\n            dashboard_address=None,\n        )\n    return DaskDistributedExecutor(client)\n</code></pre>"},{"location":"api/executor/globus/","title":"webs.executor.globus","text":"<code>webs/executor/globus.py</code>"},{"location":"api/executor/globus/#webs.executor.globus.GlobusComputeConfig","title":"GlobusComputeConfig","text":"<p>             Bases: <code>ExecutorConfig</code></p> <p>Globus Compute configuration.</p> <p>Attributes:</p> <ul> <li> <code>endpoint</code>         \u2013          <p>Globus Compute endpoint UUID.</p> </li> </ul>"},{"location":"api/executor/globus/#webs.executor.globus.GlobusComputeConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/globus/#webs.executor.globus.GlobusComputeConfig.get_executor","title":"get_executor()","text":"<pre><code>get_executor() -&gt; DAGExecutor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/globus.py</code> <pre><code>def get_executor(self) -&gt; DAGExecutor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    executor = globus_compute_sdk.Executor(\n        self.globus_compute_endpoint,\n        batch_size=self.globus_compute_batch_size,\n    )\n    return DAGExecutor(executor)\n</code></pre>"},{"location":"api/executor/parsl/","title":"webs.executor.parsl","text":"<code>webs/executor/parsl.py</code>"},{"location":"api/executor/parsl/#webs.executor.parsl.ParslConfig","title":"ParslConfig","text":"<p>             Bases: <code>ExecutorConfig</code></p> <p>Parsl configuration.</p> <p>Attributes:</p> <ul> <li> <code>endpoint</code>         \u2013          <p>Globus Compute endpoint UUID.</p> </li> </ul>"},{"location":"api/executor/parsl/#webs.executor.parsl.ParslConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/parsl/#webs.executor.parsl.ParslConfig.get_executor_config","title":"get_executor_config()","text":"<pre><code>get_executor_config() -&gt; Config\n</code></pre> <p>Create a Parsl config from this config.</p> Source code in <code>webs/executor/parsl.py</code> <pre><code>def get_executor_config(self) -&gt; Config:\n    \"\"\"Create a Parsl config from this config.\"\"\"\n    workers = (\n        self.parsl_workers\n        if self.parsl_workers is not None\n        else multiprocessing.cpu_count()\n    )\n\n    if self.parsl_use_threads:\n        executor = ThreadPoolExecutor(max_threads=workers)\n    else:\n        executor = HighThroughputExecutor(\n            label='htex-local',\n            max_workers_per_node=workers,\n            address=address_by_hostname(),\n            cores_per_worker=1,\n            provider=LocalProvider(\n                channel=LocalChannel(),\n                init_blocks=1,\n                max_blocks=1,\n            ),\n        )\n\n    return Config(executors=[executor], run_dir=self.parsl_run_dir)\n</code></pre>"},{"location":"api/executor/parsl/#webs.executor.parsl.ParslConfig.get_executor","title":"get_executor()","text":"<pre><code>get_executor() -&gt; Executor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/parsl.py</code> <pre><code>def get_executor(self) -&gt; globus_compute_sdk.Executor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    return ParslPoolExecutor(self.get_executor_config())\n</code></pre>"},{"location":"api/executor/python/","title":"webs.executor.python","text":"<code>webs/executor/python.py</code>"},{"location":"api/executor/python/#webs.executor.python.ProcessPoolConfig","title":"ProcessPoolConfig","text":"<p>             Bases: <code>ExecutorConfig</code></p> <p>Process pool executor configuration.</p> <p>Attributes:</p> <ul> <li> <code>max_processes</code>             (<code>int</code>)         \u2013          <p>Maximum number of processes.</p> </li> </ul>"},{"location":"api/executor/python/#webs.executor.python.ProcessPoolConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/python/#webs.executor.python.ProcessPoolConfig.get_executor","title":"get_executor()","text":"<pre><code>get_executor() -&gt; DAGExecutor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/python.py</code> <pre><code>def get_executor(self) -&gt; DAGExecutor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    context = multiprocessing.get_context(self.multiprocessing_context)\n    return DAGExecutor(\n        ProcessPoolExecutor(self.max_processes, mp_context=context),\n    )\n</code></pre>"},{"location":"api/executor/python/#webs.executor.python.ThreadPoolConfig","title":"ThreadPoolConfig","text":"<p>             Bases: <code>ExecutorConfig</code></p> <p>Thread pool executor configuration.</p> <p>Attributes:</p> <ul> <li> <code>max_threads</code>             (<code>int</code>)         \u2013          <p>Maximum number of threads.</p> </li> </ul>"},{"location":"api/executor/python/#webs.executor.python.ThreadPoolConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/python/#webs.executor.python.ThreadPoolConfig.get_executor","title":"get_executor()","text":"<pre><code>get_executor() -&gt; DAGExecutor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/python.py</code> <pre><code>def get_executor(self) -&gt; DAGExecutor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    return DAGExecutor(ThreadPoolExecutor(self.max_threads))\n</code></pre>"},{"location":"api/executor/ray/","title":"webs.executor.ray","text":"<code>webs/executor/ray.py</code>"},{"location":"api/executor/ray/#webs.executor.ray.RayExecutor","title":"RayExecutor","text":"<pre><code>RayExecutor(\n    address: str | None = \"local\",\n    num_cpus: int | None = None,\n)\n</code></pre> <p>             Bases: <code>Executor</code></p> <p>Ray execution engine.</p> <p>Parameters:</p> <ul> <li> <code>address</code>             (<code>str | None</code>, default:                 <code>'local'</code> )         \u2013          <p>Address to pass to <code>ray.init()</code>.</p> </li> <li> <code>num_cpus</code>             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of CPUs to use.</p> </li> </ul> Source code in <code>webs/executor/ray.py</code> <pre><code>def __init__(\n    self,\n    address: str | None = 'local',\n    num_cpus: int | None = None,\n) -&gt; None:\n    if RAY_IMPORT_ERROR is not None:  # pragma: no cover\n        raise RAY_IMPORT_ERROR\n\n    ray.init(address=address, configure_logging=False, num_cpus=num_cpus)\n    # Mapping of Python callables to Ray RemoteFunction types\n    self._remote: dict[Any, Any] = {}\n</code></pre>"},{"location":"api/executor/ray/#webs.executor.ray.RayExecutor.submit","title":"submit()","text":"<pre><code>submit(\n    function: Callable[P, T],\n    /,\n    *args: args,\n    **kwargs: kwargs,\n) -&gt; Future[T]\n</code></pre> <p>Schedule the callable to be executed.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>Callable to execute.</p> </li> <li> <code>args</code>             (<code>args</code>, default:                 <code>()</code> )         \u2013          <p>Positional arguments.</p> </li> <li> <code>kwargs</code>             (<code>kwargs</code>, default:                 <code>{}</code> )         \u2013          <p>Keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Future[T]</code>         \u2013          <p><code>Future</code>-like object representing             the result of the execution of the callable.</p> </li> </ul> Source code in <code>webs/executor/ray.py</code> <pre><code>def submit(\n    self,\n    function: Callable[P, T],\n    /,\n    *args: P.args,\n    **kwargs: P.kwargs,\n) -&gt; Future[T]:\n    \"\"\"Schedule the callable to be executed.\n\n    Args:\n        function: Callable to execute.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n\n    Returns:\n        [`Future`][concurrent.futures.Future]-like object representing \\\n        the result of the execution of the callable.\n    \"\"\"\n    args = cast(\n        P.args,\n        tuple(\n            arg.object_ref if isinstance(arg, Future) else arg  # type: ignore[attr-defined]\n            for arg in args\n        ),\n    )\n    kwargs = cast(\n        P.kwargs,\n        {\n            k: v.object_ref if isinstance(v, Future) else v  # type: ignore[attr-defined]\n            for k, v in kwargs.items()\n        },\n    )\n\n    if function in self._remote:\n        remote = self._remote[function]\n    else:\n        # This wrapper is needed because Ray will raise a TypeError\n        # on certain function type that fail the inspect.isfunction\n        # and inspect.isclass checks in\n        # https://github.com/ray-project/ray/blob/6a8997cd720e2a92c5dc2763becf39e180b8c96e/python/ray/_private/worker.py#L3018-L3037\n        def _wrapper(*args: P.args, **kwargs: P.kwargs) -&gt; T:\n            return function(*args, **kwargs)  # pragma: no cover\n\n        remote = ray.remote(_wrapper)\n        self._remote[function] = remote\n\n    object_ref = remote.remote(*args, **kwargs)\n\n    return object_ref.future()\n</code></pre>"},{"location":"api/executor/ray/#webs.executor.ray.RayExecutor.shutdown","title":"shutdown()","text":"<pre><code>shutdown(\n    wait: bool = True, *, cancel_futures: bool = False\n) -&gt; None\n</code></pre> <p>Shutdown the client.</p> Source code in <code>webs/executor/ray.py</code> <pre><code>def shutdown(\n    self,\n    wait: bool = True,\n    *,\n    cancel_futures: bool = False,\n) -&gt; None:\n    \"\"\"Shutdown the client.\"\"\"\n    ray.shutdown()\n</code></pre>"},{"location":"api/executor/ray/#webs.executor.ray.RayConfig","title":"RayConfig","text":"<p>             Bases: <code>ExecutorConfig</code></p> <p>Ray configuration.</p> <p>Attributes:</p> <ul> <li> <code>ray_address</code>             (<code>Optional[str]</code>)         \u2013          <p>Address of the Ray cluster to run on.</p> </li> <li> <code>processes</code>             (<code>Optional[str]</code>)         \u2013          <p>Number of actor processes to start in the pool. Defaults to the number of cores in the Ray cluster, or the number of cores on this machine.</p> </li> </ul>"},{"location":"api/executor/ray/#webs.executor.ray.RayConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/executor/ray/#webs.executor.ray.RayConfig.get_executor","title":"get_executor()","text":"<pre><code>get_executor() -&gt; RayExecutor\n</code></pre> <p>Create an executor instance from the config.</p> Source code in <code>webs/executor/ray.py</code> <pre><code>def get_executor(self) -&gt; RayExecutor:\n    \"\"\"Create an executor instance from the config.\"\"\"\n    return RayExecutor(\n        address=self.ray_address,\n        num_cpus=self.ray_num_cpus,\n    )\n</code></pre>"},{"location":"api/executor/workflow/","title":"webs.executor.workflow","text":"<code>webs/executor/workflow.py</code>"},{"location":"api/executor/workflow/#webs.executor.workflow.ExecutionInfo","title":"ExecutionInfo  <code>dataclass</code>","text":"<pre><code>ExecutionInfo(\n    hostname: str,\n    execution_start_time: float,\n    execution_end_time: float,\n    task_start_time: float,\n    task_end_time: float,\n    input_transform_start_time: float,\n    input_transform_end_time: float,\n    result_transform_start_time: float,\n    result_transform_end_time: float,\n)\n</code></pre> <p>Task execution information.</p>"},{"location":"api/executor/workflow/#webs.executor.workflow.TaskInfo","title":"TaskInfo  <code>dataclass</code>","text":"<pre><code>TaskInfo(\n    task_id: str,\n    function_name: str,\n    parent_task_ids: list[str],\n    submit_time: float,\n    received_time: float | None = None,\n    success: bool | None = None,\n    exception: str | None = None,\n    execution: ExecutionInfo | None = None,\n)\n</code></pre> <p>Task information.</p>"},{"location":"api/executor/workflow/#webs.executor.workflow.TaskFuture","title":"TaskFuture","text":"<pre><code>TaskFuture(\n    future: Future[_TaskResult[T]],\n    info: TaskInfo,\n    data_transformer: TaskDataTransformer[Any],\n)\n</code></pre> <p>             Bases: <code>Generic[T]</code></p> <p>Workflow task future.</p> Note <p>This class should not be instantiated by clients.</p> <p>Attributes:</p> <ul> <li> <code>info</code>         \u2013          <p>Task information and metadata.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>future</code>             (<code>Future[_TaskResult[T]]</code>)         \u2013          <p>Underlying future returned by the compute executor.</p> </li> <li> <code>info</code>             (<code>TaskInfo</code>)         \u2013          <p>Task information and metadata.</p> </li> <li> <code>data_transformer</code>             (<code>TaskDataTransformer[Any]</code>)         \u2013          <p>Data transformer used to resolve the task result.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def __init__(\n    self,\n    future: Future[_TaskResult[T]],\n    info: TaskInfo,\n    data_transformer: TaskDataTransformer[Any],\n) -&gt; None:\n    self.info = info\n    self._future = future\n    self._data_transformer = data_transformer\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.TaskFuture.cancel","title":"cancel()","text":"<pre><code>cancel() -&gt; bool\n</code></pre> <p>Attempt to cancel the task.</p> <p>If the call is currently being executed or finished running and cannot be cancelled then the method will return <code>False</code>, otherwise the call will be cancelled and the method will return <code>True</code>.</p> Source code in <code>webs/executor/workflow.py</code> <pre><code>def cancel(self) -&gt; bool:\n    \"\"\"Attempt to cancel the task.\n\n    If the call is currently being executed or finished running and\n    cannot be cancelled then the method will return `False`, otherwise\n    the call will be cancelled and the method will return `True`.\n    \"\"\"\n    return self._future.cancel()\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.TaskFuture.done","title":"done()","text":"<pre><code>done() -&gt; bool\n</code></pre> <p>Return <code>True</code> is the call was successfully cancelled or finished.</p> Source code in <code>webs/executor/workflow.py</code> <pre><code>def done(self) -&gt; bool:\n    \"\"\"Return `True` is the call was successfully cancelled or finished.\"\"\"\n    return self._future.done()\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.TaskFuture.exception","title":"exception()","text":"<pre><code>exception() -&gt; BaseException | None\n</code></pre> <p>Get the exception raised by the task or <code>None</code> if successful.</p> Source code in <code>webs/executor/workflow.py</code> <pre><code>def exception(self) -&gt; BaseException | None:\n    \"\"\"Get the exception raised by the task or `None` if successful.\"\"\"\n    return self._future.exception()\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.TaskFuture.result","title":"result()","text":"<pre><code>result(timeout: float | None = None) -&gt; T\n</code></pre> <p>Get the result of the task.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>If the task has not finished, wait up to <code>timeout</code> seconds.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>T</code>         \u2013          <p>Task result if the task completed successfully.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>TimeoutError</code>           \u2013          <p>If <code>timeout</code> is specified and the task does not complete within <code>timeout</code> seconds.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def result(self, timeout: float | None = None) -&gt; T:\n    \"\"\"Get the result of the task.\n\n    Args:\n        timeout: If the task has not finished, wait up to `timeout`\n            seconds.\n\n    Returns:\n        Task result if the task completed successfully.\n\n    Raises:\n        TimeoutError: If `timeout` is specified and the task does not\n            complete within `timeout` seconds.\n    \"\"\"\n    task_result = self._future.result(timeout=timeout)\n    result = self._data_transformer.resolve(task_result.result)\n    return result\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.WorkflowExecutor","title":"WorkflowExecutor","text":"<pre><code>WorkflowExecutor(\n    compute_executor: Executor,\n    *,\n    data_transformer: (\n        TaskDataTransformer[Any] | None\n    ) = None,\n    record_logger: RecordLogger | None = None\n)\n</code></pre> <p>Workflow executor.</p> <p>Parameters:</p> <ul> <li> <code>compute_executor</code>             (<code>Executor</code>)         \u2013          <p>Compute executor.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def __init__(\n    self,\n    compute_executor: Executor,\n    *,\n    data_transformer: TaskDataTransformer[Any] | None = None,\n    record_logger: RecordLogger | None = None,\n) -&gt; None:\n    self.compute_executor = compute_executor\n    self.data_transformer = (\n        data_transformer\n        if data_transformer is not None\n        else TaskDataTransformer(NullTransformer())\n    )\n    self.record_logger = (\n        record_logger if record_logger is not None else NullRecordLogger()\n    )\n\n    # Maps user provided functions to the wrapped function.\n    # This is tricky to type, so we just use Any.\n    self._registered_tasks: dict[\n        Callable[[Any], Any],\n        _TaskWrapper[Any, Any],\n    ] = {}\n\n    # Internal bookkeeping\n    self._running_tasks: dict[Future[Any], TaskFuture[Any]] = {}\n    self._total_tasks = 0\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.WorkflowExecutor.tasks_executed","title":"tasks_executed  <code>property</code>","text":"<pre><code>tasks_executed: int\n</code></pre> <p>Total number of tasks submitted for execution.</p>"},{"location":"api/executor/workflow/#webs.executor.workflow.WorkflowExecutor.submit","title":"submit()","text":"<pre><code>submit(\n    function: Callable[P, T], /, *args: Any, **kwargs: Any\n) -&gt; TaskFuture[T]\n</code></pre> <p>Schedule the callable to be executed.</p> <p>This function can also accept <code>TaskFuture</code> objects as input to denote dependencies between a parent and this child task.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>Callable to execute.</p> </li> <li> <code>args</code>             (<code>Any</code>, default:                 <code>()</code> )         \u2013          <p>Positional arguments.</p> </li> <li> <code>kwargs</code>             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Keyword arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TaskFuture[T]</code>         \u2013          <p><code>TaskFuture</code> object             representing the result of the execution of the callable</p> </li> <li> <code>TaskFuture[T]</code>         \u2013          <p>accessible via             <code>TaskFuture.result()</code>.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def submit(\n    self,\n    function: Callable[P, T],\n    /,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; TaskFuture[T]:\n    \"\"\"Schedule the callable to be executed.\n\n    This function can also accept\n    [`TaskFuture`][webs.executor.workflow.TaskFuture] objects as input\n    to denote dependencies between a parent and this child task.\n\n    Args:\n        function: Callable to execute.\n        args: Positional arguments.\n        kwargs: Keyword arguments.\n\n    Returns:\n        [`TaskFuture`][webs.executor.workflow.TaskFuture] object \\\n        representing the result of the execution of the callable\n        accessible via \\\n        [`TaskFuture.result()`][webs.executor.workflow.TaskFuture.result].\n    \"\"\"\n    task_id = uuid.uuid4()\n\n    if function not in self._registered_tasks:\n        self._registered_tasks[function] = _TaskWrapper(\n            function,\n            task_id=task_id,\n            data_transformer=self.data_transformer,\n        )\n\n    task = cast(\n        Callable[P, _TaskResult[T]],\n        self._registered_tasks[function],\n    )\n\n    parents = [\n        str(arg.info.task_id)\n        for arg in (*args, *kwargs.values())\n        if isinstance(arg, TaskFuture)\n    ]\n    info = TaskInfo(\n        task_id=str(task_id),\n        function_name=function.__name__,\n        parent_task_ids=parents,\n        submit_time=time.time(),\n    )\n\n    # Extract executor futures from inside TaskFuture objects\n    args = tuple(\n        arg._future if isinstance(arg, TaskFuture) else arg for arg in args\n    )\n    kwargs = {\n        k: v._future if isinstance(v, TaskFuture) else v\n        for k, v in kwargs.items()\n    }\n\n    args = self.data_transformer.transform_iterable(args)\n    kwargs = self.data_transformer.transform_mapping(kwargs)\n\n    future = self.compute_executor.submit(task, *args, **kwargs)\n    self._total_tasks += 1\n\n    task_future = TaskFuture(future, info, self.data_transformer)\n    self._running_tasks[future] = task_future\n    future.add_done_callback(self._task_done_callback)\n\n    return task_future\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.WorkflowExecutor.map","title":"map()","text":"<pre><code>map(\n    function: Callable[P, T],\n    *iterables: Iterable[args],\n    timeout: float | None = None,\n    chunksize: int = 1\n) -&gt; Iterator[T]\n</code></pre> <p>Map a function onto iterables of arguments.</p> <p>Parameters:</p> <ul> <li> <code>function</code>             (<code>Callable[P, T]</code>)         \u2013          <p>A callable that will take as many arguments as there are passed iterables.</p> </li> <li> <code>iterables</code>             (<code>Iterable[args]</code>, default:                 <code>()</code> )         \u2013          <p>Variable number of iterables.</p> </li> <li> <code>timeout</code>             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>The maximum number of seconds to wait. If None, then there is no limit on the wait time.</p> </li> <li> <code>chunksize</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Currently no supported. If greater than one, the iterables will be chopped into chunks of size chunksize and submitted to the executor. If set to one, the items in the list will be sent one at a time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Iterator[T]</code>         \u2013          <p>An iterator equivalent to: <code>map(func, *iterables)</code> but the calls             may be evaluated out-of-order.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def map(\n    self,\n    function: Callable[P, T],\n    *iterables: Iterable[P.args],\n    timeout: float | None = None,\n    chunksize: int = 1,\n) -&gt; Iterator[T]:\n    \"\"\"Map a function onto iterables of arguments.\n\n    Args:\n        function: A callable that will take as many arguments as there are\n            passed iterables.\n        iterables: Variable number of iterables.\n        timeout: The maximum number of seconds to wait. If None, then there\n            is no limit on the wait time.\n        chunksize: Currently no supported. If greater than one, the\n            iterables will be chopped into chunks of size chunksize\n            and submitted to the executor. If set to one, the items in the\n            list will be sent one at a time.\n\n    Returns:\n        An iterator equivalent to: `map(func, *iterables)` but the calls \\\n        may be evaluated out-of-order.\n    \"\"\"\n    # Source: https://github.com/python/cpython/blob/ec1398e117fb142cc830495503dbdbb1ddafe941/Lib/concurrent/futures/_base.py#L583-L625\n    if timeout is not None:\n        end_time = timeout + time.monotonic()\n\n    tasks = [self.submit(function, *args) for args in zip(*iterables)]\n\n    # Yield must be hidden in closure so that the futures are submitted\n    # before the first iterator value is required.\n    def _result_iterator() -&gt; Generator[T, None, None]:\n        # reverse to keep finishing order\n        tasks.reverse()\n        while tasks:\n            # Careful not to keep a reference to the popped future\n            if timeout is None:\n                yield _result_or_cancel(tasks.pop())\n            else:\n                yield _result_or_cancel(\n                    tasks.pop(),\n                    end_time - time.monotonic(),\n                )\n\n    return _result_iterator()\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.WorkflowExecutor.shutdown","title":"shutdown()","text":"<pre><code>shutdown(\n    wait: bool = True, *, cancel_futures: bool = False\n) -&gt; None\n</code></pre> <p>Shutdown the executor.</p> <p>Parameters:</p> <ul> <li> <code>wait</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Wait on all pending futures to complete.</p> </li> <li> <code>cancel_futures</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Cancel all pending futures that the executor has not started running. Only used in Python 3.9 and later.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def shutdown(\n    self,\n    wait: bool = True,\n    *,\n    cancel_futures: bool = False,\n) -&gt; None:\n    \"\"\"Shutdown the executor.\n\n    Args:\n        wait: Wait on all pending futures to complete.\n        cancel_futures: Cancel all pending futures that the executor\n            has not started running. Only used in Python 3.9 and later.\n    \"\"\"\n    if sys.version_info &gt;= (3, 9):  # pragma: &gt;=3.9 cover\n        self.compute_executor.shutdown(\n            wait=wait,\n            cancel_futures=cancel_futures,\n        )\n    else:  # pragma: &lt;3.9 cover\n        self.compute_executor.shutdown(wait=wait)\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.as_completed","title":"as_completed()","text":"<pre><code>as_completed(\n    tasks: Sequence[TaskFuture[T]],\n    timeout: float | None = None,\n) -&gt; Generator[TaskFuture[T], None, None]\n</code></pre> <p>Return an iterator which yields tasks as they complete.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>             (<code>Sequence[TaskFuture[T]]</code>)         \u2013          <p>Sequence of tasks.</p> </li> <li> <code>timeout</code>             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>Seconds to wait for a task to complete. If no task completes in that time, a <code>TimeoutError</code> is raised. If timeout is <code>None</code>, there is no limit to the wait time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Generator[TaskFuture[T], None, None]</code>         \u2013          <p>Iterator which yields futures as they complete (finished or cancelled         futures).</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def as_completed(\n    tasks: Sequence[TaskFuture[T]],\n    timeout: float | None = None,\n) -&gt; Generator[TaskFuture[T], None, None]:\n    \"\"\"Return an iterator which yields tasks as they complete.\n\n    Args:\n        tasks: Sequence of tasks.\n        timeout: Seconds to wait for a task to complete. If no task completes\n            in that time, a `TimeoutError` is raised. If timeout is `None`,\n            there is no limit to the wait time.\n\n    Returns:\n        Iterator which yields futures as they complete (finished or cancelled \\\n        futures).\n    \"\"\"\n    futures = {task._future: task for task in tasks}\n\n    kwargs = {'timeout': timeout}\n    if len(tasks) == 0 or isinstance(tasks[0]._future, Future):\n        _as_completed = as_completed_python\n    elif isinstance(tasks[0]._future, DaskFuture):\n        _as_completed = as_completed_dask\n        if sys.version_info &lt; (3, 9):  # pragma: &lt;3.9 cover\n            kwargs = {}\n    else:  # pragma: no cover\n        raise ValueError(f'Unsupported future type {type(tasks[0])}.')\n\n    for completed in _as_completed(futures.keys(), **kwargs):\n        yield futures[completed]\n</code></pre>"},{"location":"api/executor/workflow/#webs.executor.workflow.wait","title":"wait()","text":"<pre><code>wait(\n    tasks: Sequence[TaskFuture[T]],\n    timeout: float | None = None,\n    return_when: str = \"ALL_COMPLETED\",\n) -&gt; tuple[set[TaskFuture[T]], set[TaskFuture[T]]]\n</code></pre> <p>Wait for tasks to finish.</p> <p>Parameters:</p> <ul> <li> <code>tasks</code>             (<code>Sequence[TaskFuture[T]]</code>)         \u2013          <p>Sequence of tasks to wait on.</p> </li> <li> <code>timeout</code>             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>Maximum number of seconds to wait on tasks. Can be <code>None</code> to wait indefinitely.</p> </li> <li> <code>return_when</code>             (<code>str</code>, default:                 <code>'ALL_COMPLETED'</code> )         \u2013          <p>Either <code>\"ALL_COMPLETED\"</code> or <code>\"FIRST_COMPLETED\"</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[set[TaskFuture[T]], set[TaskFuture[T]]]</code>         \u2013          <p>Tuple containing the set of completed tasks and the set of not         completed tasks.</p> </li> </ul> Source code in <code>webs/executor/workflow.py</code> <pre><code>def wait(\n    tasks: Sequence[TaskFuture[T]],\n    timeout: float | None = None,\n    return_when: str = 'ALL_COMPLETED',\n) -&gt; tuple[set[TaskFuture[T]], set[TaskFuture[T]]]:\n    \"\"\"Wait for tasks to finish.\n\n    Args:\n        tasks: Sequence of tasks to wait on.\n        timeout: Maximum number of seconds to wait on tasks. Can be `None` to\n            wait indefinitely.\n        return_when: Either `\"ALL_COMPLETED\"` or `\"FIRST_COMPLETED\"`.\n\n    Returns:\n        Tuple containing the set of completed tasks and the set of not \\\n        completed tasks.\n    \"\"\"\n    futures = {task._future: task for task in tasks}\n\n    if len(tasks) == 0 or isinstance(tasks[0]._future, Future):\n        _wait = wait_python\n    elif isinstance(tasks[0]._future, DaskFuture):\n        _wait = wait_dask\n    else:  # pragma: no cover\n        raise ValueError(f'Unsupported future type {type(tasks[0])}.')\n\n    completed_futures, not_completed_futures = _wait(\n        list(futures.keys()),\n        timeout=timeout,\n        return_when=return_when,\n    )\n\n    completed_tasks = {futures[f] for f in completed_futures}\n    not_completed_tasks = {futures[f] for f in not_completed_futures}\n\n    return (completed_tasks, not_completed_tasks)\n</code></pre>"},{"location":"api/run/","title":"webs.run","text":"<code>webs/run/__init__.py</code>"},{"location":"api/run/config/","title":"webs.run.config","text":"<code>webs/run/config.py</code>"},{"location":"api/run/config/#webs.run.config.RunConfig","title":"RunConfig","text":"<p>             Bases: <code>Config</code></p> <p>Run configuration.</p> <p>Attributes:</p> <ul> <li> <code>log_file_level</code>             (<code>Union[int, str]</code>)         \u2013          <p>Logging level for the log file.</p> </li> <li> <code>log_file_name</code>             (<code>Optional[str]</code>)         \u2013          <p>Logging file name. If <code>None</code>, only logging to <code>stdout</code> is used.</p> </li> <li> <code>log_level</code>             (<code>Union[int, str]</code>)         \u2013          <p>Logging level for <code>stdout</code>.</p> </li> <li> <code>run_dir</code>             (<code>Union[int, str]</code>)         \u2013          <p>Runtime directory.</p> </li> </ul>"},{"location":"api/run/config/#webs.run.config.RunConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/run/config/#webs.run.config.BenchmarkConfig","title":"BenchmarkConfig","text":"<p>             Bases: <code>Config</code></p> <p>Workflow benchmark configuration.</p> <p>Attributes:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>Name of the workflow to execute.</p> </li> <li> <code>timestamp</code>             (<code>datetime</code>)         \u2013          <p>Start time of the workflow.</p> </li> <li> <code>transformer</code>             (<code>SerializeAsAny[TransformerConfig]</code>)         \u2013          <p>Transformer config.</p> </li> <li> <code>filter</code>             (<code>SerializeAsAny[FilterConfig]</code>)         \u2013          <p>Filter config.</p> </li> <li> <code>run</code>             (<code>SerializeAsAny[RunConfig]</code>)         \u2013          <p>Run configuration.</p> </li> <li> <code>workflow</code>             (<code>SerializeAsAny[Config]</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul>"},{"location":"api/run/config/#webs.run.config.BenchmarkConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/run/config/#webs.run.config.BenchmarkConfig.get_run_dir","title":"get_run_dir()","text":"<pre><code>get_run_dir() -&gt; Path\n</code></pre> <p>Create and return the path to the run directory.</p> Source code in <code>webs/run/config.py</code> <pre><code>def get_run_dir(self) -&gt; pathlib.Path:\n    \"\"\"Create and return the path to the run directory.\"\"\"\n    timestamp = self.timestamp.strftime('%Y-%m-%d-%H-%M-%S')\n    run_dir = pathlib.Path(\n        self.run.run_dir_format.format(\n            name=self.name,\n            timestamp=timestamp,\n        ),\n    )\n    run_dir.mkdir(parents=True, exist_ok=True)\n    return run_dir\n</code></pre>"},{"location":"api/run/main/","title":"webs.run.main","text":"<code>webs/run/main.py</code>"},{"location":"api/run/main/#webs.run.main.parse_args_to_config","title":"parse_args_to_config()","text":"<pre><code>parse_args_to_config(\n    argv: Sequence[str],\n) -&gt; BenchmarkConfig\n</code></pre> <p>Parse sequence of string arguments into a config.</p> <p>Parameters:</p> <ul> <li> <code>argv</code>             (<code>Sequence[str]</code>)         \u2013          <p>Sequence of string arguments.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BenchmarkConfig</code>         \u2013          <p>Configuration.</p> </li> </ul> Source code in <code>webs/run/main.py</code> <pre><code>def parse_args_to_config(argv: Sequence[str]) -&gt; BenchmarkConfig:\n    \"\"\"Parse sequence of string arguments into a config.\n\n    Args:\n        argv: Sequence of string arguments.\n\n    Returns:\n        Configuration.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description='Workflow benchmark suite.',\n        prog='python -m webs.run',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n\n    subparsers = parser.add_subparsers(\n        title='Workflows',\n        dest='name',\n        required=True,\n        help='workflow to execute',\n    )\n\n    workflow_names = sorted(get_registered_workflow_names())\n    for workflow_name in workflow_names:\n        subparser = subparsers.add_parser(\n            workflow_name,\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        )\n\n        RunConfig.add_argument_group(subparser, argv=argv, required=True)\n        ExecutorChoicesConfig.add_argument_group(\n            subparser,\n            argv=argv,\n            required=True,\n        )\n        TransformerChoicesConfig.add_argument_group(\n            subparser,\n            argv=argv,\n            required=False,\n        )\n        FilterConfig.add_argument_group(\n            subparser,\n            argv=argv,\n            required=True,\n        )\n\n        # Avoid importing the workflow and its dependencies unless the\n        # user has already specified it.\n        if workflow_name in argv:\n            workflow = get_registered_workflow(workflow_name)\n            workflow.config_type.add_argument_group(\n                subparser,\n                argv=argv,\n                required=True,\n            )\n\n    args = parser.parse_args(argv)\n    options = vars(args)\n\n    workflow_name = options['name']\n    executor_config = get_executor_config(**options)\n    transformer_config = get_transformer_config(**options)\n    filter_config = FilterConfig(**options)\n    run_config = RunConfig(**options)\n    workflow = get_registered_workflow(workflow_name)\n    workflow_config = workflow.config_type(**options)\n\n    return BenchmarkConfig(\n        name=workflow_name,\n        timestamp=datetime.now(),\n        executor=executor_config,\n        transformer=transformer_config,\n        filter=filter_config,\n        run=run_config,\n        workflow=workflow_config,\n    )\n</code></pre>"},{"location":"api/run/main/#webs.run.main.run","title":"run()","text":"<pre><code>run(config: BenchmarkConfig) -&gt; None\n</code></pre> <p>Run a workflow using the configuration.</p> <p>This function changes the current working directory to <code>config.run.run_dir</code> so that all paths are relative to the current working directory.</p> Source code in <code>webs/run/main.py</code> <pre><code>@_cwd_run_dir\ndef run(config: BenchmarkConfig) -&gt; None:\n    \"\"\"Run a workflow using the configuration.\n\n    This function changes the current working directory to\n    `config.run.run_dir` so that all paths are relative to the current\n    working directory.\n    \"\"\"\n    start = time.perf_counter()\n\n    cwd = pathlib.Path.cwd().resolve()\n\n    logger.log(RUN_LOG_LEVEL, f'Starting workflow (name={config.name})')\n    logger.log(RUN_LOG_LEVEL, config)\n    logger.log(RUN_LOG_LEVEL, f'Runtime directory: {cwd}')\n\n    config_json = config.model_dump_json(exclude={'timestamp'}, indent=4)\n    with open('config.json', 'w') as f:\n        f.write(config_json)\n\n    workflow = get_registered_workflow(config.name).from_config(\n        config.workflow,\n    )\n\n    compute_executor = config.executor.get_executor()\n    data_transformer = TaskDataTransformer(\n        transformer=config.transformer.get_transformer(),\n        filter_=config.filter.get_filter(),\n    )\n    record_logger = JSONRecordLogger(config.run.task_record_file_name)\n    executor = WorkflowExecutor(\n        compute_executor,\n        data_transformer=data_transformer,\n        record_logger=record_logger,\n    )\n\n    with workflow, record_logger, executor:\n        workflow.run(executor=executor, run_dir=cwd)\n\n    runtime = time.perf_counter() - start\n    logger.log(\n        RUN_LOG_LEVEL,\n        f'Finished workflow (name={config.name}, '\n        f'runtime={runtime:.2f}s, tasks={executor.tasks_executed})',\n    )\n</code></pre>"},{"location":"api/wf/","title":"webs.wf","text":"<code>webs/wf/__init__.py</code>"},{"location":"api/wf/cholesky/","title":"webs.wf.cholesky","text":"<code>webs/wf/cholesky/__init__.py</code> <p>Tiled Cholesky decomposition workflow.</p> <p>Computes the Cholesky decomposition of a randomly generated positive definite matrix. Based on the workflow from this paper.</p> Example <p>The following command computes the decomposition of a 1000 x 1000 matrix using 100 x 100 block/tile sizes. <pre><code>$ python -m webs.run cholesky --executor process-pool --n 1000 --block-size 100\n[2024-05-17 11:09:24.779] RUN   (webs.run) :: Starting workflow (name=cholesky)\n...\n[2024-05-17 11:09:24.810] WORK  (webs.wf.cholesky.workflow) :: Input matrix: (1000, 1000)\n[2024-05-17 11:09:24.810] WORK  (webs.wf.cholesky.workflow) :: Block size: 100\n[2024-05-17 11:09:25.000] WORK  (webs.wf.cholesky.workflow) :: Output matrix: (1000, 1000)\n[2024-05-17 11:09:25.004] RUN   (webs.run) :: Finished workflow (name=cholesky, runtime=0.23s)\n</code></pre></p>"},{"location":"api/wf/cholesky/config/","title":"webs.wf.cholesky.config","text":"<code>webs/wf/cholesky/config.py</code>"},{"location":"api/wf/cholesky/config/#webs.wf.cholesky.config.CholeskyWorkflowConfig","title":"CholeskyWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Cholesky workflow configuration.</p>"},{"location":"api/wf/cholesky/config/#webs.wf.cholesky.config.CholeskyWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/cholesky/workflow/","title":"webs.wf.cholesky.workflow","text":"<code>webs/wf/cholesky/workflow.py</code>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.CholeskyWorkflow","title":"CholeskyWorkflow","text":"<pre><code>CholeskyWorkflow(config: CholeskyWorkflowConfig)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Cholesky workflow.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>CholeskyWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def __init__(self, config: CholeskyWorkflowConfig) -&gt; None:\n    self.config = config\n    super().__init__()\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.CholeskyWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: CholeskyWorkflowConfig) -&gt; Self\n</code></pre> <p>Initialize a workflow from a config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>CholeskyWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Workflow.</p> </li> </ul> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: CholeskyWorkflowConfig) -&gt; Self:\n    \"\"\"Initialize a workflow from a config.\n\n    Args:\n        config: Workflow configuration.\n\n    Returns:\n        Workflow.\n    \"\"\"\n    return cls(config)\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.CholeskyWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Workflow task executor.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Run directory.</p> </li> </ul> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:\n    \"\"\"Run the workflow.\n\n    Args:\n        executor: Workflow task executor.\n        run_dir: Run directory.\n    \"\"\"\n    max_print_size = 8\n\n    matrix = create_psd_matrix(self.config.n)\n    lower = np.zeros_like(matrix)\n\n    n = matrix.shape[0]\n    block_size = min(self.config.block_size, n)\n\n    if matrix.shape[0] &lt;= max_print_size:\n        logger.log(\n            WORK_LOG_LEVEL,\n            f'Input matrix: {matrix.shape}\\n{matrix}',\n        )\n    else:\n        logger.log(WORK_LOG_LEVEL, f'Input matrix: {matrix.shape}')\n    logger.log(WORK_LOG_LEVEL, f'Block size: {block_size}')\n\n    for k in range(0, n, block_size):\n        end_k = min(k + block_size, n)\n        lower_tasks: dict[tuple[int, int], TaskFuture[np.ndarray]] = {}\n\n        lower_tasks[(k, k)] = executor.submit(\n            potrf,\n            matrix[k:end_k, k:end_k],\n        )\n\n        for i in range(k + block_size, n, block_size):\n            end_i = min(i + block_size, n)\n\n            lower_tasks[(i, k)] = executor.submit(\n                trsm,\n                lower_tasks[(k, k)],\n                matrix[i:end_i, k:end_k],\n            )\n\n        gemm_tasks: dict[tuple[int, int], TaskFuture[np.ndarray]] = {}\n\n        for i in range(k + block_size, n, block_size):\n            end_i = min(i + block_size, n)\n            for j in range(i, n, block_size):\n                end_j = min(j + block_size, n)\n\n                syrk_task = executor.submit(\n                    syrk,\n                    matrix[i:end_i, j:end_j],\n                    lower_tasks[(i, k)],\n                )\n\n                gemm_tasks[(i, j)] = executor.submit(\n                    gemm,\n                    syrk_task,\n                    lower_tasks[(i, k)],\n                    lower_tasks[(j, k)],\n                )\n\n        for (i, j), tile in lower_tasks.items():\n            end_i = min(i + block_size, n)\n            end_j = min(j + block_size, n)\n            lower[i:end_i, j:end_j] = tile.result()\n\n        for (i, j), tile in gemm_tasks.items():\n            end_i = min(i + block_size, n)\n            end_j = min(j + block_size, n)\n            matrix[i:end_i, j:end_j] = tile.result()\n\n    if matrix.shape[0] &lt;= max_print_size:\n        logger.log(WORK_LOG_LEVEL, f'Output matrix:\\n{lower}')\n    else:\n        logger.log(WORK_LOG_LEVEL, f'Output matrix: {lower.shape}')\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.potrf","title":"potrf()","text":"<pre><code>potrf(tile: ndarray) -&gt; ndarray\n</code></pre> <p>POTRF task.</p> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def potrf(tile: np.ndarray) -&gt; np.ndarray:\n    \"\"\"POTRF task.\"\"\"\n    return np.linalg.cholesky(tile)\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.trsm","title":"trsm()","text":"<pre><code>trsm(lower: ndarray, block: ndarray) -&gt; ndarray\n</code></pre> <p>TRSM task.</p> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def trsm(lower: np.ndarray, block: np.ndarray) -&gt; np.ndarray:\n    \"\"\"TRSM task.\"\"\"\n    return np.linalg.solve(lower, block.T).T\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.syrk","title":"syrk()","text":"<pre><code>syrk(tile: ndarray, lower: ndarray) -&gt; ndarray\n</code></pre> <p>SYRK task.</p> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def syrk(tile: np.ndarray, lower: np.ndarray) -&gt; np.ndarray:\n    \"\"\"SYRK task.\"\"\"\n    return tile - np.dot(lower, lower.T)\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.gemm","title":"gemm()","text":"<pre><code>gemm(a: ndarray, b: ndarray, c: ndarray) -&gt; ndarray\n</code></pre> <p>GEMM task.</p> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def gemm(a: np.ndarray, b: np.ndarray, c: np.ndarray) -&gt; np.ndarray:\n    \"\"\"GEMM task.\"\"\"\n    return a - np.dot(b, c)\n</code></pre>"},{"location":"api/wf/cholesky/workflow/#webs.wf.cholesky.workflow.create_psd_matrix","title":"create_psd_matrix()","text":"<pre><code>create_psd_matrix(n: int) -&gt; ndarray\n</code></pre> <p>Create a positive semi-definite matrix.</p> <p>Parameters:</p> <ul> <li> <code>n</code>             (<code>int</code>)         \u2013          <p>Create an <code>n</code> x <code>n</code> square matrix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Random matrix that is positive semi-definite.</p> </li> </ul> Source code in <code>webs/wf/cholesky/workflow.py</code> <pre><code>def create_psd_matrix(n: int) -&gt; np.ndarray:\n    \"\"\"Create a positive semi-definite matrix.\n\n    Args:\n        n: Create an `n` x `n` square matrix.\n\n    Returns:\n        Random matrix that is positive semi-definite.\n    \"\"\"\n    psd = np.random.randn(n, n)\n    psd = np.dot(psd, psd.T)\n    psd += n * np.eye(n)\n    return psd\n</code></pre>"},{"location":"api/wf/docking/","title":"webs.wf.docking","text":"<code>webs/wf/docking/__init__.py</code> <p>Protein docking workflow.</p> <p>The code in this module was adapted from: ParslDock</p> <p>Protein docking aims to predict the orientation and position of two molecules, where one molecule is a protein, and the other is a protein or a smaller molecule (ligand). Docking is commonly used in structure-based drug design, as it can predict the strength of docking between target small molecule ligands (drugs) to target binding site (receptors).</p> <p>Need to install libGL and MGLTool The environment variable MGLTOOLS_HOME needs to be set prior to workflow execution. A Conda <code>environment.yaml</code> is provided within the original notebook. It is recommended to install dependencies this way. NOTE: Dependencies are not compatible with ARM64 architecture. It may be necessary to set <code>OMP_NUM_THREADS=1</code> with certain executors.</p> <p>Sample input data provided in ParslDock tutorial Input files needed include <code>dataset_orz_original_1k.csv</code> for the SMILES string, <code>1iep_receptor.pdbqt</code> as the input receptor and <code>set_element.tcl</code> as the tcl script path.</p> <p>Example command: <pre><code>python -m webs.run docking     --executor process-pool     --max-processes 40     --smi-file-name-ligand ${PWD}/dataset_orz_original_1k.csv     --receptor ${PWD}/1iep_receptor.pdbqt     --tcl-path ${PWD}/set_element.tcl\n</code></pre></p>"},{"location":"api/wf/docking/config/","title":"webs.wf.docking.config","text":"<code>webs/wf/docking/config.py</code>"},{"location":"api/wf/docking/config/#webs.wf.docking.config.DockingWorkflowConfig","title":"DockingWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Synthetic workflow configuration.</p>"},{"location":"api/wf/docking/config/#webs.wf.docking.config.DockingWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/docking/train/","title":"webs.wf.docking.train","text":"<code>webs/wf/docking/train.py</code> <p>Module adapted from <code>&lt;https://github.com/Parsl/parsl-docking-tutorial/blob/main/ml_functions.py&gt;</code>.</p>"},{"location":"api/wf/docking/train/#webs.wf.docking.train.MorganFingerprintTransformer","title":"MorganFingerprintTransformer","text":"<pre><code>MorganFingerprintTransformer(\n    length: int = 256, radius: int = 4\n)\n</code></pre> <p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Class that converts SMILES strings to fingerprint vectors.</p> Source code in <code>webs/wf/docking/train.py</code> <pre><code>def __init__(self, length: int = 256, radius: int = 4):\n    self.length = length\n    self.radius = radius\n</code></pre>"},{"location":"api/wf/docking/train/#webs.wf.docking.train.MorganFingerprintTransformer.fit","title":"fit()","text":"<pre><code>fit(\n    X: list[str], y: array[int] | None = None\n) -&gt; MorganFingerprintTransformer\n</code></pre> <p>Train model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>             (<code>list[str]</code>)         \u2013          <p>list of SMILES strings</p> </li> <li> <code>y</code>             (<code>array[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>array of true fingerprints. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MorganFingerprintTransformer</code> (            <code>MorganFingerprintTransformer</code> )        \u2013          <p>the trained model</p> </li> </ul> Source code in <code>webs/wf/docking/train.py</code> <pre><code>def fit(\n    self,\n    X: list[str],  # noqa: N803\n    y: np.array[int] | None = None,\n) -&gt; MorganFingerprintTransformer:\n    \"\"\"Train model.\n\n    Args:\n        X (list[str]): list of SMILES strings\n        y (np.array[int] | None): array of true fingerprints.\n            Defaults to None.\n\n    Returns:\n        MorganFingerprintTransformer: the trained model\n    \"\"\"\n    return self  # Don't need to do anything\n</code></pre>"},{"location":"api/wf/docking/train/#webs.wf.docking.train.MorganFingerprintTransformer.transform","title":"transform()","text":"<pre><code>transform(\n    X: list[str], y: array[int] | None = None\n) -&gt; array[int]\n</code></pre> <p>Compute the fingerprints.</p> <p>Parameters:</p> <ul> <li> <code>X</code>             (<code>list[str]</code>)         \u2013          <p>List of SMILES strings</p> </li> <li> <code>y</code>             (<code>array[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>array of true fingerprints. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>array[int]</code>         \u2013          <p>np.array[int]: Array of predicted fingerprints</p> </li> </ul> Source code in <code>webs/wf/docking/train.py</code> <pre><code>def transform(\n    self,\n    X: list[str],  # noqa: N803\n    y: np.array[int] | None = None,\n) -&gt; np.array[int]:\n    \"\"\"Compute the fingerprints.\n\n    Args:\n        X: List of SMILES strings\n        y (np.array[int] | None): array of true fingerprints.\n            Defaults to None.\n\n    Returns:\n        np.array[int]: Array of predicted fingerprints\n    \"\"\"\n    fps = []\n    for x in X:\n        fps.append(\n            compute_morgan_fingerprints(x, self.length, self.radius),\n        )\n\n    return fps\n</code></pre>"},{"location":"api/wf/docking/train/#webs.wf.docking.train.compute_morgan_fingerprints","title":"compute_morgan_fingerprints()","text":"<pre><code>compute_morgan_fingerprints(\n    smiles: str,\n    fingerprint_length: int,\n    fingerprint_radius: int,\n) -&gt; tuple[int, int]\n</code></pre> <p>Get Morgan Fingerprint of a specific SMILES string.</p> <p>Adapted from: <code>&lt;https://github.com/google-research/google-research/blob/&gt; dfac4178ccf521e8d6eae45f7b0a33a6a5b691ee/mol_dqn/chemgraph/dqn/deep_q_networks.py#L750&gt;</code> Args:   graph (str): The molecule as a SMILES string   fingerprint_length (int): Bit-length of fingerprint   fingerprint_radius (int): Radius used to compute fingerprint Returns:   np.array.shape = [hparams, fingerprint_length]. The Morgan fingerprint.</p> Source code in <code>webs/wf/docking/train.py</code> <pre><code>def compute_morgan_fingerprints(\n    smiles: str,\n    fingerprint_length: int,\n    fingerprint_radius: int,\n) -&gt; tuple[int, int]:\n    \"\"\"Get Morgan Fingerprint of a specific SMILES string.\n\n    Adapted from: `&lt;https://github.com/google-research/google-research/blob/&gt;\n    dfac4178ccf521e8d6eae45f7b0a33a6a5b691ee/mol_dqn/chemgraph/dqn/deep_q_networks.py#L750&gt;`\n    Args:\n      graph (str): The molecule as a SMILES string\n      fingerprint_length (int): Bit-length of fingerprint\n      fingerprint_radius (int): Radius used to compute fingerprint\n    Returns:\n      np.array.shape = [hparams, fingerprint_length]. The Morgan fingerprint.\n    \"\"\"\n    from rdkit import Chem\n    from rdkit import DataStructs\n    from rdkit.Chem import rdFingerprintGenerator\n\n    # Parse the molecule\n    molecule = Chem.MolFromSmiles(smiles)\n\n    # Compute the fingerprint\n    mfpgen = rdFingerprintGenerator.GetMorganGenerator(\n        radius=fingerprint_radius,\n        fpSize=fingerprint_length,\n    )\n    fingerprint = mfpgen.GetFingerprint(\n        molecule,\n    )\n    arr = np.zeros((1,), dtype=bool)\n\n    # ConvertToNumpyArray takes ~ 0.19 ms, while\n    # np.asarray takes ~ 4.69 ms\n    DataStructs.ConvertToNumpyArray(fingerprint, arr)\n    return arr\n</code></pre>"},{"location":"api/wf/docking/train/#webs.wf.docking.train.train_model","title":"train_model()","text":"<pre><code>train_model(training_data: DataFrame) -&gt; Pipeline\n</code></pre> <p>Train a machine learning model using Morgan Fingerprints.</p> <p>Parameters:</p> <ul> <li> <code>training_data</code>             (<code>DataFrame</code>)         \u2013          <p>Dataframe with a 'smiles' and 'score' column that contains molecule structure and docking score, respectfully.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          <p>sklearn.Pipeline: A trained model</p> </li> </ul> Source code in <code>webs/wf/docking/train.py</code> <pre><code>def train_model(training_data: pd.DataFrame) -&gt; Pipeline:\n    \"\"\"Train a machine learning model using Morgan Fingerprints.\n\n    Args:\n        training_data: Dataframe with a 'smiles' and 'score' column\n            that contains molecule structure and docking score, respectfully.\n\n    Returns:\n        sklearn.Pipeline: A trained model\n    \"\"\"\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.pipeline import Pipeline\n\n    model = Pipeline(\n        [\n            ('fingerprint', MorganFingerprintTransformer()),\n            (\n                'knn',\n                KNeighborsRegressor(\n                    n_neighbors=4,\n                    weights='distance',\n                    metric='jaccard',\n                    n_jobs=-1,\n                ),\n            ),  # n_jobs = -1 lets the model run all available processors\n        ],\n    )\n\n    return model.fit(training_data['smiles'], training_data['score'])\n</code></pre>"},{"location":"api/wf/docking/train/#webs.wf.docking.train.run_model","title":"run_model()","text":"<pre><code>run_model(model: Pipeline, smiles: list[str]) -&gt; DataFrame\n</code></pre> <p>Run a model on a list of smiles strings.</p> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>Pipeline</code>)         \u2013          <p>Trained model that takes SMILES strings as inputs</p> </li> <li> <code>smiles</code>             (<code>list[str]</code>)         \u2013          <p>List of molecules to evaluate</p> </li> </ul> <p>Returns:     A dataframe with the molecules and their predicted outputs</p> Source code in <code>webs/wf/docking/train.py</code> <pre><code>def run_model(model: Pipeline, smiles: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Run a model on a list of smiles strings.\n\n    Args:\n        model (sklearn.Pipeline): Trained model that\n            takes SMILES strings as inputs\n        smiles (list[str]): List of molecules to evaluate\n    Returns:\n        A dataframe with the molecules and their predicted outputs\n    \"\"\"\n    import pandas as pd\n\n    pred_y = model.predict(smiles)\n    return pd.DataFrame({'smiles': smiles, 'score': pred_y})\n</code></pre>"},{"location":"api/wf/docking/workflow/","title":"webs.wf.docking.workflow","text":"<code>webs/wf/docking/workflow.py</code>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.DockingWorkflow","title":"DockingWorkflow","text":"<pre><code>DockingWorkflow(config: DockingWorkflowConfig)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Protein docking workflow.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>DockingWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def __init__(self, config: DockingWorkflowConfig) -&gt; None:\n    self.smi_file_name_ligand = pathlib.Path(config.smi_file_name_ligand)\n    self.receptor = pathlib.Path(config.receptor)\n    self.tcl_path = pathlib.Path(config.tcl_path)\n    self.initial_simulations = config.initial_simulations\n    self.num_iterations = config.num_iterations\n    self.batch_size = config.batch_size\n    self.seed = config.seed\n    super().__init__()\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.DockingWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: DockingWorkflowConfig) -&gt; Self\n</code></pre> <p>Initialize a workflow from a config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>DockingWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Workflow.</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DockingWorkflowConfig) -&gt; Self:\n    \"\"\"Initialize a workflow from a config.\n\n    Args:\n        config: Workflow configuration.\n\n    Returns:\n        Workflow.\n    \"\"\"\n    return cls(config)\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.DockingWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Workflow task executor.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Run directory.</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:  # noqa: PLR0915\n    \"\"\"Run the workflow.\n\n    Args:\n        executor: Workflow task executor.\n        run_dir: Run directory.\n    \"\"\"\n    futures: list[TaskFuture[tuple[str, float] | str]] = []\n    train_data = []\n    smiles_simulated = []\n    train_output_file = pathlib.Path('training-results.json')\n\n    search_space = pd.read_csv(self.smi_file_name_ligand)\n    search_space = search_space[['TITLE', 'SMILES']]\n\n    # start with an initial set of random smiles\n    selected_smiles = search_space.sample(\n        self.initial_simulations,\n        random_state=self.seed,\n    )\n    for i in range(self.initial_simulations):\n        smiles = selected_smiles.iloc[i]['SMILES']\n\n        fname = uuid.uuid4().hex\n\n        pdb_file = pathlib.Path(f'{fname}.pdb')\n        output_pdb = pathlib.Path(f'{fname}-coords.pdb')\n        pdbqt_file = pathlib.Path(f'{fname}-coords.pdbqt')\n        vina_conf_file = pathlib.Path(f'{fname}-config.txt')\n        output_ligand_pdbqt = pathlib.Path(f'{fname}-out.pdb')\n\n        smi_future = executor.submit(smi_to_pdb, smiles, pdb_file=pdb_file)\n        element_future = executor.submit(\n            set_element,\n            smi_future,\n            output_pdb=output_pdb,\n            tcl_path=self.tcl_path,\n        )\n        pdbqt_future = executor.submit(\n            pdb_to_pdbqt,\n            element_future,\n            pdbqt_file=pdbqt_file,\n        )\n        config_future = executor.submit(\n            make_autodock_config,\n            self.receptor,\n            pdbqt_future,\n            vina_conf_file,\n            output_ligand_pdbqt,\n        )\n        dock_future = executor.submit(autodock_vina, config_future, smiles)\n        _ = executor.submit(\n            cleanup,\n            dock_future,\n            smi_future,\n            element_future,\n            pdbqt_future,\n            config_future,\n            output_ligand_pdbqt,\n        )\n\n        futures.append(dock_future)\n\n    # wait for all the futures to finish\n    while len(futures) &gt; 0:\n        future = next(as_completed(futures))\n        dock_score = future.result()\n\n        assert isinstance(dock_score, tuple), dock_score\n        smiles, score = dock_score\n\n        futures.remove(future)\n\n        logger.info(f'Computation for {smiles} succeeded: {score}')\n\n        train_data.append(\n            {\n                'smiles': smiles,\n                'score': score,\n                'time': monotonic(),\n            },\n        )\n        smiles_simulated.append(smiles)\n\n    training_df = pd.DataFrame(train_data)\n\n    # train model, run inference, and run more simulations\n    for i in range(self.num_iterations):\n        logger.info(f'\\nStarting batch {i}')\n        m = train_model(training_df)\n        predictions = run_model(m, search_space['SMILES'])\n        predictions.sort_values(\n            'score',\n            ascending=True,\n            inplace=True,\n        )\n\n        train_data = []\n        futures = []\n        batch_count = 0\n        for smiles in predictions['smiles']:\n            if smiles not in smiles_simulated:\n                fname = uuid.uuid4().hex\n\n                pdb_file = pathlib.Path(f'{fname}.pdb')\n                output_pdb = pathlib.Path(f'{fname}-coords.pdb')\n                pdbqt_file = pathlib.Path(f'{fname}-coords.pdbqt')\n                vina_conf_file = pathlib.Path(f'{fname}-config.txt')\n                output_ligand_pdbqt = pathlib.Path(f'{fname}-out.pdb')\n\n                smi_future = executor.submit(\n                    smi_to_pdb,\n                    smiles,\n                    pdb_file=pdb_file,\n                )\n                element_future = executor.submit(\n                    set_element,\n                    smi_future,\n                    output_pdb=output_pdb,\n                    tcl_path=self.tcl_path,\n                )\n                pdbqt_future = executor.submit(\n                    pdb_to_pdbqt,\n                    element_future,\n                    pdbqt_file=pdb_file,\n                )\n                config_future = executor.submit(\n                    make_autodock_config,\n                    self.receptor,\n                    pdbqt_future,\n                    vina_conf_file,\n                    output_ligand_pdbqt_file=output_ligand_pdbqt,\n                )\n                dock_future = executor.submit(\n                    autodock_vina,\n                    config_future,\n                    smiles,\n                )\n                executor.submit(\n                    cleanup,\n                    dock_future,\n                    smi_future,\n                    element_future,\n                    pdbqt_future,\n                    config_future,\n                    output_ligand_pdbqt,\n                )\n\n                futures.append(dock_future)\n\n                batch_count += 1\n\n            if batch_count &gt; self.batch_size:\n                break\n\n        # wait for all the workflows to complete\n        while len(futures) &gt; 0:\n            future = next(as_completed(futures))\n            dock_score = future.result()\n\n            assert isinstance(dock_score, tuple), dock_score\n\n            smiles, score = dock_score\n            futures.remove(future)\n\n            logger.info(f'Computation for {smiles} succeeded: {score}')\n\n            train_data.append(\n                {\n                    'smiles': smiles,\n                    'score': score,\n                    'time': monotonic(),\n                },\n            )\n            smiles_simulated.append(smiles)\n\n        training_df = pd.concat(\n            (training_df, pd.DataFrame(train_data)),\n            ignore_index=True,\n        )\n\n    training_df.to_json(train_output_file)\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.smi_to_pdb","title":"smi_to_pdb()","text":"<pre><code>smi_to_pdb(smiles: str, pdb_file: Path) -&gt; Path\n</code></pre> <p>Convert SMILES string to PDB representation.</p> <p>The conversion to PDB file will contain atomic coordinates that will be used for docking.</p> <p>Parameters:</p> <ul> <li> <code>smiles</code>             (<code>str</code>)         \u2013          <p>the molecule representation in SMILES format</p> </li> <li> <code>pdb_file</code>             (<code>Path</code>)         \u2013          <p>the path of the PDB file to create</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: The created PDB file</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def smi_to_pdb(smiles: str, pdb_file: pathlib.Path) -&gt; pathlib.Path:\n    \"\"\"Convert SMILES string to PDB representation.\n\n    The conversion to PDB file will contain atomic coordinates\n    that will be used for docking.\n\n    Args:\n        smiles (str): the molecule representation in\n            SMILES format\n        pdb_file (pathlib.Path): the path of the PDB file\n            to create\n\n    Returns:\n        pathlib.Path: The created PDB file\n    \"\"\"\n    from rdkit import Chem\n    from rdkit.Chem import AllChem\n\n    # Convert SMILES to RDKit molecule object\n    mol = Chem.MolFromSmiles(smiles)\n    # Add hydrogens to the molecule\n    mol = Chem.AddHs(mol)\n    # Generate a 3D conformation for the molecule\n    AllChem.EmbedMolecule(mol)\n    AllChem.MMFFOptimizeMolecule(mol)\n\n    # Write the molecule to a PDB file\n    writer = Chem.PDBWriter(pdb_file)\n    writer.write(mol)\n    writer.close()\n\n    return pdb_file\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.set_element","title":"set_element()","text":"<pre><code>set_element(\n    input_pdb: Path, output_pdb: Path, tcl_path: Path\n) -&gt; Path\n</code></pre> <p>Add coordinated to the PDB file using VMD.</p> <p>Parameters:</p> <ul> <li> <code>input_pdb</code>             (<code>Path</code>)         \u2013          <p>path of input PDB file.</p> </li> <li> <code>output_pdb</code>             (<code>Path</code>)         \u2013          <p>path to PDB file with atomic coordinates</p> </li> <li> <code>tcl_path</code>             (<code>Path</code>)         \u2013          <p>path to TCL script</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: the newly created PDB file path</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def set_element(\n    input_pdb: pathlib.Path,\n    output_pdb: pathlib.Path,\n    tcl_path: pathlib.Path,\n) -&gt; pathlib.Path:\n    \"\"\"Add coordinated to the PDB file using VMD.\n\n    Args:\n        input_pdb (pathlib.Path): path of input PDB file.\n        output_pdb (pathlib.Path): path to PDB file with atomic coordinates\n        tcl_path (pathlib.Path): path to TCL script\n\n    Returns:\n        pathlib.Path: the newly created PDB file path\n    \"\"\"\n    command = f'vmd -dispdev text -e {tcl_path} -args {input_pdb} {output_pdb}'\n\n    result = subprocess.check_output(command.split())\n    logger.info(result)\n    return output_pdb\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.pdb_to_pdbqt","title":"pdb_to_pdbqt()","text":"<pre><code>pdb_to_pdbqt(\n    pdb_file: Path, pdbqt_file: Path, ligand: bool = True\n) -&gt; Path\n</code></pre> <p>Convert PDB file to PDBQT format.</p> <p>PDBQT files are similar to the PDB format, but also includes connectivity information</p> <p>Parameters:</p> <ul> <li> <code>pdb_file</code>             (<code>Path</code>)         \u2013          <p>input PDB file to convert</p> </li> <li> <code>pdbqt_file</code>             (<code>Path</code>)         \u2013          <p>output converted PDBQT file</p> </li> <li> <code>ligand</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If the molecule is a ligand or not. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: the path to the created PDBQT file</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def pdb_to_pdbqt(\n    pdb_file: pathlib.Path,\n    pdbqt_file: pathlib.Path,\n    ligand: bool = True,\n) -&gt; pathlib.Path:\n    \"\"\"Convert PDB file to PDBQT format.\n\n    PDBQT files are similar to the PDB format, but also\n    includes connectivity information\n\n    Args:\n        pdb_file (pathlib.Path): input PDB file to convert\n        pdbqt_file (pathlib.Path): output converted PDBQT file\n        ligand (bool, optional): If the molecule is a ligand or not.\n            Defaults to True.\n\n    Returns:\n        pathlib.Path: the path to the created PDBQT file\n    \"\"\"\n    autodocktools_path: str | None = os.getenv('MGLTOOLS_HOME')\n    assert autodocktools_path is not None\n\n    script, flag = (\n        ('prepare_ligand4.py', 'l')\n        if ligand\n        else ('prepare_receptor4.py', 'r')\n    )\n\n    command = (\n        f\"{'python2.7'} \"\n        f\"\"\"{(Path(autodocktools_path)\n            / 'MGLToolsPckgs/AutoDockTools/Utilities24'\n            / script)} \"\"\"\n        f\" -{flag} {pdb_file} \"\n        f\" -o {pdbqt_file} \"\n        f\" -U nphs_lps_waters\"\n    )\n    result = subprocess.check_output(\n        command.split(),\n        cwd=pdb_file.parent,\n        encoding='utf-8',\n    )\n    logger.info(result)\n\n    return pdbqt_file\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.make_autodock_config","title":"make_autodock_config()","text":"<pre><code>make_autodock_config(\n    input_receptor_pdbqt_file: Path,\n    input_ligand_pdbqt_file: Path,\n    output_conf_file: Path,\n    output_ligand_pdbqt_file: Path,\n    center: tuple[float, float, float] = (\n        15.614,\n        53.38,\n        15.455,\n    ),\n    size: tuple[int, int, int] = (20, 20, 20),\n    exhaustiveness: int = 20,\n    num_modes: int = 20,\n    energy_range: int = 10,\n) -&gt; Path\n</code></pre> <p>Create configuration for AutoDock Vina.</p> <p>Create a configuration file for AutoDock Vina by describing the target receptor and setting coordinate bounds for the docking experiment.</p> <p>Parameters:</p> <ul> <li> <code>input_receptor_pdbqt_file</code>             (<code>Path</code>)         \u2013          <p>target receptor PDBQT file</p> </li> <li> <code>input_ligand_pdbqt_file</code>             (<code>Path</code>)         \u2013          <p>target ligand PDBQT file</p> </li> <li> <code>output_conf_file</code>             (<code>Path</code>)         \u2013          <p>the generated Vina conf file</p> </li> <li> <code>output_ligand_pdbqt_file</code>             (<code>Path</code>)         \u2013          <p>output ligand PDBQT file path</p> </li> <li> <code>center</code>             (<code>Tuple[float, float, float]</code>, default:                 <code>(15.614, 53.38, 15.455)</code> )         \u2013          <p>center coordinates. Defaults to (15.614, 53.380, 15.455).</p> </li> <li> <code>size</code>             (<code>Tuple[int, int, int]</code>, default:                 <code>(20, 20, 20)</code> )         \u2013          <p>size of the search space. Defaults to (20, 20, 20).</p> </li> <li> <code>exhaustiveness</code>             (<code>int</code>, default:                 <code>20</code> )         \u2013          <p>number of monte carlo simulations. Defaults to 20.</p> </li> <li> <code>num_modes</code>             (<code>int</code>, default:                 <code>20</code> )         \u2013          <p>number of binding modes. Defaults to 20.</p> </li> <li> <code>energy_range</code>             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>maximum energy difference between the best binding mode and the worst one displayed (kcal/mol). Defaults to 10.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: path of created output configuration file</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def make_autodock_config(  # noqa: PLR0913\n    input_receptor_pdbqt_file: pathlib.Path,\n    input_ligand_pdbqt_file: pathlib.Path,\n    output_conf_file: pathlib.Path,\n    output_ligand_pdbqt_file: pathlib.Path,\n    center: tuple[float, float, float] = (15.614, 53.380, 15.455),\n    size: tuple[int, int, int] = (20, 20, 20),\n    exhaustiveness: int = 20,\n    num_modes: int = 20,\n    energy_range: int = 10,\n) -&gt; pathlib.Path:\n    \"\"\"Create configuration for AutoDock Vina.\n\n    Create a configuration file for AutoDock Vina by describing\n    the target receptor and setting coordinate bounds for the\n    docking experiment.\n\n    Args:\n        input_receptor_pdbqt_file (pathlib.Path): target receptor PDBQT file\n        input_ligand_pdbqt_file (pathlib.Path): target ligand PDBQT file\n        output_conf_file (pathlib.Path): the generated Vina conf file\n        output_ligand_pdbqt_file (pathlib.Path): output ligand PDBQT file path\n        center (Tuple[float, float, float]): center coordinates.\n            Defaults to (15.614, 53.380, 15.455).\n        size (Tuple[int, int, int]): size of the search space.\n            Defaults to (20, 20, 20).\n        exhaustiveness (int, optional): number of monte carlo simulations.\n            Defaults to 20.\n        num_modes (int, optional): number of binding modes. Defaults to 20.\n        energy_range (int, optional): maximum energy difference between\n            the best binding mode and the worst one displayed (kcal/mol).\n            Defaults to 10.\n\n    Returns:\n        pathlib.Path: path of created output configuration file\n    \"\"\"\n    # Format configuration file\n    file_contents = (\n        f'receptor = {input_receptor_pdbqt_file}\\n'\n        f'ligand = {input_ligand_pdbqt_file}\\n'\n        f'center_x = {center[0]}\\n'\n        f'center_y = {center[1]}\\n'\n        f'center_z = {center[2]}\\n'\n        f'size_x = {size[0]}\\n'\n        f'size_y = {size[1]}\\n'\n        f'size_z = {size[2]}\\n'\n        f'exhaustiveness = {exhaustiveness}\\n'\n        f'num_modes = {num_modes}\\n'\n        f'energy_range = {energy_range}\\n'\n        f'out = {output_ligand_pdbqt_file}\\n'\n    )\n    # Write configuration file\n    with open(output_conf_file, 'w') as f:\n        f.write(file_contents)\n\n    return output_conf_file\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.autodock_vina","title":"autodock_vina()","text":"<pre><code>autodock_vina(\n    config_file: Path, smiles: str, num_cpu: int = 1\n) -&gt; tuple[str, float] | str\n</code></pre> <p>Compute the docking score.</p> <p>The docking score captures the potential energy change when the protein and ligand are docked. A strong binding is represented by a negative score, weaker (or no) binders are represented by positive scores.</p> <p>Parameters:</p> <ul> <li> <code>config_file</code>             (<code>Path</code>)         \u2013          <p>Vina configuration file</p> </li> <li> <code>smiles</code>             (<code>str</code>)         \u2013          <p>the SMILES string of molecule</p> </li> <li> <code>num_cpu</code>             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>number of CPUs to use. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, float] | str</code>         \u2013          <p>tuple[str, float] | str: the docking score for the associated molecule or the error statement</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def autodock_vina(\n    config_file: pathlib.Path,\n    smiles: str,\n    num_cpu: int = 1,\n) -&gt; tuple[str, float] | str:\n    \"\"\"Compute the docking score.\n\n    The docking score captures the potential energy change\n    when the protein and ligand are docked. A strong binding\n    is represented by a negative score, weaker (or no) binders\n    are represented by positive scores.\n\n    Args:\n        config_file (pathlib.Path): Vina configuration file\n        smiles (str): the SMILES string of molecule\n        num_cpu (int, optional): number of CPUs to use. Defaults to 1.\n\n    Returns:\n        tuple[str, float] | str: the docking score for the associated\n            molecule or the error statement\n    \"\"\"\n    autodock_vina_exe = 'vina'\n    try:\n        command = f'{autodock_vina_exe} --config {config_file} --cpu {num_cpu}'\n        result = subprocess.check_output(command.split(), encoding='utf-8')\n\n        # find the last row of the table and extract the affinity score\n        result_list = result.split('\\n')\n        last_row = result_list[-3]\n        score = last_row.split()\n        return (smiles, float(score[1]))\n    except subprocess.CalledProcessError as e:\n        return (\n            f\"Command '{e.cmd}' returned non-zero exit status {e.returncode}\"\n        )\n    except Exception as e:\n        return f'Error: {e}'\n</code></pre>"},{"location":"api/wf/docking/workflow/#webs.wf.docking.workflow.cleanup","title":"cleanup()","text":"<pre><code>cleanup(\n    dock_result: tuple[str, float] | str,\n    pdb: Path,\n    pdb_coords: Path,\n    pdb_qt: Path,\n    autodoc_config: Path,\n    docking: Path,\n) -&gt; None\n</code></pre> <p>Cleanup output directory.</p> <p>Parameters:</p> <ul> <li> <code>dock_result</code>             (<code>tuple[str, float] | str</code>)         \u2013          <p>Docking score output</p> </li> <li> <code>pdb</code>             (<code>Path</code>)         \u2013          <p>pdb file generated from SMILES string</p> </li> <li> <code>pdb_coords</code>             (<code>Path</code>)         \u2013          <p>pdb file with atomic coordinates</p> </li> <li> <code>pdb_qt</code>             (<code>Path</code>)         \u2013          <p>pdqt file</p> </li> <li> <code>autodoc_config</code>             (<code>Path</code>)         \u2013          <p>autodock vina config file</p> </li> <li> <code>docking</code>             (<code>Path</code>)         \u2013          <p>output ligand file</p> </li> </ul> Source code in <code>webs/wf/docking/workflow.py</code> <pre><code>def cleanup(  # noqa: PLR0913\n    dock_result: tuple[str, float] | str,\n    pdb: pathlib.Path,\n    pdb_coords: pathlib.Path,\n    pdb_qt: pathlib.Path,\n    autodoc_config: pathlib.Path,\n    docking: pathlib.Path,\n) -&gt; None:\n    \"\"\"Cleanup output directory.\n\n    Args:\n        dock_result (tuple[str, float] | str): Docking score output\n        pdb (pathlib.Path): pdb file generated from SMILES string\n        pdb_coords (pathlib.Path): pdb file with atomic coordinates\n        pdb_qt (pathlib.Path): pdqt file\n        autodoc_config (pathlib.Path): autodock vina config file\n        docking (pathlib.Path): output ligand file\n    \"\"\"\n    pdb.unlink(missing_ok=True)\n    pdb_coords.unlink(missing_ok=True)\n    pdb_qt.unlink(missing_ok=True)\n    autodoc_config.unlink(missing_ok=True)\n    docking.unlink(missing_ok=True)\n</code></pre>"},{"location":"api/wf/fedlearn/","title":"webs.wf.fedlearn","text":"<code>webs/wf/fedlearn/__init__.py</code> <p>Federated learning workflow.</p> <p>This submodule implements a Federated Learning (FL) workflow.</p> <p>At a high level, FL is a paradigm for performing deep learning on decentralized data hosted on decentralized devices (e.g., Internet-of-Things devices). Each of these devices separately train their own copy of a shared model architecture. Their locally-trained copies are aggregated over time to update a global model which has essentially been able to learn over all the data in the system without directly accessing any data.</p> <p>For more information on FL as a whole, please refer to the following paper.</p> <p>This FedLearn workflow simulates FL with simple deep neural networks mnd select set of baseline datasets.</p>"},{"location":"api/wf/fedlearn/#webs.wf.fedlearn--example","title":"Example","text":"<pre><code>python -m webs.run fedlearn --executor process-pool --data-name mnist --data-root data/ --data-download true --num-rounds 1 --participation 0.5\n</code></pre>"},{"location":"api/wf/fedlearn/config/","title":"webs.wf.fedlearn.config","text":"<code>webs/wf/fedlearn/config.py</code>"},{"location":"api/wf/fedlearn/config/#webs.wf.fedlearn.config.DataChoices","title":"DataChoices","text":"<p>             Bases: <code>Enum</code></p> <p>Dataset options.</p>"},{"location":"api/wf/fedlearn/config/#webs.wf.fedlearn.config.FedLearnWorkflowConfig","title":"FedLearnWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Config for the Federated Learning workflow.</p>"},{"location":"api/wf/fedlearn/config/#webs.wf.fedlearn.config.FedLearnWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/fedlearn/modules/","title":"webs.wf.fedlearn.modules","text":"<code>webs/wf/fedlearn/modules.py</code>"},{"location":"api/wf/fedlearn/modules/#webs.wf.fedlearn.modules.CifarModule","title":"CifarModule","text":"<pre><code>CifarModule(num_classes: int)\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Cifar model.</p> <p>Source: https://www.kaggle.com/code/shadabhussain/cifar-10-cnn-using-pytorch</p> Source code in <code>webs/wf/fedlearn/modules.py</code> <pre><code>def __init__(self, num_classes: int):\n    super().__init__()\n    self.num_classes = num_classes\n    self.network = nn.Sequential(\n        nn.Conv2d(3, 32, kernel_size=3, padding=1),\n        nn.ReLU(),\n        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2),  # output: 64 x 16 x 16\n        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2),  # output: 128 x 8 x 8\n        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2),  # output: 256 x 4 x 4\n        nn.Flatten(),\n        nn.Linear(256 * 4 * 4, 1024),\n        nn.ReLU(),\n        nn.Linear(1024, 512),\n        nn.ReLU(),\n        nn.Linear(512, num_classes),\n    )\n</code></pre>"},{"location":"api/wf/fedlearn/modules/#webs.wf.fedlearn.modules.CifarModule.forward","title":"forward()","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass.</p> Source code in <code>webs/wf/fedlearn/modules.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\"\"\"\n    return self.network(x)\n</code></pre>"},{"location":"api/wf/fedlearn/modules/#webs.wf.fedlearn.modules.MnistModule","title":"MnistModule","text":"<pre><code>MnistModule()\n</code></pre> <p>             Bases: <code>Module</code></p> <p>Model for MNIST and FashionMNIST data.</p> Source code in <code>webs/wf/fedlearn/modules.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.flattener = nn.Flatten()\n    self.fc1 = nn.Linear(28 * 28, 56 * 56)\n    self.fc2 = nn.Linear(56 * 56, 28 * 28)\n    self.fc3 = nn.Linear(28 * 28, 14 * 14)\n    self.classifier = nn.Linear(14 * 14, 10)\n</code></pre>"},{"location":"api/wf/fedlearn/modules/#webs.wf.fedlearn.modules.MnistModule.forward","title":"forward()","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Forward pass.</p> Source code in <code>webs/wf/fedlearn/modules.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass.\"\"\"\n    x = self.flattener(x)\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = F.relu(self.fc3(x))\n    x = self.classifier(x)\n    return x\n</code></pre>"},{"location":"api/wf/fedlearn/modules/#webs.wf.fedlearn.modules.create_model","title":"create_model()","text":"<pre><code>create_model(data: DataChoices) -&gt; Module\n</code></pre> <p>Create a model suitable for the dataset choice.</p> Note <p>The currently supported dataset options are <code>MNIST</code>, <code>FashionMNIST</code>, <code>CIFAR10</code>, and <code>CIFAR100</code>.</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>DataChoices</code>)         \u2013          <p>Name of dataset that will be used for training (and testing).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Module</code>         \u2013          <p>PyTorch module to be used for FL workflow.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If an unsupported value for <code>data</code> is provided.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/modules.py</code> <pre><code>def create_model(data: DataChoices) -&gt; nn.Module:\n    \"\"\"Create a model suitable for the dataset choice.\n\n    Note:\n        The currently supported dataset options are `MNIST`, `FashionMNIST`,\n        `CIFAR10`, and `CIFAR100`.\n\n    Args:\n        data: Name of dataset that will be used for training (and testing).\n\n    Returns:\n        PyTorch module to be used for FL workflow.\n\n    Raises:\n        ValueError: If an unsupported value for `data` is provided.\n    \"\"\"\n    name = data.value.lower()\n\n    if name == 'cifar10':\n        return CifarModule(10)\n    elif name == 'cifar100':\n        return CifarModule(100)\n    elif name in ('fmnist', 'mnist'):\n        return MnistModule()\n    else:\n        raise ValueError(\n            f'Unknown dataset \"{data.value}\". Supported options are '\n            \"'cifar10', 'cifar100', 'fmnist', and 'mnist'.\",\n        )\n</code></pre>"},{"location":"api/wf/fedlearn/modules/#webs.wf.fedlearn.modules.load_data","title":"load_data()","text":"<pre><code>load_data(\n    data_name: DataChoices,\n    root: Path,\n    train: bool,\n    download: bool = False,\n) -&gt; Dataset\n</code></pre> <p>Load dataset to train with for FL workflow.</p> <p>Parameters:</p> <ul> <li> <code>data_name</code>             (<code>DataChoices</code>)         \u2013          <p>Dataset choice.</p> </li> <li> <code>root</code>             (<code>Path</code>)         \u2013          <p>Root dataset directory.</p> </li> <li> <code>train</code>             (<code>bool</code>)         \u2013          <p>Flag for if training.</p> </li> <li> <code>download</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Should the dataset be downloaded.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dataset</code> (            <code>Dataset</code> )        \u2013          <p>description</p> </li> </ul> Source code in <code>webs/wf/fedlearn/modules.py</code> <pre><code>def load_data(\n    data_name: DataChoices,\n    root: pathlib.Path,\n    train: bool,\n    download: bool = False,\n) -&gt; Dataset:\n    \"\"\"Load dataset to train with for FL workflow.\n\n    Args:\n        data_name: Dataset choice.\n        root: Root dataset directory.\n        train: Flag for if training.\n        download: Should the dataset be downloaded.\n\n    Returns:\n        Dataset: _description_\n    \"\"\"\n    kwargs = {\n        'root': root,\n        'train': train,\n        'transform': transforms.ToTensor(),\n        'download': download,\n    }\n    name = data_name.value.lower()\n    if name == 'cifar10':\n        return torchvision.datasets.CIFAR10(**kwargs)\n    elif name == 'cifar100':\n        return torchvision.datasets.CIFAR100(**kwargs)\n    elif name == 'fmnist':\n        return torchvision.datasets.FashionMNIST(**kwargs)\n    elif name == 'mnist':\n        return torchvision.datasets.MNIST(**kwargs)\n    else:\n        raise ValueError(f'Unknown dataset: {data_name}.')\n</code></pre>"},{"location":"api/wf/fedlearn/tasks/","title":"webs.wf.fedlearn.tasks","text":"<code>webs/wf/fedlearn/tasks.py</code>"},{"location":"api/wf/fedlearn/tasks/#webs.wf.fedlearn.tasks.no_local_train","title":"no_local_train()","text":"<pre><code>no_local_train(\n    client: Client,\n    round_idx: int,\n    epochs: int,\n    batch_size: int,\n    lr: float,\n    device: device,\n) -&gt; list[Result]\n</code></pre> <p>No-op version of local_train.</p> <p>Returns:</p> <ul> <li> <code>list[Result]</code>         \u2013          <p>Empty result list.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/tasks.py</code> <pre><code>def no_local_train(  # noqa: PLR0913\n    client: Client,\n    round_idx: int,\n    epochs: int,\n    batch_size: int,\n    lr: float,\n    device: torch.device,\n) -&gt; list[Result]:\n    \"\"\"No-op version of [local_train][webs.wf.fedlearn.tasks.local_train].\n\n    Returns:\n        Empty result list.\n    \"\"\"\n    results: list[Result] = []\n    return results\n</code></pre>"},{"location":"api/wf/fedlearn/tasks/#webs.wf.fedlearn.tasks.local_train","title":"local_train()","text":"<pre><code>local_train(\n    client: Client,\n    round_idx: int,\n    epochs: int,\n    batch_size: int,\n    lr: float,\n    device: device,\n) -&gt; list[Result]\n</code></pre> <p>Local training job.</p> <p>Parameters:</p> <ul> <li> <code>client</code>             (<code>Client</code>)         \u2013          <p>The client to train.</p> </li> <li> <code>round_idx</code>             (<code>int</code>)         \u2013          <p>The current round number.</p> </li> <li> <code>epochs</code>             (<code>int</code>)         \u2013          <p>Number of epochs.</p> </li> <li> <code>batch_size</code>             (<code>int</code>)         \u2013          <p>Batch size when iterating through data.</p> </li> <li> <code>lr</code>             (<code>float</code>)         \u2013          <p>Learning rate.</p> </li> <li> <code>device</code>             (<code>device</code>)         \u2013          <p>Backend hardware to train with.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Result]</code>         \u2013          <p>List of results that record the training history.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/tasks.py</code> <pre><code>def local_train(  # noqa: PLR0913\n    client: Client,\n    round_idx: int,\n    epochs: int,\n    batch_size: int,\n    lr: float,\n    device: torch.device,\n) -&gt; list[Result]:\n    \"\"\"Local training job.\n\n    Args:\n        client (Client): The client to train.\n        round_idx (int): The current round number.\n        epochs (int): Number of epochs.\n        batch_size (int): Batch size when iterating through data.\n        lr (float): Learning rate.\n        device (torch.device): Backend hardware to train with.\n\n    Returns:\n        List of results that record the training history.\n    \"\"\"\n    from datetime import datetime\n\n    results: list[Result] = []\n    client.model.to(device)\n    client.model.train()\n    optimizer = torch.optim.SGD(client.model.parameters(), lr=lr)\n    loader = DataLoader(client.data, batch_size=batch_size)\n\n    for epoch in range(epochs):\n        epoch_results = []\n        log_every_n_batches = 100\n        running_loss = 0.0\n\n        for batch_idx, batch in enumerate(loader):\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            preds = client.model(inputs)\n            loss = F.cross_entropy(preds, targets)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            if batch_idx % log_every_n_batches == (log_every_n_batches - 1):\n                epoch_results.append(\n                    {\n                        'time': datetime.now(),\n                        'client_idx': client.idx,\n                        'round_idx': round_idx,\n                        'epoch': epoch,\n                        'batch_idx': batch_idx,\n                        'train_loss': running_loss / log_every_n_batches,\n                    },\n                )\n                running_loss = 0.0\n\n        results.extend(epoch_results)\n\n    return results\n</code></pre>"},{"location":"api/wf/fedlearn/tasks/#webs.wf.fedlearn.tasks.test_model","title":"test_model()","text":"<pre><code>test_model(\n    model: Module,\n    data: Dataset,\n    round_idx: int,\n    device: device,\n) -&gt; Result\n</code></pre> <p>Evaluate a model.</p> Source code in <code>webs/wf/fedlearn/tasks.py</code> <pre><code>def test_model(\n    model: nn.Module,\n    data: Dataset,\n    round_idx: int,\n    device: torch.device,\n) -&gt; Result:\n    \"\"\"Evaluate a model.\"\"\"\n    from datetime import datetime\n\n    model.eval()\n    with torch.no_grad():\n        model.to(device)\n        loader = DataLoader(data, batch_size=1)\n        total_loss, n_batches = 0.0, 0\n        for batch in loader:\n            inputs, targets = batch\n            inputs, targets = inputs.to(device), targets.to(device)\n            preds = model(inputs)\n            loss = F.cross_entropy(preds, targets)\n\n            total_loss += loss.item()\n            n_batches += 1\n\n    res: Result = {\n        'time': datetime.now(),\n        'round_idx': round_idx,\n        'test_loss': total_loss / n_batches,\n    }\n    return res\n</code></pre>"},{"location":"api/wf/fedlearn/types/","title":"webs.wf.fedlearn.types","text":"<code>webs/wf/fedlearn/types.py</code>"},{"location":"api/wf/fedlearn/types/#webs.wf.fedlearn.types.ClientID","title":"ClientID  <code>module-attribute</code>","text":"<pre><code>ClientID: TypeAlias = int\n</code></pre> <p>Integer IDs for <code>Client</code> instances.</p>"},{"location":"api/wf/fedlearn/types/#webs.wf.fedlearn.types.Result","title":"Result  <code>module-attribute</code>","text":"<pre><code>Result: TypeAlias = Dict[str, Any]\n</code></pre> <p>Result type for each FL epoch, round, and workflow.</p>"},{"location":"api/wf/fedlearn/types/#webs.wf.fedlearn.types.Client","title":"Client","text":"<p>             Bases: <code>BaseModel</code></p> <p>Client class.</p>"},{"location":"api/wf/fedlearn/types/#webs.wf.fedlearn.types.Client.idx","title":"idx  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>idx: ClientID = Field(description='Client ID')\n</code></pre> <p>Client ID.</p>"},{"location":"api/wf/fedlearn/types/#webs.wf.fedlearn.types.Client.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Module = Field(description=\"Client's local model\")\n</code></pre> <p>Client's local model.</p>"},{"location":"api/wf/fedlearn/types/#webs.wf.fedlearn.types.Client.data","title":"data  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data: Optional[Subset] = Field(\n    description=\"The subset of data this client will train on.\"\n)\n</code></pre> <p>The subset of data this client will train on.</p>"},{"location":"api/wf/fedlearn/utils/","title":"webs.wf.fedlearn.utils","text":"<code>webs/wf/fedlearn/utils.py</code>"},{"location":"api/wf/fedlearn/utils/#webs.wf.fedlearn.utils.create_clients","title":"create_clients()","text":"<pre><code>create_clients(\n    num_clients: int,\n    data_name: DataChoices,\n    train: bool,\n    train_data: Dataset,\n    data_alpha: float,\n    rng: Generator,\n) -&gt; list[Client]\n</code></pre> <p>Create many clients with disjoint sets of data.</p> <p>Parameters:</p> <ul> <li> <code>num_clients</code>             (<code>int</code>)         \u2013          <p>Number of clients to create.</p> </li> <li> <code>data_name</code>             (<code>DataChoices</code>)         \u2013          <p>The name of the data used. Used for initializing the corresponding model.</p> </li> <li> <code>train</code>             (<code>bool</code>)         \u2013          <p>If the workflow is using the no-op training task, then this function skips the step for giving each client their own subset of data.</p> </li> <li> <code>train_data</code>             (<code>Dataset</code>)         \u2013          <p>The original dataset that will be split across the clients.</p> </li> <li> <code>data_alpha</code>             (<code>float</code>)         \u2013          <p>The Dirichlet distribution alpha value for the number of samples across clients.</p> </li> <li> <code>rng</code>             (<code>Generator</code>)         \u2013          <p>Random number generator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[Client]</code>         \u2013          <p>List of clients for the workflow.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/utils.py</code> <pre><code>def create_clients(  # noqa: PLR0913\n    num_clients: int,\n    data_name: DataChoices,\n    train: bool,\n    train_data: Dataset,\n    data_alpha: float,\n    rng: Generator,\n) -&gt; list[Client]:\n    \"\"\"Create many clients with disjoint sets of data.\n\n    Args:\n        num_clients: Number of clients to create.\n        data_name: The name of the data used. Used for initializing the\n            corresponding model.\n        train: If the workflow is using the no-op training task, then this\n            function skips the step for giving each client their own subset\n            of data.\n        train_data: The original dataset that will be split across the clients.\n        data_alpha: The\n            [Dirichlet](https://en.wikipedia.org/wiki/Dirichlet_distribution)\n            distribution alpha value for the number of samples across clients.\n        rng (Generator): Random number generator.\n\n    Returns:\n        List of clients for the workflow.\n    \"\"\"\n    client_ids = list(range(num_clients))\n\n    if train:\n        client_indices: dict[int, list[int]] = {idx: [] for idx in client_ids}\n\n        alpha = [data_alpha] * num_clients\n        client_popularity = rng.dirichlet(alpha)\n\n        for data_idx, _ in enumerate(train_data):\n            selected_client: ClientID = rng.choice(\n                client_ids,\n                size=1,\n                p=client_popularity,\n            )[0]\n            client_indices[selected_client].append(data_idx)\n\n        client_subsets = {\n            idx: Subset(train_data, client_indices[idx]) for idx in client_ids\n        }\n    else:\n        client_subsets = {idx: None for idx in client_ids}\n\n    clients = []\n    for idx in client_ids:\n        client = Client(\n            idx=idx,\n            model=create_model(data_name),\n            data=client_subsets[idx],\n        )\n        clients.append(client)\n\n    return clients\n</code></pre>"},{"location":"api/wf/fedlearn/utils/#webs.wf.fedlearn.utils.unweighted_module_avg","title":"unweighted_module_avg()","text":"<pre><code>unweighted_module_avg(\n    selected_clients: list[Client],\n) -&gt; OrderedDict[str, Tensor]\n</code></pre> <p>Compute the unweighted average of models.</p> Source code in <code>webs/wf/fedlearn/utils.py</code> <pre><code>def unweighted_module_avg(\n    selected_clients: list[Client],\n) -&gt; OrderedDict[str, torch.Tensor]:\n    \"\"\"Compute the unweighted average of models.\"\"\"\n    models = [client.model for client in selected_clients]\n    w = 1 / len(selected_clients)\n\n    with torch.no_grad():\n        avg_weights = OrderedDict()\n        for model in models:\n            for name, value in model.state_dict().items():\n                partial = w * torch.clone(value)\n                if name not in avg_weights:\n                    avg_weights[name] = partial\n                else:\n                    avg_weights[name] += partial\n\n    return avg_weights\n</code></pre>"},{"location":"api/wf/fedlearn/workflow/","title":"webs.wf.fedlearn.workflow","text":"<code>webs/wf/fedlearn/workflow.py</code>"},{"location":"api/wf/fedlearn/workflow/#webs.wf.fedlearn.workflow.FedLearnWorkflow","title":"FedLearnWorkflow","text":"<pre><code>FedLearnWorkflow(\n    num_clients: int,\n    num_rounds: int,\n    data_name: DataChoices,\n    batch_size: int,\n    epochs: int,\n    lr: float,\n    data_dir: str = \"data/\",\n    device: str = \"cpu\",\n    download: bool = False,\n    train: bool = True,\n    test: bool = True,\n    data_alpha: float = 100000.0,\n    participation_prob: float = 1.0,\n    seed: int | None = None,\n)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Federated learning workflow.</p> Tip <p>Download the data of interest ahead of running the workflow (i.e., set <code>download=False</code>).</p> <p>Parameters:</p> <ul> <li> <code>num_clients</code>             (<code>int</code>)         \u2013          <p>Number of simulated clients in the FL workflow.</p> </li> <li> <code>num_rounds</code>             (<code>int</code>)         \u2013          <p>Number of aggregation rounds to perform.</p> </li> <li> <code>data_name</code>             (<code>DataChoices</code>)         \u2013          <p>Data (and corresponding model) to use.</p> </li> <li> <code>batch_size</code>             (<code>int</code>)         \u2013          <p>Batch size used for local training across all clients.</p> </li> <li> <code>epochs</code>             (<code>int</code>)         \u2013          <p>Number of epochs used during local training on all the clients.</p> </li> <li> <code>lr</code>             (<code>float</code>)         \u2013          <p>Learning rate used during local training on all the clients.</p> </li> <li> <code>data_dir</code>             (<code>str</code>, default:                 <code>'data/'</code> )         \u2013          <p>Root directory where the dataset is stored or where you wish to download the data (i.e., <code>download=True</code>).</p> </li> <li> <code>device</code>             (<code>str</code>, default:                 <code>'cpu'</code> )         \u2013          <p>Device to use for model training (e.g., <code>'cuda'</code>, <code>'cpu'</code>, <code>'mps'</code>).</p> </li> <li> <code>download</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If <code>True</code>, the dataset will be downloaded to the <code>root</code> arg directory. If <code>False</code> (default), the workflow will expect the data to already be downloaded.</p> </li> <li> <code>train</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If <code>True</code> (default), the local training will be run. If `False, then a no-op version of the workflow will be performed where no training is done. This is useful for debugging purposes.</p> </li> <li> <code>test</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If <code>True</code> (default), model testing is done at the end of each aggregation round.</p> </li> <li> <code>data_alpha</code>             (<code>float</code>, default:                 <code>100000.0</code> )         \u2013          <p>The number of data samples across clients is defined by a Dirichlet distribution. This value is used to define the uniformity of the amount of data samples across all clients. When data alpha is large, then the number of data samples across clients is uniform (default). When the value is very small, then the sample distribution becomes more non-uniform. Note: this value must be greater than 0.</p> </li> <li> <code>participation_prob</code>             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>The portion of clients that participate in an aggregation round. If set to 1.0, then all clients participate in each round; if 0.5 then half of the clients, and so on. At least one client will be selected regardless of this value and the number of clients.</p> </li> <li> <code>seed</code>             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Seed for reproducibility.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/workflow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    num_clients: int,\n    num_rounds: int,\n    data_name: DataChoices,\n    batch_size: int,\n    epochs: int,\n    lr: float,\n    data_dir: str = 'data/',\n    device: str = 'cpu',\n    download: bool = False,\n    train: bool = True,\n    test: bool = True,\n    data_alpha: float = 1e5,\n    participation_prob: float = 1.0,\n    seed: int | None = None,\n) -&gt; None:\n    self.rng = np.random.default_rng(seed)\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    self.data_name = data_name\n    self.global_model = create_model(self.data_name)\n\n    self.train, self.test = train, test\n    self.train_data, self.test_data = None, None\n    root = pathlib.Path(data_dir)\n    if self.train:\n        self.train_data = load_data(\n            self.data_name,\n            root,\n            train=True,\n            download=download,\n        )\n    if self.test:\n        self.test_data = load_data(\n            self.data_name,\n            root,\n            train=False,\n            download=download,\n        )\n\n    self.device = torch.device(device)\n    self.epochs = epochs\n    self.batch_size = batch_size\n    self.lr = lr\n\n    self.participation_prob = participation_prob\n\n    self.num_rounds = num_rounds\n    self.num_clients = num_clients\n    if data_alpha &lt;= 0:\n        raise ValueError('Argument `data_alpha` must be greater than 0.')\n    self.data_alpha = data_alpha\n\n    logger.log(WORK_LOG_LEVEL, 'Creating clients')\n    self.clients = create_clients(\n        self.num_clients,\n        self.data_name,\n        self.train,\n        self.train_data,\n        self.data_alpha,\n        self.rng,\n    )\n\n    _setup_platform()\n\n    super().__init__()\n</code></pre>"},{"location":"api/wf/fedlearn/workflow/#webs.wf.fedlearn.workflow.FedLearnWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: FedLearnWorkflowConfig) -&gt; Self\n</code></pre> <p>Load and initialize the workflow from a command line config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>FedLearnWorkflowConfig</code>)         \u2013          <p>Config instance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Instance of the workflow.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: FedLearnWorkflowConfig) -&gt; Self:\n    \"\"\"Load and initialize the workflow from a command line config.\n\n    Args:\n        config (FedLearnWorkflowConfig): Config instance.\n\n    Returns:\n        Instance of the workflow.\n    \"\"\"\n    return cls(\n        num_clients=config.num_clients,\n        num_rounds=config.num_rounds,\n        device=config.device,\n        epochs=config.batch_size,\n        batch_size=config.batch_size,\n        lr=config.lr,\n        data_name=config.data_name,\n        data_dir=config.data_root,\n        download=config.data_download,\n        train=config.train,\n        test=config.test,\n        participation_prob=config.participation,\n        seed=config.seed,\n    )\n</code></pre>"},{"location":"api/wf/fedlearn/workflow/#webs.wf.fedlearn.workflow.FedLearnWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Executor to launch jobs/tasks for the workflow.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Directory for run outputs.</p> </li> </ul> Source code in <code>webs/wf/fedlearn/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:\n    \"\"\"Run the workflow.\n\n    Args:\n        executor: Executor to launch jobs/tasks for the workflow.\n        run_dir: Directory for run outputs.\n    \"\"\"\n    results = []\n    for round_idx in range(self.num_rounds):\n        preface = f'({round_idx+1}/{self.num_rounds})'\n        logger.log(\n            WORK_LOG_LEVEL,\n            f'{preface} Starting local training for this round.',\n        )\n\n        train_result = self._federated_round(round_idx, executor, run_dir)\n        results.extend(train_result)\n\n        if self.test_data is not None:\n            logger.log(\n                WORK_LOG_LEVEL,\n                f'{preface} Starting the test for the global model.',\n            )\n            test_result = executor.submit(\n                test_model,\n                self.global_model,\n                self.test_data,\n                round_idx,\n                self.device,\n            ).result()\n            logger.log(\n                WORK_LOG_LEVEL,\n                f\"{preface} Finished testing with test_loss=\"\n                f\"{test_result['test_loss']}\",\n            )\n</code></pre>"},{"location":"api/wf/mapreduce/","title":"webs.wf.mapreduce","text":"<code>webs/wf/mapreduce/__init__.py</code> <p>Mapreduce workflow.</p> <p>Counts words in a text corpus (randomly generated or the Enron email dataset) using a mapreduce strategy.</p>"},{"location":"api/wf/mapreduce/#webs.wf.mapreduce--setup","title":"Setup","text":"<p>This workflow does not require any external libraries.</p> <p>Executing the workflow with the Enron dataset requires downloading the maildir folder from https://www.cs.cmu.edu/~enron/.</p>"},{"location":"api/wf/mapreduce/#webs.wf.mapreduce--examples","title":"Examples","text":"<p>To see all parameters, run the following command: <pre><code>python -m webs.run mapreduce --help\n</code></pre></p> <p>The following commands work on any platform, but the specific results were recorded on an M3 MacBook Air.</p> <ol> <li>Simple configuration: <pre><code>python -m webs.run mapreduce --executor thread-pool --mode random --map-task-count 1\n</code></pre> <pre><code>Result:\n  Total words: 500\n  Runtime: 0.01s\n</code></pre></li> <li>Increased map task count:    Optionally, specify the number of most frequent words to save and the    output file name:    <pre><code>python -m webs.run mapreduce --executor thread-pool --mode random --map-task-count 10 --n-freq 20 --out my-out.txt\n</code></pre> <pre><code>Result:\n  Total words: 5000\n  Runtime: 0.01s\n</code></pre></li> <li>Longer paragraphs with longer word length: <pre><code>python -m webs.run mapreduce --executor thread-pool --mode random --map-task-count 10 --word-count 1000000 --word-len-min 2 --word-len-max 2\n</code></pre> <pre><code>Result:\n  Total words: 10000000\n  Runtime: 7.09s\n</code></pre></li> <li>Invalid word length range: <pre><code>python -m webs.run mapreduce --executor thread-pool --mode random --map-task-count 10 --word-count 1000000 --word-len-min 5 --word-len-max 1\n</code></pre> <pre><code>Result:\n  Error: ValueError: empty range for randrange() (5, 2, -3)\n</code></pre></li> <li>Use the Enron email dataset:    This requires specifying the root path of the maildir through <code>--mail-dir</code>.    <pre><code>python -m webs.run mapreduce --executor thread-pool --mode enron --map-task-count 10 --mail-dir ~/Downloads/maildir\n</code></pre> <pre><code>Result:\n  Runtime: 52.12s\n</code></pre></li> </ol>"},{"location":"api/wf/mapreduce/config/","title":"webs.wf.mapreduce.config","text":"<code>webs/wf/mapreduce/config.py</code>"},{"location":"api/wf/mapreduce/config/#webs.wf.mapreduce.config.MapreduceWorkflowConfig","title":"MapreduceWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Mapreduce workflow configuration.</p>"},{"location":"api/wf/mapreduce/config/#webs.wf.mapreduce.config.MapreduceWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/mapreduce/utils/","title":"webs.wf.mapreduce.utils","text":"<code>webs/wf/mapreduce/utils.py</code>"},{"location":"api/wf/mapreduce/utils/#webs.wf.mapreduce.utils.generate_word","title":"generate_word()","text":"<pre><code>generate_word(word_min_len: int, word_max_len: int) -&gt; str\n</code></pre> <p>Generate a random word.</p> Source code in <code>webs/wf/mapreduce/utils.py</code> <pre><code>def generate_word(word_min_len: int, word_max_len: int) -&gt; str:\n    \"\"\"Generate a random word.\"\"\"\n    length = random.randint(word_min_len, word_max_len)\n    return ''.join(random.choices(string.ascii_lowercase, k=length))\n</code></pre>"},{"location":"api/wf/mapreduce/utils/#webs.wf.mapreduce.utils.generate_paragraph","title":"generate_paragraph()","text":"<pre><code>generate_paragraph(\n    word_count: int, word_min_len: int, word_max_len: int\n) -&gt; str\n</code></pre> <p>Generate a paragraph with the specified number of words.</p> Source code in <code>webs/wf/mapreduce/utils.py</code> <pre><code>def generate_paragraph(\n    word_count: int,\n    word_min_len: int,\n    word_max_len: int,\n) -&gt; str:\n    \"\"\"Generate a paragraph with the specified number of words.\"\"\"\n    words = [\n        generate_word(word_min_len, word_max_len) for _ in range(word_count)\n    ]\n    return ' '.join(words)\n</code></pre>"},{"location":"api/wf/mapreduce/utils/#webs.wf.mapreduce.utils.generate_paragraphs_for_map_tasks","title":"generate_paragraphs_for_map_tasks()","text":"<pre><code>generate_paragraphs_for_map_tasks(\n    task_count: int,\n    word_count: int,\n    word_min_len: int,\n    word_max_len: int,\n) -&gt; list[str]\n</code></pre> <p>Generate task_count paragraphs for the map tasks.</p> Source code in <code>webs/wf/mapreduce/utils.py</code> <pre><code>def generate_paragraphs_for_map_tasks(\n    task_count: int,\n    word_count: int,\n    word_min_len: int,\n    word_max_len: int,\n) -&gt; list[str]:\n    \"\"\"Generate task_count paragraphs for the map tasks.\"\"\"\n    paragraphs = [\n        generate_paragraph(word_count, word_min_len, word_max_len)\n        for _ in range(task_count)\n    ]\n    return paragraphs\n</code></pre>"},{"location":"api/wf/mapreduce/utils/#webs.wf.mapreduce.utils.generate_author_lists_for_map_tasks","title":"generate_author_lists_for_map_tasks()","text":"<pre><code>generate_author_lists_for_map_tasks(\n    task_count: int, mail_dir: str\n) -&gt; list[list[str]]\n</code></pre> <p>Generate task_count lists of email authors for the map tasks.</p> Source code in <code>webs/wf/mapreduce/utils.py</code> <pre><code>def generate_author_lists_for_map_tasks(\n    task_count: int,\n    mail_dir: str,\n) -&gt; list[list[str]]:\n    \"\"\"Generate task_count lists of email authors for the map tasks.\"\"\"\n    mail_dir = os.path.expanduser(mail_dir)\n\n    # Get list of all immediate subdirectories\n    author_dirs = [\n        name\n        for name in os.listdir(mail_dir)\n        if os.path.isdir(os.path.join(mail_dir, name))\n    ]\n\n    # Split the list of directories into task_count sublists\n    author_lists: list[list[str]] = [[] for _ in range(task_count)]\n    for i, author in enumerate(author_dirs):\n        author_lists[i % task_count].append(author)\n\n    return author_lists\n</code></pre>"},{"location":"api/wf/mapreduce/workflow/","title":"webs.wf.mapreduce.workflow","text":"<code>webs/wf/mapreduce/workflow.py</code>"},{"location":"api/wf/mapreduce/workflow/#webs.wf.mapreduce.workflow.MapreduceWorkflow","title":"MapreduceWorkflow","text":"<pre><code>MapreduceWorkflow(config: MapreduceWorkflowConfig)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Mapreduce workflow.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>MapreduceWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> Source code in <code>webs/wf/mapreduce/workflow.py</code> <pre><code>def __init__(self, config: MapreduceWorkflowConfig) -&gt; None:\n    self.config = config\n    super().__init__()\n</code></pre>"},{"location":"api/wf/mapreduce/workflow/#webs.wf.mapreduce.workflow.MapreduceWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: MapreduceWorkflowConfig) -&gt; Self\n</code></pre> <p>Initialize a workflow from a config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>MapreduceWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Workflow.</p> </li> </ul> Source code in <code>webs/wf/mapreduce/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: MapreduceWorkflowConfig) -&gt; Self:\n    \"\"\"Initialize a workflow from a config.\n\n    Args:\n        config: Workflow configuration.\n\n    Returns:\n        Workflow.\n    \"\"\"\n    return cls(config)\n</code></pre>"},{"location":"api/wf/mapreduce/workflow/#webs.wf.mapreduce.workflow.MapreduceWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the MapReduce workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Workflow task executor.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Run directory.</p> </li> </ul> Source code in <code>webs/wf/mapreduce/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:\n    \"\"\"Run the MapReduce workflow.\n\n    Args:\n        executor: Workflow task executor.\n        run_dir: Run directory.\n    \"\"\"\n    # Perform the map phase\n    logger.log(WORK_LOG_LEVEL, 'Starting map phase')\n    map_counters: list[Counter[str]] = []\n\n    if self.config.mode == 'enron':\n        author_lists = generate_author_lists_for_map_tasks(\n            self.config.map_task_count,\n            self.config.mail_dir,\n        )\n        map_task_inputs = zip(\n            [self.config.mail_dir] * self.config.map_task_count,\n            author_lists,\n        )\n        map_counters.extend(\n            executor.map(\n                _map_function_for_enron_run_mode,\n                map_task_inputs,\n            ),\n        )\n    else:\n        paragraphs = generate_paragraphs_for_map_tasks(\n            self.config.map_task_count,\n            self.config.word_count,\n            self.config.word_len_min,\n            self.config.word_len_max,\n        )\n\n        map_counters.extend(\n            executor.map(map_function_for_random_run_mode, paragraphs),\n        )\n\n    logger.log(\n        WORK_LOG_LEVEL,\n        'Map phase completed. Starting reduce phase',\n    )\n\n    # Perform the reduce phase\n    reduce_task = executor.submit(reduce_function, map_counters)\n\n    # Examine the reduce phase result\n    most_common_words = reduce_task.result().most_common(\n        self.config.n_freq,\n    )\n\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'{self.config.n_freq} most frequent words:',\n    )\n    for word, count in most_common_words:\n        logger.log(WORK_LOG_LEVEL, f'{word:10s}: {count}')\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'Total number of words {sum(reduce_task.result().values())}',\n    )\n    # Save the reduce phase result\n    output_file_path = os.path.join(run_dir, self.config.out)\n    with open(output_file_path, 'w') as f:\n        for word, count in most_common_words:\n            f.write(f'{word},{count}\\n')\n\n    logger.log(WORK_LOG_LEVEL, f'Results saved to: {output_file_path}')\n</code></pre>"},{"location":"api/wf/mapreduce/workflow/#webs.wf.mapreduce.workflow.map_function_for_random_run_mode","title":"map_function_for_random_run_mode()","text":"<pre><code>map_function_for_random_run_mode(\n    paragraph: str,\n) -&gt; Counter[str]\n</code></pre> <p>Map function to count words in a paragraph.</p> Source code in <code>webs/wf/mapreduce/workflow.py</code> <pre><code>def map_function_for_random_run_mode(paragraph: str) -&gt; Counter[str]:\n    \"\"\"Map function to count words in a paragraph.\"\"\"\n    word_counts = Counter(paragraph.split())\n    return word_counts\n</code></pre>"},{"location":"api/wf/mapreduce/workflow/#webs.wf.mapreduce.workflow.reduce_function","title":"reduce_function()","text":"<pre><code>reduce_function(\n    counts_list: list[Counter[str]],\n) -&gt; Counter[str]\n</code></pre> <p>Reduce function to combine word counts.</p> Source code in <code>webs/wf/mapreduce/workflow.py</code> <pre><code>def reduce_function(counts_list: list[Counter[str]]) -&gt; Counter[str]:\n    \"\"\"Reduce function to combine word counts.\"\"\"\n    total_counts: Counter[str] = Counter()\n    for counts in counts_list:\n        total_counts.update(counts)\n    return total_counts\n</code></pre>"},{"location":"api/wf/mapreduce/workflow/#webs.wf.mapreduce.workflow.map_function_for_enron_run_mode","title":"map_function_for_enron_run_mode()","text":"<pre><code>map_function_for_enron_run_mode(\n    mail_dir: str, authors: list[str]\n) -&gt; Counter[str]\n</code></pre> <p>Count words in all files under mail_dir/author for each author.</p> Source code in <code>webs/wf/mapreduce/workflow.py</code> <pre><code>def map_function_for_enron_run_mode(\n    mail_dir: str,\n    authors: list[str],\n) -&gt; Counter[str]:\n    \"\"\"Count words in all files under mail_dir/author for each author.\"\"\"\n    mail_dir = os.path.expanduser(mail_dir)\n    word_count: Counter[str] = Counter()\n\n    for author in authors:\n        author_dir = os.path.join(mail_dir, author)\n\n        # Walk through all files in the author's directory\n        for root, _, files in os.walk(author_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n\n                try:  # Count words in each file\n                    with open(file_path, errors='ignore') as f:\n                        for line in f:\n                            words = line.split()\n                            word_count.update(words)\n                except Exception as e:\n                    logger.log(\n                        WORK_LOG_LEVEL,\n                        f\"Error processing file '{file_path}': {e}\",\n                    )\n\n    return word_count\n</code></pre>"},{"location":"api/wf/moldesign/","title":"webs.wf.moldesign","text":"<code>webs/wf/moldesign/__init__.py</code> <p>Molecular Design Workflow.</p> <p>This workflow is based on the Molecular Design with Parsl.</p>"},{"location":"api/wf/moldesign/#webs.wf.moldesign--installation","title":"Installation","text":"<p>This workflow has certain dependencies which require Conda to install. To get started, create a Conda environment, install the Conda dependencies, and then install the <code>webs</code> package into the Conda environment.</p> <pre><code>conda create -p $PWD/env python=3.11\nconda activate ./env\nconda install -c conda-forge xtb-python==22.1\npip install -e .[moldesign]\n</code></pre>"},{"location":"api/wf/moldesign/#webs.wf.moldesign--data","title":"Data","text":"<p>The data needs to be downloaded first. <pre><code>curl -o data/QM9-search.tsv https://raw.githubusercontent.com/ExaWorks/molecular-design-parsl-demo/main/data/QM9-search.tsv\n</code></pre></p>"},{"location":"api/wf/moldesign/#webs.wf.moldesign--running","title":"Running","text":"<pre><code>python -m webs.run moldesign --dataset data/QM9-search.tsv --executor process-pool --max-processes 4\n</code></pre> <p>Additional parameters are available with <code>python -m webs.run moldesign --help</code>. It may be necessary to set <code>OMP_NUM_THREADS=1</code> with certain executors.</p>"},{"location":"api/wf/moldesign/chemfunctions/","title":"webs.wf.moldesign.chemfunctions","text":"<code>webs/wf/moldesign/chemfunctions.py</code> <p>Functions used to perform chemistry tasks in the Parsl workflow.</p> <p>While Parsl does offer the ability to run functions defined in a Jupyter notebook, we define them here to keep the notebook cleaner</p>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.MorganFingerprintTransformer","title":"MorganFingerprintTransformer","text":"<pre><code>MorganFingerprintTransformer(\n    length: int = 256, radius: int = 4\n)\n</code></pre> <p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Class that converts SMILES strings to fingerprint vectors.</p> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def __init__(self, length: int = 256, radius: int = 4) -&gt; None:\n    self.length = length\n    self.radius = radius\n</code></pre>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.MorganFingerprintTransformer.fit","title":"fit()","text":"<pre><code>fit(X: Any, y: Any = None) -&gt; Self\n</code></pre> <p>Fit the transformer.</p> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def fit(self, X: Any, y: Any = None) -&gt; Self:  # noqa: N803\n    \"\"\"Fit the transformer.\"\"\"\n    return self  # Do need to do anything\n</code></pre>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.MorganFingerprintTransformer.transform","title":"transform()","text":"<pre><code>transform(X: Any, y: Any = None) -&gt; Any\n</code></pre> <p>Compute the fingerprints.</p> <p>Parameters:</p> <ul> <li> <code>X</code>             (<code>Any</code>)         \u2013          <p>List of SMILES strings.</p> </li> <li> <code>y</code>             (<code>Any</code>, default:                 <code>None</code> )         \u2013          <p>Ignored.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>         \u2013          <p>Array of fingerprints.</p> </li> </ul> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def transform(self, X: Any, y: Any = None) -&gt; Any:  # noqa: N803\n    \"\"\"Compute the fingerprints.\n\n    Args:\n        X: List of SMILES strings.\n        y: Ignored.\n\n    Returns:\n        Array of fingerprints.\n    \"\"\"\n    my_func = partial(\n        compute_morgan_fingerprints,\n        fingerprint_length=self.length,\n        fingerprint_radius=self.radius,\n    )\n    with ProcessPoolExecutor(max_workers=n_workers) as pool:\n        fing = list(pool.map(my_func, X, chunksize=2048))\n    return np.vstack(fing)\n</code></pre>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.generate_initial_xyz","title":"generate_initial_xyz()","text":"<pre><code>generate_initial_xyz(mol_string: str) -&gt; str\n</code></pre> <p>Generate the XYZ coordinates for a molecule.</p> <p>Parameters:</p> <ul> <li> <code>mol_string</code>             (<code>str</code>)         \u2013          <p>SMILES string</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <ul> <li>InChI string for the molecule</li> </ul> </li> <li> <code>str</code>         \u2013          <ul> <li>XYZ coordinates for the molecule</li> </ul> </li> </ul> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def generate_initial_xyz(mol_string: str) -&gt; str:\n    \"\"\"Generate the XYZ coordinates for a molecule.\n\n    Args:\n        mol_string: SMILES string\n\n    Returns:\n        - InChI string for the molecule\n        - XYZ coordinates for the molecule\n    \"\"\"\n    # Generate 3D coordinates for the molecule\n    mol = Chem.MolFromSmiles(mol_string)\n    if mol is None:\n        raise ValueError(f'Parse failure for {mol_string}')\n    mol = Chem.AddHs(mol)\n    AllChem.EmbedMolecule(mol, randomSeed=1)\n    AllChem.MMFFOptimizeMolecule(mol)\n\n    # Save geometry as 3D coordinates\n    xyz = f'{mol.GetNumAtoms()}\\n'\n    xyz += mol_string + '\\n'\n    conf = mol.GetConformer()\n    for i, a in enumerate(mol.GetAtoms()):\n        s = a.GetSymbol()\n        c = conf.GetAtomPosition(i)\n        xyz += f'{s} {c[0]} {c[1]} {c[2]}\\n'\n\n    return xyz\n</code></pre>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.compute_morgan_fingerprints","title":"compute_morgan_fingerprints()","text":"<pre><code>compute_morgan_fingerprints(\n    smiles: str,\n    fingerprint_length: int,\n    fingerprint_radius: int,\n) -&gt; ndarray\n</code></pre> <p>Get Morgan Fingerprint of a specific SMILES string.</p> <p>Adapted from: https://github.com/google-research/google-research/blob/dfac4178ccf521e8d6eae45f7b0a33a6a5b691ee/mol_dqn/chemgraph/dqn/deep_q_networks.py#L750</p> <p>Parameters:</p> <ul> <li> <code>smiles</code>             (<code>str</code>)         \u2013          <p>The molecule as a SMILES string.</p> </li> <li> <code>fingerprint_length</code>             (<code>int</code>)         \u2013          <p>Bit-length of fingerprint.</p> </li> <li> <code>fingerprint_radius</code>             (<code>int</code>)         \u2013          <p>Radius used to compute fingerprint.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Array with shape <code>[hparams, fingerprint_length]</code> of the Morgan         fingerprint.</p> </li> </ul> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def compute_morgan_fingerprints(\n    smiles: str,\n    fingerprint_length: int,\n    fingerprint_radius: int,\n) -&gt; np.ndarray:\n    \"\"\"Get Morgan Fingerprint of a specific SMILES string.\n\n    Adapted from:\n    https://github.com/google-research/google-research/blob/dfac4178ccf521e8d6eae45f7b0a33a6a5b691ee/mol_dqn/chemgraph/dqn/deep_q_networks.py#L750\n\n    Args:\n        smiles: The molecule as a SMILES string.\n        fingerprint_length: Bit-length of fingerprint.\n        fingerprint_radius: Radius used to compute fingerprint.\n\n    Returns:\n        Array with shape `[hparams, fingerprint_length]` of the Morgan \\\n        fingerprint.\n    \"\"\"\n    # Parse the molecule\n    molecule = Chem.MolFromSmiles(smiles)\n\n    # Compute the fingerprint\n    fingerprint = AllChem.GetMorganFingerprintAsBitVect(\n        molecule,\n        fingerprint_radius,\n        fingerprint_length,\n    )\n    arr = np.zeros((1,), dtype=np.bool_)\n\n    # ConvertToNumpyArray takes ~ 0.19 ms, while\n    # np.asarray takes ~ 4.69 ms\n    DataStructs.ConvertToNumpyArray(fingerprint, arr)\n    return arr\n</code></pre>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.train_model","title":"train_model()","text":"<pre><code>train_model(\n    smiles: list[str], properties: list[float]\n) -&gt; Pipeline\n</code></pre> <p>Train a machine learning model using Morgan Fingerprints.</p> <p>Parameters:</p> <ul> <li> <code>smiles</code>             (<code>list[str]</code>)         \u2013          <p>SMILES strings for each molecule</p> </li> <li> <code>properties</code>             (<code>list[float]</code>)         \u2013          <p>List of a property for each molecule</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          <p>A trained model.</p> </li> </ul> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def train_model(smiles: list[str], properties: list[float]) -&gt; Pipeline:\n    \"\"\"Train a machine learning model using Morgan Fingerprints.\n\n    Args:\n        smiles: SMILES strings for each molecule\n        properties: List of a property for each molecule\n\n    Returns:\n        A trained model.\n    \"\"\"\n    model = Pipeline(\n        [\n            ('fingerprint', MorganFingerprintTransformer()),\n            (\n                'knn',\n                KNeighborsRegressor(\n                    n_neighbors=4,\n                    weights='distance',\n                    metric='jaccard',\n                    n_jobs=-1,\n                ),\n            ),\n            # n_jobs = -1 lets the model run all available processors\n        ],\n    )\n\n    return model.fit(smiles, properties)\n</code></pre>"},{"location":"api/wf/moldesign/chemfunctions/#webs.wf.moldesign.chemfunctions.run_model","title":"run_model()","text":"<pre><code>run_model(model: Any, smiles: list[str]) -&gt; DataFrame\n</code></pre> <p>Run a model on a list of smiles strings.</p> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>Any</code>)         \u2013          <p>Trained model that takes SMILES strings as inputs.</p> </li> <li> <code>smiles</code>             (<code>list[str]</code>)         \u2013          <p>List of molecules to evaluate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>A dataframe with the molecules and their predicted outputs.</p> </li> </ul> Source code in <code>webs/wf/moldesign/chemfunctions.py</code> <pre><code>def run_model(model: Any, smiles: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Run a model on a list of smiles strings.\n\n    Args:\n        model: Trained model that takes SMILES strings as inputs.\n        smiles: List of molecules to evaluate.\n\n    Returns:\n        A dataframe with the molecules and their predicted outputs.\n    \"\"\"\n    pred_y = model.predict(smiles)\n    return pd.DataFrame({'smiles': smiles, 'ie': pred_y})\n</code></pre>"},{"location":"api/wf/moldesign/config/","title":"webs.wf.moldesign.config","text":"<code>webs/wf/moldesign/config.py</code>"},{"location":"api/wf/moldesign/config/#webs.wf.moldesign.config.MoldesignWorkflowConfig","title":"MoldesignWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Moldesign workflow configuration.</p>"},{"location":"api/wf/moldesign/config/#webs.wf.moldesign.config.MoldesignWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/moldesign/tasks/","title":"webs.wf.moldesign.tasks","text":"<code>webs/wf/moldesign/tasks.py</code>"},{"location":"api/wf/moldesign/tasks/#webs.wf.moldesign.tasks.train_model","title":"train_model()","text":"<pre><code>train_model(train_data: DataFrame) -&gt; Pipeline\n</code></pre> <p>Train a machine learning model using Morgan Fingerprints.</p> <p>Parameters:</p> <ul> <li> <code>train_data</code>             (<code>DataFrame</code>)         \u2013          <p>Dataframe with a 'smiles' and 'ie' column that contains molecule structure and property, respectfully.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Pipeline</code>         \u2013          <p>A trained model.</p> </li> </ul> Source code in <code>webs/wf/moldesign/tasks.py</code> <pre><code>def train_model(train_data: pd.DataFrame) -&gt; Pipeline:\n    \"\"\"Train a machine learning model using Morgan Fingerprints.\n\n    Args:\n        train_data: Dataframe with a 'smiles' and 'ie' column\n            that contains molecule structure and property, respectfully.\n\n    Returns:\n        A trained model.\n    \"\"\"\n    # Imports for python functions run remotely must be defined inside the\n    # function\n    from sklearn.neighbors import KNeighborsRegressor\n    from sklearn.pipeline import Pipeline\n\n    from webs.wf.moldesign.chemfunctions import MorganFingerprintTransformer\n\n    model = Pipeline(\n        [\n            ('fingerprint', MorganFingerprintTransformer()),\n            # n_jobs = -1 lets the model run all available processors\n            (\n                'knn',\n                KNeighborsRegressor(\n                    n_neighbors=4,\n                    weights='distance',\n                    metric='jaccard',\n                    n_jobs=-1,\n                ),\n            ),\n        ],\n    )\n\n    # Ray arrays are immutable so need to clone.\n    return model.fit(train_data['smiles'].clone(), train_data['ie'].clone())\n</code></pre>"},{"location":"api/wf/moldesign/tasks/#webs.wf.moldesign.tasks.run_model","title":"run_model()","text":"<pre><code>run_model(model: Pipeline, smiles: list[str]) -&gt; DataFrame\n</code></pre> <p>Run a model on a list of smiles strings.</p> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>Pipeline</code>)         \u2013          <p>Trained model that takes SMILES strings as inputs.</p> </li> <li> <code>smiles</code>             (<code>list[str]</code>)         \u2013          <p>List of molecules to evaluate.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>A dataframe with the molecules and their predicted outputs.</p> </li> </ul> Source code in <code>webs/wf/moldesign/tasks.py</code> <pre><code>def run_model(model: Pipeline, smiles: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Run a model on a list of smiles strings.\n\n    Args:\n        model: Trained model that takes SMILES strings as inputs.\n        smiles: List of molecules to evaluate.\n\n    Returns:\n        A dataframe with the molecules and their predicted outputs.\n    \"\"\"\n    pred_y = model.predict(smiles)\n    return pd.DataFrame({'smiles': smiles, 'ie': pred_y})\n</code></pre>"},{"location":"api/wf/moldesign/tasks/#webs.wf.moldesign.tasks.combine_inferences","title":"combine_inferences()","text":"<pre><code>combine_inferences(*inputs: DataFrame) -&gt; DataFrame\n</code></pre> <p>Concatenate a series of inferences into a single DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>inputs</code>             (<code>DataFrame</code>, default:                 <code>()</code> )         \u2013          <p>A list of the component DataFrames.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>A single DataFrame containing the same inferences.</p> </li> </ul> Source code in <code>webs/wf/moldesign/tasks.py</code> <pre><code>def combine_inferences(*inputs: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Concatenate a series of inferences into a single DataFrame.\n\n    Args:\n        inputs: A list of the component DataFrames.\n\n    Returns:\n        A single DataFrame containing the same inferences.\n    \"\"\"\n    return pd.concat(inputs, ignore_index=True)\n</code></pre>"},{"location":"api/wf/moldesign/workflow/","title":"webs.wf.moldesign.workflow","text":"<code>webs/wf/moldesign/workflow.py</code>"},{"location":"api/wf/moldesign/workflow/#webs.wf.moldesign.workflow.MoldesignWorkflow","title":"MoldesignWorkflow","text":"<pre><code>MoldesignWorkflow(config: MoldesignWorkflowConfig)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Molecular design workflow.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>MoldesignWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> Source code in <code>webs/wf/moldesign/workflow.py</code> <pre><code>def __init__(self, config: MoldesignWorkflowConfig) -&gt; None:\n    self.config = config\n    super().__init__()\n</code></pre>"},{"location":"api/wf/moldesign/workflow/#webs.wf.moldesign.workflow.MoldesignWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: MoldesignWorkflowConfig) -&gt; Self\n</code></pre> <p>Initialize a workflow from a config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>MoldesignWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Workflow.</p> </li> </ul> Source code in <code>webs/wf/moldesign/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: MoldesignWorkflowConfig) -&gt; Self:\n    \"\"\"Initialize a workflow from a config.\n\n    Args:\n        config: Workflow configuration.\n\n    Returns:\n        Workflow.\n    \"\"\"\n    return cls(config)\n</code></pre>"},{"location":"api/wf/moldesign/workflow/#webs.wf.moldesign.workflow.MoldesignWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Workflow task executor.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Run directory.</p> </li> </ul> Source code in <code>webs/wf/moldesign/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:  # noqa: PLR0915\n    \"\"\"Run the workflow.\n\n    Args:\n        executor: Workflow task executor.\n        run_dir: Run directory.\n    \"\"\"\n    start_time = time.monotonic()\n\n    search_space = pd.read_csv(self.config.dataset, sep='\\s+')\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'Loaded search space (size={len(search_space):,})',\n    )\n\n    # Submit with some random guesses\n    train_data_list = []\n    init_mols = list(\n        search_space.sample(\n            self.config.initial_count,\n            random_state=self.config.seed,\n        )['smiles'],\n    )\n    sim_futures: dict[TaskFuture[float], str] = {\n        executor.submit(compute_vertical, mol): mol for mol in init_mols\n    }\n    logger.log(WORK_LOG_LEVEL, 'Submitted initial computations')\n    logger.info(f'Initial set: {init_mols}')\n    already_ran = set()\n\n    # Loop until you finish populating the initial set\n    while len(sim_futures) &gt; 0:\n        # First, get the next completed computation from the list\n        future: TaskFuture[float] = next(\n            as_completed(list(sim_futures.keys())),\n        )\n\n        # Remove it from the list of still-running task and get the input\n        smiles = sim_futures.pop(future)\n        already_ran.add(smiles)\n\n        # Check if the run completed successfully\n        if future.exception() is not None:\n            # If it failed, pick a new SMILES string at random and submit\n            smiles = search_space.sample(\n                1,\n                random_state=self.config.seed,\n            ).iloc[0]['smiles']\n            new_future = executor.submit(\n                compute_vertical,\n                smiles,\n            )\n            sim_futures[new_future] = smiles\n        else:\n            # If it succeeded, store the result\n            train_data_list.append(\n                {\n                    'smiles': smiles,\n                    'ie': future.result(),\n                    'batch': 0,\n                    'time': time.monotonic() - start_time,\n                },\n            )\n\n    logger.log(WORK_LOG_LEVEL, 'Done computing initial set')\n\n    # Create the initial training set as a\n    train_data = pd.DataFrame(train_data_list)\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'Created initial training set (size={len(train_data)})',\n    )\n\n    # Loop until complete\n    batch = 1\n    while len(train_data) &lt; self.config.search_count:\n        # Train and predict as show in the previous section.\n        train_future = executor.submit(train_model, train_data)\n        logger.log(WORK_LOG_LEVEL, 'Submitting inference tasks')\n        inference_futures = [\n            executor.submit(run_model, train_future, chunk)\n            for chunk in np.array_split(search_space['smiles'], 64)\n        ]\n        predictions = executor.submit(\n            combine_inferences,\n            *inference_futures,\n        ).result()\n        logger.log(\n            WORK_LOG_LEVEL,\n            f'Inference results received (size={len(predictions)})',\n        )\n\n        # Sort the predictions in descending order, and submit new\n        # molecules from them.\n        predictions.sort_values('ie', ascending=False, inplace=True)\n        sim_futures = {}\n        for smiles in predictions['smiles']:\n            if smiles not in already_ran:\n                new_future = executor.submit(compute_vertical, smiles)\n                sim_futures[new_future] = smiles\n                already_ran.add(smiles)\n                if len(sim_futures) &gt;= self.config.batch_size:\n                    break\n        logger.log(\n            WORK_LOG_LEVEL,\n            f'Submitted new computations (size={len(sim_futures)})',\n        )\n\n        # Wait for every task in the current batch to complete, and store\n        # successful results.\n        new_results = []\n        for future in as_completed(list(sim_futures.keys())):\n            if future.exception() is None:\n                new_results.append(\n                    {\n                        'smiles': sim_futures[future],\n                        'ie': future.result(),\n                        'batch': batch,\n                        'time': time.monotonic() - start_time,\n                    },\n                )\n\n        # Update the training data and repeat\n        batch += 1\n        train_data = pd.concat(\n            (train_data, pd.DataFrame(new_results)),\n            ignore_index=True,\n        )\n\n    fig, ax = plt.subplots(figsize=(4.5, 3.0))\n\n    ax.scatter(train_data['time'], train_data['ie'])\n    ax.step(train_data['time'], train_data['ie'].cummax(), 'k--')\n\n    ax.set_xlabel('Walltime (s)')\n    ax.set_ylabel('Ion. Energy (Ha)')\n\n    fig.tight_layout()\n\n    train_data.to_csv(run_dir / 'results.csv', index=False)\n</code></pre>"},{"location":"api/wf/montage/","title":"webs.wf.montage","text":"<code>webs/wf/montage/__init__.py</code> <p>Montage Mosaic workflow.</p> <p>This workflow is based on the Parsl implementation of the Montage Getting Started tutorial. The workflow takes a directory of inputs and uses Montage tools to create a mosaic of them.</p> <p>Montage binaries must be pre-installed prior to execution. Binaries are available on Homebrew (MacOS) or http://montage.ipac.caltech.edu/docs/download2.html</p> <p>Input data available here: http://montage.ipac.caltech.edu/docs/Kimages.tar</p> <p>Example execution command: <pre><code>python -m webs.run montage         --img-folder ${PWD}/Kimages         --img-tbl Kimages.tbl         --img-hdr Ktemplate.hdr         --output-dir Kprojdir         --executor process-pool         --max-processes 10\n</code></pre></p>"},{"location":"api/wf/montage/config/","title":"webs.wf.montage.config","text":"<code>webs/wf/montage/config.py</code>"},{"location":"api/wf/montage/config/#webs.wf.montage.config.MontageWorkflowConfig","title":"MontageWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Montage workflow configuration.</p>"},{"location":"api/wf/montage/config/#webs.wf.montage.config.MontageWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/montage/workflow/","title":"webs.wf.montage.workflow","text":"<code>webs/wf/montage/workflow.py</code>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.MontageWorkflow","title":"MontageWorkflow","text":"<pre><code>MontageWorkflow(\n    img_folder: str,\n    img_tbl: str,\n    img_hdr: str,\n    output_dir: str,\n)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Montage workflow.</p> <p>Parameters:</p> <ul> <li> <code>img_folder</code>             (<code>str</code>)         \u2013          <p>path to input image directory</p> </li> <li> <code>img_tbl</code>             (<code>str</code>)         \u2013          <p>name of the image table file</p> </li> <li> <code>img_hdr</code>             (<code>str</code>)         \u2013          <p>name of the image header file</p> </li> <li> <code>output_dir</code>             (<code>str</code>)         \u2013          <p>output directory path</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def __init__(\n    self,\n    img_folder: str,\n    img_tbl: str,\n    img_hdr: str,\n    output_dir: str,\n) -&gt; None:\n    self.img_folder = pathlib.Path(img_folder).absolute()\n    self.img_tbl = pathlib.Path(img_tbl).absolute()\n    self.img_hdr = pathlib.Path(img_hdr).absolute()\n    self.output_dir = pathlib.Path(output_dir).absolute()\n    super().__init__()\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.MontageWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: MontageWorkflowConfig) -&gt; Self\n</code></pre> <p>Initialize a workflow from a config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>MontageWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Workflow.</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: MontageWorkflowConfig) -&gt; Self:\n    \"\"\"Initialize a workflow from a config.\n\n    Args:\n        config: Workflow configuration.\n\n    Returns:\n        Workflow.\n    \"\"\"\n    return cls(\n        img_folder=config.img_folder,\n        img_tbl=config.img_tbl,\n        img_hdr=config.img_hdr,\n        output_dir=config.output_dir,\n    )\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.MontageWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Workflow task executor.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Run directory.</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:\n    \"\"\"Run the workflow.\n\n    Args:\n        executor: Workflow task executor.\n        run_dir: Run directory.\n    \"\"\"\n    output_dir_fut = executor.submit(\n        configure_montage,\n        self.img_folder,\n        self.img_tbl,\n        self.img_hdr,\n        self.output_dir,\n    )\n\n    mproject_outputs = []\n\n    for image in self.img_folder.glob('*.fits'):\n        input_image = self.img_folder / image\n        output_image_name = f'hdu0_{image.name}'\n\n        out = executor.submit(\n            mproject,\n            input_fn=input_image,\n            template=self.img_hdr,\n            output_dir=output_dir_fut,\n            output_name=output_image_name,\n        )\n        mproject_outputs.append(out)\n\n    wait(mproject_outputs)\n\n    img_tbl_fut = executor.submit(\n        mimgtbl,\n        img_dir=self.output_dir,\n        tbl_name='images.tbl',\n        output_dir=output_dir_fut,\n    )\n\n    diffs_data_fut = executor.submit(\n        moverlaps,\n        img_tbl=img_tbl_fut,\n        diffs_name='diffs.tbl',\n        output_dir=output_dir_fut,\n    )\n    diffs_dir, diffs_tbl = diffs_data_fut.result()\n\n    df = pd.read_csv(diffs_tbl, comment='#', sep='\\\\s+').drop(0)\n    images1 = list(df['|.1'])\n    images2 = list(df['cntr2'])\n    outputs = list(df['|.2'])\n    outputs_2 = []\n\n    for i in range(len(images1)):\n        image1 = self.output_dir / images1[i]\n        image2 = self.output_dir / images2[i]\n\n        out = executor.submit(\n            mdiff,\n            image_1=image1,\n            image_2=image2,\n            output_name=outputs[i],\n            template=self.img_hdr,\n            output_dir=output_dir_fut,\n        )\n        outputs_2.append(out)\n\n    wait(outputs_2)\n\n    fcorrdir = executor.submit(\n        bgexec_prep,\n        img_table=img_tbl_fut,\n        diffs_table=diffs_tbl,\n        diff_dir=diffs_dir,\n        output_dir=self.output_dir,\n    )\n\n    corrdir, corrtbl = fcorrdir.result()\n\n    corrections = pd.read_csv(\n        corrtbl,\n        comment='|',\n        sep='\\\\s+',\n    )\n    corrections.loc[90] = list(corrections.columns)\n    corrections.columns = ['id', 'a', 'b', 'c']\n\n    # for i in range(len(corrections)):\n    #    corrections.loc['id'][i] = int(corrections['id'][i])\n    corrections['id'] = corrections['id'].astype(int)\n\n    images_table = pd.read_csv(\n        img_tbl_fut.result(),\n        comment='|',\n        sep='\\\\s+',\n    )\n\n    bgexec_outputs = []\n\n    for i in range(len(images_table)):\n        input_image = list(images_table['fitshdr'])[i]\n        file_name = (list(images_table['fitshdr'])[i]).replace(\n            str(self.output_dir) + '/',\n            '',\n        )\n        output_image = corrdir / file_name\n        correction_values = list(\n            corrections.loc[corrections['id'] == i].values[0],\n        )\n\n        output_mb = executor.submit(\n            mbackground,\n            in_image=input_image,\n            out_image=output_image,\n            a=correction_values[1],\n            b=correction_values[2],\n            c=correction_values[3],\n        )\n\n        bgexec_outputs.append(output_mb)\n\n    wait(bgexec_outputs)\n\n    mosaic_out = self.output_dir / 'm17.fits'\n    mosaic_future = executor.submit(\n        madd,\n        img_tbl_fut,\n        self.img_hdr,\n        mosaic_out,\n        corrdir,\n    )\n\n    logger.info(f'Created output FITS file at {mosaic_future.result()}')\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.print_message","title":"print_message()","text":"<pre><code>print_message(message: str) -&gt; None\n</code></pre> <p>Print a message.</p> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def print_message(message: str) -&gt; None:\n    \"\"\"Print a message.\"\"\"\n    logger.log(WORK_LOG_LEVEL, message)\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.configure_montage","title":"configure_montage()","text":"<pre><code>configure_montage(\n    img_folder: Path,\n    img_tbl: Path,\n    img_hdr: Path,\n    output_dir: Path,\n) -&gt; Path\n</code></pre> <p>Montage Mosaic workflow setup.</p> <p>This function generates a header file bounding a collection of data specified by the input image dir.</p> <p>Parameters:</p> <ul> <li> <code>img_folder</code>             (<code>str</code>)         \u2013          <p>path to input image directory</p> </li> <li> <code>img_tbl</code>             (<code>str</code>)         \u2013          <p>name of the image table file</p> </li> <li> <code>img_hdr</code>             (<code>str</code>)         \u2013          <p>name of the image header file</p> </li> <li> <code>output_dir</code>             (<code>str</code>)         \u2013          <p>output directory path</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: the created output directory</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def configure_montage(\n    img_folder: pathlib.Path,\n    img_tbl: pathlib.Path,\n    img_hdr: pathlib.Path,\n    output_dir: pathlib.Path,\n) -&gt; pathlib.Path:\n    \"\"\"Montage Mosaic workflow setup.\n\n    This function generates a header file bounding a\n    collection of data specified by the input image dir.\n\n    Args:\n        img_folder (str): path to input image directory\n        img_tbl (str): name of the image table file\n        img_hdr (str): name of the image header file\n        output_dir (str): output directory path\n\n    Returns:\n        pathlib.Path: the created output directory\n    \"\"\"\n    import montage_wrapper as montage\n\n    imgtbl_log = montage.mImgtbl(str(img_folder), str(img_tbl))\n\n    logger.debug(imgtbl_log)\n\n    mkhdr_log = montage.mMakeHdr(str(img_tbl), str(img_hdr))\n    logger.debug(mkhdr_log)\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    return output_dir\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.mproject","title":"mproject()","text":"<pre><code>mproject(\n    input_fn: Path,\n    template: Path,\n    output_dir: Path,\n    output_name: Path,\n) -&gt; Path\n</code></pre> <p>Wrapper to Montage mProject function.</p> <p>The function reprojects a single image to the scale defined in a FITS header template file.</p> <p>Parameters:</p> <ul> <li> <code>input_fn</code>             (<code>Path</code>)         \u2013          <p>FITS file to reproject</p> </li> <li> <code>template</code>             (<code>Path</code>)         \u2013          <p>FITS header file used to define the desired output</p> </li> <li> <code>output_dir</code>             (<code>Path</code>)         \u2013          <p>output directory path</p> </li> <li> <code>output_name</code>             (<code>Path</code>)         \u2013          <p>output file basename</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: output directory</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def mproject(\n    input_fn: pathlib.Path,\n    template: pathlib.Path,\n    output_dir: pathlib.Path,\n    output_name: pathlib.Path,\n) -&gt; pathlib.Path:\n    \"\"\"Wrapper to Montage mProject function.\n\n    The function reprojects a single image to the scale defined\n    in a FITS header template file.\n\n    Args:\n        input_fn (pathlib.Path): FITS file to reproject\n        template (pathlib.Path): FITS header file used to define\n            the desired output\n        output_dir (pathlib.Path): output directory path\n        output_name (pathlib.Path): output file basename\n\n    Returns:\n        pathlib.Path: output directory\n    \"\"\"\n    import montage_wrapper as montage\n\n    output = output_dir / output_name\n    project_log = montage.mProject(str(input_fn), str(output), str(template))\n    logger.debug(project_log)\n\n    return output_dir  # helps establish dependencies\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.mimgtbl","title":"mimgtbl()","text":"<pre><code>mimgtbl(\n    img_dir: Path, tbl_name: str, output_dir: Path\n) -&gt; Path\n</code></pre> <p>Wrapper to Montage Imgtbl function.</p> <p>The function extracts the FITS header geometry information from a set of files and creates an ASCII image metadata table.</p> <p>Parameters:</p> <ul> <li> <code>img_dir</code>             (<code>Path</code>)         \u2013          <p>Input image directory</p> </li> <li> <code>tbl_name</code>             (<code>str</code>)         \u2013          <p>FITS table name</p> </li> <li> <code>output_dir</code>             (<code>Path</code>)         \u2013          <p>Output directory to save hdr to</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: created table file</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def mimgtbl(\n    img_dir: pathlib.Path,\n    tbl_name: str,\n    output_dir: pathlib.Path,\n) -&gt; pathlib.Path:\n    \"\"\"Wrapper to Montage Imgtbl function.\n\n    The function extracts the FITS header geometry information\n    from a set of files and creates an ASCII image metadata table.\n\n    Args:\n        img_dir (pathlib.Path): Input image directory\n        tbl_name (str): FITS table name\n        output_dir (pathlib.Path): Output directory to save hdr to\n\n    Returns:\n        pathlib.Path: created table file\n    \"\"\"\n    import montage_wrapper as montage\n\n    # It's the same path over and over so just grab the first occurrence\n    tbl_file = output_dir / tbl_name\n    imgtbl_log = montage.mImgtbl(str(img_dir), str(tbl_file))\n    logger.debug(imgtbl_log)\n\n    return tbl_file\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.moverlaps","title":"moverlaps()","text":"<pre><code>moverlaps(\n    img_tbl: Path, diffs_name: str, output_dir: Path\n) -&gt; tuple[Path, Path]\n</code></pre> <p>Wrapper to Montage Overlaps function.</p> <p>The function takes a list of of images and generates a list of overlaps.</p> <p>Parameters:</p> <ul> <li> <code>img_tbl</code>             (<code>Path</code>)         \u2013          <p>image metadata file</p> </li> <li> <code>diffs_name</code>             (<code>str</code>)         \u2013          <p>output table name of overlapping areas</p> </li> <li> <code>output_dir</code>             (<code>Path</code>)         \u2013          <p>output directory path</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Path, Path]</code>         \u2013          <p>tuple[pathlib.Path, pathlib.Path]: directory containing overlap images and difference table</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def moverlaps(\n    img_tbl: pathlib.Path,\n    diffs_name: str,\n    output_dir: pathlib.Path,\n) -&gt; tuple[pathlib.Path, pathlib.Path]:\n    \"\"\"Wrapper to Montage Overlaps function.\n\n    The function takes a list of of images and generates a list of overlaps.\n\n    Args:\n        img_tbl (pathlib.Path): image metadata file\n        diffs_name (str): output table name of overlapping areas\n        output_dir (pathlib.Path): output directory path\n\n    Returns:\n        tuple[pathlib.Path, pathlib.Path]: directory containing\n            overlap images and difference table\n    \"\"\"\n    import montage_wrapper as montage\n\n    diffs_tbl = output_dir / diffs_name\n    overlaps_log = montage.mOverlaps(str(img_tbl), str(diffs_tbl))\n\n    logger.debug(overlaps_log)\n    diff_dir = output_dir / 'diffdir'\n    diff_dir.mkdir(parents=True, exist_ok=True)\n    return diff_dir, diffs_tbl\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.mdiff","title":"mdiff()","text":"<pre><code>mdiff(\n    image_1: Path,\n    image_2: Path,\n    template: Path,\n    output_dir: Path,\n    output_name: str,\n) -&gt; Path\n</code></pre> <p>Wrapper to Montage diff function.</p> <p>The function subtracts one image from another (both in the same projection)</p> <p>Parameters:</p> <ul> <li> <code>image_1</code>             (<code>Path</code>)         \u2013          <p>first input file for differencing.</p> </li> <li> <code>image_2</code>             (<code>Path</code>)         \u2013          <p>second input file for differencing.</p> </li> <li> <code>template</code>             (<code>Path</code>)         \u2013          <p>FITS header file used to define the desired output.</p> </li> <li> <code>output_dir</code>             (<code>Path</code>)         \u2013          <p>Output directory name.</p> </li> <li> <code>output_name</code>             (<code>str</code>)         \u2013          <p>output difference file basename.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: output filepath</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def mdiff(\n    image_1: pathlib.Path,\n    image_2: pathlib.Path,\n    template: pathlib.Path,\n    output_dir: pathlib.Path,\n    output_name: str,\n) -&gt; pathlib.Path:\n    \"\"\"Wrapper to Montage diff function.\n\n    The function subtracts one image from another\n    (both in the same projection)\n\n    Args:\n        image_1 (pathlib.Path): first input file\n            for differencing.\n        image_2 (pathlib.Path): second input file\n            for differencing.\n        template (pathlib.Path): FITS header file used to\n            define the desired output.\n        output_dir (pathlib.Path): Output directory name.\n        output_name (str): output difference file basename.\n\n    Returns:\n        pathlib.Path: output filepath\n    \"\"\"\n    import montage_wrapper as montage\n\n    diff_dir = output_dir / 'diffdir'\n    diff_dir.mkdir(exist_ok=True, parents=True)\n    output_file = diff_dir / output_name\n    log_str = montage.mDiff(\n        str(image_1),\n        str(image_2),\n        str(output_file),\n        str(template),\n    )\n    logger.debug(log_str)\n\n    return output_file\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.bgexec_prep","title":"bgexec_prep()","text":"<pre><code>bgexec_prep(\n    img_table: Path,\n    diffs_table: Path,\n    diff_dir: Path,\n    output_dir: Path,\n) -&gt; tuple[Path, Path]\n</code></pre> <p>Prep to call Montage bg function.</p> <p>The function creates an image-to-image difference parameter table and then applies a set of corrections to achieve a best global fit.</p> <p>Parameters:</p> <ul> <li> <code>img_table</code>             (<code>Path</code>)         \u2013          <p>reprojected image metadata list</p> </li> <li> <code>diffs_table</code>             (<code>Path</code>)         \u2013          <p>table file list of input difference images</p> </li> <li> <code>diff_dir</code>             (<code>Path</code>)         \u2013          <p>directory for temporary difference files</p> </li> <li> <code>output_dir</code>             (<code>Path</code>)         \u2013          <p>output directory path</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Path, Path]</code>         \u2013          <p>tuple[pathlib.Path, pathlib.Path]: correction dir path, corrections table path</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def bgexec_prep(\n    img_table: pathlib.Path,\n    diffs_table: pathlib.Path,\n    diff_dir: pathlib.Path,\n    output_dir: pathlib.Path,\n) -&gt; tuple[pathlib.Path, pathlib.Path]:\n    \"\"\"Prep to call Montage bg function.\n\n    The function creates an image-to-image difference parameter table and\n    then applies a set of corrections to achieve a best global fit.\n\n    Args:\n        img_table (pathlib.Path): reprojected image metadata list\n        diffs_table (pathlib.Path): table file list of input difference images\n        diff_dir (pathlib.Path): directory for temporary difference files\n        output_dir (pathlib.Path): output directory path\n\n    Returns:\n        tuple[pathlib.Path, pathlib.Path]: correction dir path,\n            corrections table path\n    \"\"\"\n    import montage_wrapper as montage\n\n    fits_tbl = output_dir / 'fits.tbl'\n    corrections_tbl = output_dir / 'corrections.tbl'\n\n    fit_log = montage.mFitExec(\n        str(diffs_table),\n        str(fits_tbl),\n        str(diff_dir),\n    )\n    logger.debug(fit_log)\n\n    bg_log = montage.mBgModel(\n        str(img_table),\n        str(fits_tbl),\n        str(corrections_tbl),\n    )\n    logger.info(bg_log)\n\n    corrdir = output_dir / 'corrdir'\n    corrdir.mkdir(parents=True, exist_ok=True)\n\n    return corrdir, corrections_tbl\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.mbackground","title":"mbackground()","text":"<pre><code>mbackground(\n    in_image: Path,\n    out_image: Path,\n    a: float,\n    b: float,\n    c: float,\n) -&gt; Path\n</code></pre> <p>Wrapper to Montage Background function.</p> <p>Function subtracts a planar background from a FITS image.</p> <p>Parameters:</p> <ul> <li> <code>in_image</code>             (<code>Path</code>)         \u2013          <p>Input FITS file</p> </li> <li> <code>out_image</code>             (<code>Path</code>)         \u2013          <p>Output background-removed FITS file</p> </li> <li> <code>a</code>             (<code>float</code>)         \u2013          <p>A coefficient in (A*x + B*y + C) background level equation</p> </li> <li> <code>b</code>             (<code>float</code>)         \u2013          <p>B coefficient in (A*x + B*y + C) background level equation</p> </li> <li> <code>c</code>             (<code>float</code>)         \u2013          <p>C level in (A*x + B*y + C) background level equation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: output image path</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def mbackground(\n    in_image: pathlib.Path,\n    out_image: pathlib.Path,\n    a: float,\n    b: float,\n    c: float,\n) -&gt; pathlib.Path:\n    \"\"\"Wrapper to Montage Background function.\n\n    Function subtracts a planar background from a FITS image.\n\n    Args:\n        in_image (pathlib.Path): Input FITS file\n        out_image (pathlib.Path): Output background-removed FITS file\n        a (float): A coefficient in (A*x + B*y + C) background level equation\n        b (float): B coefficient in (A*x + B*y + C) background level equation\n        c (float): C level in (A*x + B*y + C) background level equation\n\n    Returns:\n        pathlib.Path: output image path\n    \"\"\"\n    import montage_wrapper as montage\n\n    background_log = montage.mBackground(\n        str(in_image),\n        str(out_image),\n        a,\n        b,\n        c,\n    )\n    logger.debug(background_log)\n\n    return out_image\n</code></pre>"},{"location":"api/wf/montage/workflow/#webs.wf.montage.workflow.madd","title":"madd()","text":"<pre><code>madd(\n    images_table: Path,\n    template_header: Path,\n    out_image: Path,\n    corr_dir: Path,\n) -&gt; Path\n</code></pre> <p>Coadd reprojected images to form a mosaic.</p> <p>Parameters:</p> <ul> <li> <code>images_table</code>             (<code>Path</code>)         \u2013          <p>table file containing metadata for images to be coadded.</p> </li> <li> <code>template_header</code>             (<code>Path</code>)         \u2013          <p>FITS header template to use in generation of output FITS.</p> </li> <li> <code>out_image</code>             (<code>Path</code>)         \u2013          <p>Output FITS image</p> </li> <li> <code>corr_dir</code>             (<code>Path</code>)         \u2013          <p>Directory containing reprojected image</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Path</code>         \u2013          <p>pathlib.Path: FITS output header file.</p> </li> </ul> Source code in <code>webs/wf/montage/workflow.py</code> <pre><code>def madd(\n    images_table: pathlib.Path,\n    template_header: pathlib.Path,\n    out_image: pathlib.Path,\n    corr_dir: pathlib.Path,\n) -&gt; pathlib.Path:\n    \"\"\"Coadd reprojected images to form a mosaic.\n\n    Args:\n        images_table (pathlib.Path): table file containing metadata\n            for images to be coadded.\n        template_header (pathlib.Path): FITS header template to use\n            in generation of output FITS.\n        out_image (pathlib.Path): Output FITS image\n        corr_dir (pathlib.Path): Directory containing reprojected image\n\n    Returns:\n        pathlib.Path: FITS output header file.\n    \"\"\"\n    import montage_wrapper as montage\n\n    add_log = montage.mAdd(\n        str(images_table),\n        str(template_header),\n        str(out_image),\n        str(corr_dir),\n    )\n    logger.debug(add_log)\n    return out_image\n</code></pre>"},{"location":"api/wf/synthetic/","title":"webs.wf.synthetic","text":"<code>webs/wf/synthetic/__init__.py</code> <p>Synthetic workflow.</p> <p>The synthetic workflow generates an arbitrary number of no-op sleep tasks that take and produce random data.</p> <p>The following arguments are supported:</p> <ul> <li><code>--structure</code></li> <li><code>--task-count</code></li> <li><code>--task-data-bytes</code></li> <li><code>--task-sleep</code></li> <li><code>--bag-max-running</code></li> </ul> <p>The workflow supports four workflow structures:</p> <ul> <li><code>bag</code>: Executes a \"bag-of-tasks\" where <code>task-count</code> tasks are executed. At   most <code>bag-max-running</code> will be running at any given time. This is useful   for testing scalability.</li> <li><code>diamond</code>: Executes a diamond workflow where the output of an initial task   is given to <code>task-count</code> intermediate tasks, executed in parallel, and   the outputs of the intermediate tasks are aggregated in a single, final   task.</li> <li><code>reduce</code>: Executes <code>task-count</code> independent tasks in parallel and a single   reduce task that takes the output of all of the independent tasks.</li> <li><code>sequential</code>: Executes a chain of <code>task-count</code> tasks where each subsequent   task depends on the output data of the prior. There is no parallelism, but   is useful for evaluating task and data overheads.</li> </ul>"},{"location":"api/wf/synthetic/#webs.wf.synthetic--running","title":"Running","text":"<p>Note that all arguments are required regardless of the <code>--structure</code>.</p> <pre><code>python -m webs.run synthetic --executor process-pool --max-processes 4 --structure bag --task-count 40 --task-data-bytes 10000 --task-sleep 1 --bag-max-running 4\n</code></pre>"},{"location":"api/wf/synthetic/config/","title":"webs.wf.synthetic.config","text":"<code>webs/wf/synthetic/config.py</code>"},{"location":"api/wf/synthetic/config/#webs.wf.synthetic.config.WorkflowStructure","title":"WorkflowStructure","text":"<p>             Bases: <code>Enum</code></p> <p>Workflow structure types.</p>"},{"location":"api/wf/synthetic/config/#webs.wf.synthetic.config.SyntheticWorkflowConfig","title":"SyntheticWorkflowConfig","text":"<p>             Bases: <code>Config</code></p> <p>Synthetic workflow configuration.</p>"},{"location":"api/wf/synthetic/config/#webs.wf.synthetic.config.SyntheticWorkflowConfig.add_argument_group","title":"add_argument_group()  <code>classmethod</code>","text":"<pre><code>add_argument_group(\n    parser: ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True\n) -&gt; None\n</code></pre> <p>Add model fields as arguments of an argument group on the parser.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>             (<code>ArgumentParser</code>)         \u2013          <p>Parser to add a new argument group to.</p> </li> <li> <code>argv</code>             (<code>Sequence[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional sequence of string arguments.</p> </li> <li> <code>required</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Mark arguments without defaults as required.</p> </li> </ul> Source code in <code>webs/config.py</code> <pre><code>@classmethod\ndef add_argument_group(\n    cls,\n    parser: argparse.ArgumentParser,\n    *,\n    argv: Sequence[str] | None = None,\n    required: bool = True,\n) -&gt; None:\n    \"\"\"Add model fields as arguments of an argument group on the parser.\n\n    Args:\n        parser: Parser to add a new argument group to.\n        argv: Optional sequence of string arguments.\n        required: Mark arguments without defaults as required.\n    \"\"\"\n    group = parser.add_argument_group(cls.__name__)\n    for field_name, field_info in cls.model_fields.items():\n        arg_name = field_name.replace('_', '-').lower()\n        group.add_argument(\n            f'--{arg_name}',\n            dest=field_name,\n            # type=field_info.annotation,\n            default=field_info.get_default(),\n            required=field_info.is_required() and required,\n            help=field_info.description,\n        )\n</code></pre>"},{"location":"api/wf/synthetic/utils/","title":"webs.wf.synthetic.utils","text":"<code>webs/wf/synthetic/utils.py</code>"},{"location":"api/wf/synthetic/utils/#webs.wf.synthetic.utils.randbytes","title":"randbytes()","text":"<pre><code>randbytes(size: int) -&gt; bytes\n</code></pre> <p>Get random byte string of specified size.</p> <p>Uses <code>random.randbytes()</code> in Python 3.9 or newer and <code>os.urandom()</code> in Python 3.8 and older.</p> <p>Parameters:</p> <ul> <li> <code>size</code>             (<code>int</code>)         \u2013          <p>size of byte string to return.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bytes</code>         \u2013          <p>random byte string.</p> </li> </ul> Source code in <code>webs/wf/synthetic/utils.py</code> <pre><code>def randbytes(size: int) -&gt; bytes:\n    \"\"\"Get random byte string of specified size.\n\n    Uses `random.randbytes()` in Python 3.9 or newer and\n    `os.urandom()` in Python 3.8 and older.\n\n    Args:\n        size (int): size of byte string to return.\n\n    Returns:\n        random byte string.\n    \"\"\"\n    max_bytes = int(1e9)\n    if sys.version_info &gt;= (3, 9) and size &lt; max_bytes:  # pragma: &gt;=3.9 cover\n        return random.randbytes(size)\n    else:  # pragma: &lt;3.9 cover\n        return os.urandom(size)\n</code></pre>"},{"location":"api/wf/synthetic/workflow/","title":"webs.wf.synthetic.workflow","text":"<code>webs/wf/synthetic/workflow.py</code>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.SyntheticWorkflow","title":"SyntheticWorkflow","text":"<pre><code>SyntheticWorkflow(\n    structure: WorkflowStructure,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n    bag_max_running: int,\n    *,\n    warmup_task: bool = True\n)\n</code></pre> <p>             Bases: <code>ContextManagerAddIn</code></p> <p>Synthetic workflow.</p> <p>Parameters:</p> <ul> <li> <code>structure</code>             (<code>WorkflowStructure</code>)         \u2013          <p>Workflow structure.</p> </li> <li> <code>task_count</code>             (<code>int</code>)         \u2013          <p>Number of tasks.</p> </li> <li> <code>task_data_bytes</code>             (<code>int</code>)         \u2013          <p>Size of random input and output data of tasks.</p> </li> <li> <code>task_sleep</code>             (<code>float</code>)         \u2013          <p>Seconds to sleep for in each task.</p> </li> <li> <code>bag_max_running</code>             (<code>int</code>)         \u2013          <p>Maximum concurrently executing tasks in the \"bag\" workflow.</p> </li> </ul> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def __init__(  # noqa: PLR0913\n    self,\n    structure: WorkflowStructure,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n    bag_max_running: int,\n    *,\n    warmup_task: bool = True,\n) -&gt; None:\n    self.structure = structure\n    self.task_count = task_count\n    self.task_data_bytes = task_data_bytes\n    self.task_sleep = task_sleep\n    self.bag_max_running = bag_max_running\n    self.warmup_task = warmup_task\n    super().__init__()\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.SyntheticWorkflow.from_config","title":"from_config()  <code>classmethod</code>","text":"<pre><code>from_config(config: SyntheticWorkflowConfig) -&gt; Self\n</code></pre> <p>Initialize a workflow from a config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>             (<code>SyntheticWorkflowConfig</code>)         \u2013          <p>Workflow configuration.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Self</code>         \u2013          <p>Workflow.</p> </li> </ul> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>@classmethod\ndef from_config(cls, config: SyntheticWorkflowConfig) -&gt; Self:\n    \"\"\"Initialize a workflow from a config.\n\n    Args:\n        config: Workflow configuration.\n\n    Returns:\n        Workflow.\n    \"\"\"\n    return cls(\n        structure=config.structure,\n        task_count=config.task_count,\n        task_data_bytes=config.task_data_bytes,\n        task_sleep=config.task_sleep,\n        bag_max_running=config.bag_max_running,\n        warmup_task=config.warmup_task,\n    )\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.SyntheticWorkflow.run","title":"run()","text":"<pre><code>run(executor: WorkflowExecutor, run_dir: Path) -&gt; None\n</code></pre> <p>Run the workflow.</p> <p>Parameters:</p> <ul> <li> <code>executor</code>             (<code>WorkflowExecutor</code>)         \u2013          <p>Workflow task executor.</p> </li> <li> <code>run_dir</code>             (<code>Path</code>)         \u2013          <p>Run directory.</p> </li> </ul> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:\n    \"\"\"Run the workflow.\n\n    Args:\n        executor: Workflow task executor.\n        run_dir: Run directory.\n    \"\"\"\n    if self.warmup_task:\n        logger.log(WORK_LOG_LEVEL, 'Submitting warmup task')\n        executor.submit(warmup_task).result()\n        logger.log(WORK_LOG_LEVEL, 'Warmup task completed')\n\n    logger.log(WORK_LOG_LEVEL, f'Starting {self.structure.value} workflow')\n    if self.structure == WorkflowStructure.BAG:\n        run_bag_of_tasks(\n            executor,\n            task_count=self.task_count,\n            task_data_bytes=self.task_data_bytes,\n            task_sleep=self.task_sleep,\n            max_running_tasks=self.bag_max_running,\n        )\n    elif self.structure == WorkflowStructure.DIAMOND:\n        run_diamond(\n            executor,\n            task_count=self.task_count,\n            task_data_bytes=self.task_data_bytes,\n            task_sleep=self.task_sleep,\n        )\n    elif self.structure == WorkflowStructure.REDUCE:\n        run_reduce(\n            executor,\n            task_count=self.task_count,\n            task_data_bytes=self.task_data_bytes,\n            task_sleep=self.task_sleep,\n        )\n    elif self.structure == WorkflowStructure.SEQUENTIAL:\n        run_sequential(\n            executor,\n            task_count=self.task_count,\n            task_data_bytes=self.task_data_bytes,\n            task_sleep=self.task_sleep,\n        )\n    else:\n        raise AssertionError(\n            f'Unhandled workflow structure type {self.structure}.',\n        )\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.noop_task","title":"noop_task()","text":"<pre><code>noop_task(\n    *data: bytes,\n    output_size: int,\n    sleep: float,\n    task_id: UUID | None = None\n) -&gt; bytes\n</code></pre> <p>No-op sleep task.</p> <p>Parameters:</p> <ul> <li> <code>data</code>             (<code>bytes</code>, default:                 <code>()</code> )         \u2013          <p>Input byte strings.</p> </li> <li> <code>output_size</code>             (<code>int</code>)         \u2013          <p>Size in bytes of output byte-string.</p> </li> <li> <code>sleep</code>             (<code>float</code>)         \u2013          <p>Minimum runtime of the task. Time required to generate the output data will be subtracted from this sleep time.</p> </li> <li> <code>task_id</code>             (<code>UUID | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional unique task ID to prevent executors from caching the task result.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>bytes</code>         \u2013          <p>Byte-string of length <code>output_size</code>.</p> </li> </ul> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def noop_task(\n    *data: bytes,\n    output_size: int,\n    sleep: float,\n    task_id: uuid.UUID | None = None,\n) -&gt; bytes:\n    \"\"\"No-op sleep task.\n\n    Args:\n        data: Input byte strings.\n        output_size: Size in bytes of output byte-string.\n        sleep: Minimum runtime of the task. Time required to generate the\n            output data will be subtracted from this sleep time.\n        task_id: Optional unique task ID to prevent executors from caching\n            the task result.\n\n    Returns:\n        Byte-string of length `output_size`.\n    \"\"\"\n    start = time.perf_counter_ns()\n    # Validate the data is real\n    assert all(len(d) &gt;= 0 for d in data)\n    result = randbytes(output_size)\n    elapsed = (time.perf_counter_ns() - start) / 1e9\n\n    # Remove elapsed time for generating result from remaining sleep time.\n    time.sleep(max(0, sleep - elapsed))\n    return result\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.warmup_task","title":"warmup_task()","text":"<pre><code>warmup_task() -&gt; None\n</code></pre> <p>No-op warmup task.</p> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def warmup_task() -&gt; None:\n    \"\"\"No-op warmup task.\"\"\"\n    pass\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.run_bag_of_tasks","title":"run_bag_of_tasks()","text":"<pre><code>run_bag_of_tasks(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n    max_running_tasks: int,\n) -&gt; None\n</code></pre> <p>Run bag of tasks workflow.</p> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def run_bag_of_tasks(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n    max_running_tasks: int,\n) -&gt; None:\n    \"\"\"Run bag of tasks workflow.\"\"\"\n    max_running_tasks = min(max_running_tasks, task_count)\n    start = time.monotonic()\n\n    running_tasks = [\n        executor.submit(\n            noop_task,\n            randbytes(task_data_bytes),\n            output_size=task_data_bytes,\n            sleep=task_sleep,\n            task_id=uuid.uuid4(),\n        )\n        for _ in range(max_running_tasks)\n    ]\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'Submitted {max_running_tasks} initial tasks',\n    )\n\n    completed_tasks = 0\n    submitted_tasks = len(running_tasks)\n\n    while submitted_tasks &lt; task_count:\n        finished_tasks, _ = wait(running_tasks, return_when='FIRST_COMPLETED')\n        for task in finished_tasks:\n            assert task.exception() is None\n            running_tasks.remove(task)\n            completed_tasks += 1\n\n        new_tasks = [\n            executor.submit(\n                noop_task,\n                randbytes(task_data_bytes),\n                output_size=task_data_bytes,\n                sleep=task_sleep,\n                task_id=uuid.uuid4(),\n            )\n            for _ in finished_tasks\n        ]\n        running_tasks.extend(new_tasks)\n        submitted_tasks += len(new_tasks)\n\n        if completed_tasks % max_running_tasks == 0:\n            rate = completed_tasks / (time.monotonic() - start)\n            logger.log(\n                WORK_LOG_LEVEL,\n                f'Completed {completed_tasks}/{task_count} tasks '\n                f'(rate: {rate:.2f} tasks/s, running tasks: '\n                f'{len(running_tasks)})',\n            )\n\n    wait(running_tasks, return_when='ALL_COMPLETED')\n    # Validate task results are real\n    assert all(len(task.result()) &gt;= 0 for task in running_tasks)\n    completed_tasks += len(running_tasks)\n    rate = completed_tasks / (time.monotonic() - start)\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'Completed {completed_tasks}/{task_count} (rate: {rate:.2f} tasks/s)',\n    )\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.run_diamond","title":"run_diamond()","text":"<pre><code>run_diamond(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n) -&gt; None\n</code></pre> <p>Run diamond workflow.</p> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def run_diamond(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n) -&gt; None:\n    \"\"\"Run diamond workflow.\"\"\"\n    initial_task = executor.submit(\n        noop_task,\n        randbytes(task_data_bytes),\n        output_size=task_data_bytes,\n        sleep=task_sleep,\n        task_id=uuid.uuid4(),\n    )\n    logger.log(WORK_LOG_LEVEL, 'Submitted initial task')\n\n    intermediate_tasks = [\n        executor.submit(\n            noop_task,\n            initial_task,\n            output_size=task_data_bytes,\n            sleep=task_sleep,\n            task_id=uuid.uuid4(),\n        )\n        for _ in range(task_count)\n    ]\n    logger.log(\n        WORK_LOG_LEVEL,\n        f'Submitting {task_count} intermediate tasks',\n    )\n\n    final_task = executor.submit(\n        noop_task,\n        *intermediate_tasks,\n        output_size=task_data_bytes,\n        sleep=task_sleep,\n        task_id=uuid.uuid4(),\n    )\n    logger.log(WORK_LOG_LEVEL, 'Submitted final task')\n\n    final_task.result()\n    logger.log(WORK_LOG_LEVEL, 'Final task completed')\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.run_reduce","title":"run_reduce()","text":"<pre><code>run_reduce(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n) -&gt; None\n</code></pre> <p>Run reduce worklow.</p> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def run_reduce(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n) -&gt; None:\n    \"\"\"Run reduce worklow.\"\"\"\n    map_tasks = [\n        executor.submit(\n            noop_task,\n            randbytes(task_data_bytes),\n            output_size=task_data_bytes,\n            sleep=task_sleep,\n            task_id=uuid.uuid4(),\n        )\n        for _ in range(task_count)\n    ]\n    logger.log(WORK_LOG_LEVEL, f'Submitted {task_count} initial tasks')\n\n    reduce_task = executor.submit(\n        noop_task,\n        *map_tasks,\n        output_size=task_data_bytes,\n        sleep=task_sleep,\n        task_id=uuid.uuid4(),\n    )\n    logger.log(WORK_LOG_LEVEL, 'Submitted reduce task')\n\n    reduce_task.result()\n    logger.log(WORK_LOG_LEVEL, 'Reduce task completed')\n</code></pre>"},{"location":"api/wf/synthetic/workflow/#webs.wf.synthetic.workflow.run_sequential","title":"run_sequential()","text":"<pre><code>run_sequential(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n) -&gt; None\n</code></pre> <p>Run sequential workflow.</p> Source code in <code>webs/wf/synthetic/workflow.py</code> <pre><code>def run_sequential(\n    executor: WorkflowExecutor,\n    task_count: int,\n    task_data_bytes: int,\n    task_sleep: float,\n) -&gt; None:\n    \"\"\"Run sequential workflow.\"\"\"\n    start = time.monotonic()\n    initial_data = randbytes(task_data_bytes)\n    tasks: list[TaskFuture[bytes]] = []\n\n    for i in range(task_count):\n        input_data = initial_data if i == 0 else tasks[-1]\n        task = executor.submit(\n            noop_task,\n            input_data,\n            output_size=task_data_bytes,\n            sleep=task_sleep,\n            task_id=uuid.uuid4(),\n        )\n        tasks.append(task)\n        logger.log(\n            WORK_LOG_LEVEL,\n            f'Submitted task {i+1}/{task_count} '\n            f'(task_id={task.info.task_id})',\n        )\n\n    for i, task in enumerate(as_completed(tasks)):\n        assert task.done()\n        logger.log(\n            WORK_LOG_LEVEL,\n            f'Received task {i+1}/{task_count} (task_id: {task.info.task_id})',\n        )\n\n    # Validate the final result in the sequence\n    assert len(tasks[-1].result()) &gt;= 0\n\n    rate = task_count / (time.monotonic() - start)\n    logger.log(WORK_LOG_LEVEL, f'Task completion rate: {rate:.3f} tasks/s')\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#getting-started-for-local-development","title":"Getting Started for Local Development","text":"<p>We recommend using Tox to setup the development environment. This will create a new virtual environment with all of the required packages installed and WEBS installed in editable mode with the necessary extras options.</p> <pre><code>$ git clone https://github.com/proxystore/taps\n$ cd foobar\n$ tox --devenv venv -e py311\n$ . venv/bin/activate\n</code></pre> <p>Warning</p> <p>Running Tox in a Conda environment is possible but it may conflict with Tox's ability to find the correct Python versions. E.g., if your Conda environment is Python 3.11, running <code>$ tox -e p310</code> may still use Python 3.11.</p> <p>To install manually: <pre><code>$ git clone https://github.com/proxystore/taps\n$ cd taps\n$ python -m venv venv\n$ . venv/bin/activate\n$ pip install -e .[dev,docs]\n</code></pre></p>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":"<p>FooBar uses pre-commit and Tox for continuous integration (test, linting, etc.).</p>"},{"location":"contributing/#linting-and-type-checking-pre-commit","title":"Linting and Type Checking (pre-commit)","text":"<p>To use pre-commit, install the hook and then run against files.</p> <pre><code>$ pre-commit install\n$ pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#tests-tox","title":"Tests (tox)","text":"<p>The entire CI workflow can be run with <code>$ tox</code>. This will test against multiple versions of Python and can be slow.</p> <p>Module-level unit-test are located in the <code>tests/</code> directory and its structure is intended to match that of <code>foobar/</code>. E.g. the tests for <code>webs/x/y.py</code> are located in <code>tests/x/y_test.py</code>; however, additional test files can be added as needed. Tests should be narrowly focused and target a single aspect of the code's functionality, tests should not test internal implementation details of the code, and tests should not be dependent on the order in which they are run.</p> <p>Code that is useful for building tests but is not a test itself belongs in the <code>testing/</code> directory.</p> <pre><code># Run all tests in tests/\n$ tox -e py39\n# Run a specific test\n$ tox -e py39 -- tests/x/y_test.py::test_z\n</code></pre>"},{"location":"contributing/#docs","title":"Docs","text":"<p>If code changes require an update to the documentation (e.g., for function signature changes, new modules, etc.), the documentation can be built using MKDocs.</p> <pre><code># Manually\n$ pip install -e .[docs]\n$ mkdocs build --strict  # Build only to site/index.html\n$ mkdocs serve           # Serve locally\n\n# With tox (will only build, does not serve)\n$ tox -e docs\n</code></pre> <p>Docstrings are automatically generated, but it is recommended to check the generated docstrings to make sure details/links/etc. are correct.</p>"},{"location":"contributing/issues-pull-requests/","title":"Issues and Pull Requests","text":""},{"location":"contributing/issues-pull-requests/#issues","title":"Issues","text":"<p>Issue Tracker</p> <p>We use GitHub issues to report problems, request and track changes, and discuss future ideas. If you open an issue for a specific problem, please follow the template guides.</p>"},{"location":"contributing/issues-pull-requests/#pull-requests","title":"Pull Requests","text":"<p>We use the standard GitHub contribution cycle where all contributions are made via pull requests (including code owners!).</p> <ol> <li>Fork the repository and clone to your local machine.</li> <li> <p>Create local changes.</p> <ul> <li>Changes should conform to the style and testing guidelines, referenced   above.</li> <li>Preferred commit message format (source):<ul> <li>separate subject from body with a blank line,</li> <li>limit subject line to 50 characters,</li> <li>capitalize first word of subject line,</li> <li>do not end the subject line with a period,</li> <li>use the imperative mood for subject lines,</li> <li>include related issue numbers at end of subject line,</li> <li>wrap body at 72 characters, and</li> <li>use the body to explain what/why rather than how.</li> <li>Example: <code>Fix concurrency bug in Store (#42)</code></li> </ul> </li> </ul> </li> <li> <p>Push commits to your fork.</p> <ul> <li>Please squash commits fixing mistakes to keep the git history clean.   For example, if commit \"b\" follows commit \"a\" and only fixes a small typo   from \"a\", please squash \"a\" and \"b\" into a single, correct commit.   This keeps the commit history readable and easier to search through when   debugging (e.g., git blame/bisect).</li> </ul> </li> <li>Open a pull request in this repository.<ul> <li>The pull request should include a description of the motivation for the   PR and included changes. A PR template is provided to guide this process.</li> </ul> </li> </ol>"},{"location":"contributing/releases/","title":"Releases","text":""},{"location":"contributing/releases/#release-timeline","title":"Release Timeline","text":"<p>Releases are created on an as-needed basis. Milestones are the Issue Tracker are used to track features to be included in upcoming releases.</p>"},{"location":"contributing/releases/#creating-releases","title":"Creating Releases","text":"<ol> <li>Choose the next version number, referred to as <code>{VERSION}</code> for the    rest of the instructions. Versioning follows semver    (<code>major.minor.patch</code>) with optional PEP-440    pre-release/post-release/dev-release segments. Major/minor/patch numbers    start at 0 and pre-release/post-release/dev-release segments start at 1.</li> <li>Update the version in <code>pyproject.toml</code> to <code>{VERSION}</code>.</li> <li>Commit and merge the version updates/changelogs into main.</li> <li>Tag the release commit and push (typically this is the commit updating the    version numbers).    <pre><code>$ git tag -s v{VERSION} -m \"FooBar v{VERSION}\"\n$ git push origin v{VERSION}\n</code></pre>    Note the version number is prepended by \"v\" for the tags so we can    distinguish release tags from non-release tags.</li> <li>Create a new release on GitHub using the tag. The title should be    <code>FooBar v{VERSION}</code>.</li> <li>Official release:<ol> <li>Use the \"Generate release notes\" option and set the previous tag as the previous official release tag. E.g., for <code>v0.4.1</code>, the previous release tag should be <code>v0.4.0</code> and NOT <code>v0.4.1a1</code>.</li> <li>Add an \"Upgrade Steps\" section at the top (see previous releases for examples).</li> <li>Review the generated notes and edit as needed. PRs are organized by tag, but some PRs will be missing tags and need to be moved from the \"Other Changes\" section to the correct section.</li> <li>Select \"Set as the latest release.\"</li> </ol> </li> <li>Unofficial release: (alpha/dev builds)<ol> <li>Do NOT generate release notes. The body can be along the lines of \"Development pre-prelease for <code>V{VERSION}</code>.\"</li> <li>Leave the previous tag as \"auto.\"</li> <li>Select \"Set as a pre-release.\"</li> </ol> </li> </ol>"},{"location":"contributing/style-guide/","title":"Style Guide","text":"<p>The Python code and docstring format mostly follows Google's Python Style Guide, but the pre-commit config is the authoritative source for code format compliance.</p> <p>Nits:</p> <ul> <li>Avoid imports in <code>__init__.py</code> (reduces the likelihood of circular imports).</li> <li>Prefer pure functions where possible.</li> <li>Define all class attributes inside <code>__init__</code> so all attributes are visible   in one place. Attributes that are defined later can be set as <code>None</code>   as a placeholder.</li> <li>Prefer f-strings (<code>f'name: {name}</code>) over string format   (<code>'name: {}'.format(name)</code>). Never use the <code>%</code> operator.</li> <li>Prefer typing.NamedTuple over collections.namedtuple.</li> <li>Use lower-case and no punctuation for log messages, but use upper-case and   punctuation for exception values.   <pre><code>logger.info(f'new connection opened to {address}')\nraise ValueError('Name must contain alphanumeric characters only.')\n</code></pre></li> <li>Document all exceptions that may be raised by a function in the docstring.</li> </ul>"},{"location":"guides/","title":"Guides","text":"<ul> <li>Creating Workflows</li> </ul>"},{"location":"guides/creating-workflows/","title":"Creating Workflows","text":"<p>This guide describes creating a workflow within the WEBS framework.</p>"},{"location":"guides/creating-workflows/#installation","title":"Installation","text":"<p>A development environment needs to be configured first. Fork the repository and clone your fork locally. Then, configure a virtual environment with the WEBS package and development dependencies.</p> <pre><code>python -m venv venv\n. venv/bin/activate\npip install -e .[dev,docs]\n</code></pre> <p>See Getting Started for Local Development for detailed instructions on running the linters and continuous integration tests.</p>"},{"location":"guides/creating-workflows/#workflow-structure","title":"Workflow Structure","text":"<p>Our example workflow is going to be called <code>foobar</code>. All workflows in WEBS are composed of two required components: a <code>Config</code> and a <code>Workflow</code>. The <code>Config</code> is a Pydantic <code>BaseModel</code> with some extra functionality for constructing a config instance from command line arguments. The <code>Workflow</code> is a protocol with two key methods to implement: <code>from_config()</code> and <code>run()</code>.</p> <p>All workflows are submodules of <code>webs/wf/</code>. In this example, we will create the <code>webs/wf/foobar</code> directory containing the following files.</p> <pre><code>webs/\n\u251c\u2500 wf/\n\u2502  \u251c\u2500 __init__.py\n\u2502  \u251c\u2500 foobar/\n\u2502  \u2502  \u251c\u2500 __init__.py\n\u2502  \u2502  \u251c\u2500 config.py\n\u2502  \u2502  \u251c\u2500 workflow.py\n</code></pre> <p>The first file, <code>webs/wf/foobar/config.py</code>, will contain the configuration model for the workflow. This configuration should contain all of the parameters that the user needs to provide or that can be adjusted for the workflow. In our case, the <code>foobar</code> workflow is simply going to print a user defined message <code>n</code> number of times. webs/wf/foobar/config.py<pre><code>from __future__ import annotations\n\nfrom pydantic import Field\n\nfrom webs.config import Config\n\n\nclass FoobarWorkflowConfig(Config):\n    \"\"\"Foobar workflow configuration.\"\"\"\n\n    message: str = Field(description='message to print')\n    repeat: int = Field(1, description='number of times to repeat message')\n</code></pre> The <code>Config</code> class supports required arguments without default values (e.g., <code>message</code>) and optional arguments with default values (e.g., <code>repeat</code>).</p> <p>The second file, <code>webs/wf/foobar/workflow.py</code>, will contain the core workflow logic. Task functions and workflow code can be included here or in another module within <code>webs/wf/foobar</code>. For example, this trivial example workflow will be entirely contained within <code>webs/wf/foobar/workflow.py</code> but more complex workflows may want to split up the code across many modules. Nonetheless, the entry point for the workflow will be in the <code>run()</code> method inside of <code>webs/wf/foorbar/workflow.py</code>. We'll first list the code, then discuss the important sections. webs/wf/foobar/workflow.py<pre><code>from __future__ import annotations\n\nimport logging\nimport pathlib\nimport sys\n\nif sys.version_info &gt;= (3, 11):  # pragma: &gt;=3.11 cover\n    from typing import Self\nelse:  # pragma: &lt;3.11 cover\n    from typing_extensions import Self\n\nfrom webs.context import ContextManagerAddIn\nfrom webs.executor.workflow import WorkflowExecutor\nfrom webs.logging import WORK_LOG_LEVEL\nfrom webs.wf.foobar.config import FoobarWorkflowConfig\nfrom webs.workflow import register\n\nlogger = logging.getLogger(__name__)\n\n\ndef print_message(message: str) -&gt; None:\n    \"\"\"Print a message.\"\"\"\n    logger.log(WORK_LOG_LEVEL, message)\n\n\n@register()\nclass FoobarWorkflow(ContextManagerAddIn):\n    \"\"\"Foobar workflow.\n\n    Args:\n        message: Message to print.\n        repeat: Number of times to repeat the message.\n    \"\"\"\n\n    name = 'foobar'\n    config_type = FoobarWorkflowConfig\n\n    def __init__(self, message: str, repeat: int = 1) -&gt; None:\n        self.message = message\n        self.repeat = repeat\n        super().__init__()\n\n    @classmethod\n    def from_config(cls, config: FoobarWorkflowConfig) -&gt; Self:\n        \"\"\"Initialize a workflow from a config.\n\n        Args:\n            config: Workflow configuration.\n\n        Returns:\n            Workflow.\n        \"\"\"\n        return cls(message=config.message, repeat=config.repeat)\n\n    def run(self, executor: WorkflowExecutor, run_dir: pathlib.Path) -&gt; None:\n        \"\"\"Run the workflow.\n\n        Args:\n            executor: Workflow task executor.\n            run_dir: Run directory.\n        \"\"\"\n        for _ in range(self.repeat):\n            task = executor.submit(print_message, self.message)\n            task.result()  # Wait on task to finish\n</code></pre></p> <ol> <li>Workflows in WEBS are composed on tasks which are just Python functions.    Here, our task is the <code>print_message</code> function.</li> <li>The <code>FoobarWorkflow</code> implements the <code>Workflow</code> protocol.    Importantly, <code>FoobarWorkflow</code> is decorated by the <code>@register()</code> decorator which tells WEBS to add this workflow to the command line interface (CLI).    The <code>@register()</code> decorator uses the <code>name = 'foobar'</code> attribute of <code>FoobarWorkflow</code> as the name used in the CLI to select this workflow.    The <code>config_type = FoobarWorkflowConfig</code> attribute tells the CLI that the <code>FoobarWorkflowConfig</code> we defined in <code>webs/wf/foobar/config.py</code> is the configuration to create command line arguments from.</li> <li>When the CLI is invoked for this workflow, the arguments will be parsed into a <code>FoobarWorkflowConfig</code> and then <code>FoobarWorkflow.from_config()</code> will be used to instantiate the workflow runner.</li> <li>Once <code>FoobarWorkflow</code> is instantiated from the config, <code>FoobarWorkflow.run()</code> will be invoked.    This method takes two arguments: a <code>WorkflowExecutor</code> and a path to the invocations run directory.    Workflows are free to use the run directory as needed, such as to store result files.</li> </ol>"},{"location":"guides/creating-workflows/#workflow-executor","title":"Workflow Executor","text":"<p>The <code>WorkflowExecutor</code> is the key abstraction of the WEBS framework. The CLI arguments provided by the user for the compute engine, data management, and task logging logic are used to create a <code>WorkflowExecutor</code> instance which is then provided to the workflow. <code>WorkflowExecutor.submit()</code> is the primary method that workflows will use to execute tasks asynchronously. This method returns a <code>TaskFuture</code> object with a <code>result()</code> which will wait on the task to finish and return the result. Alternatively, <code>WorkflowExecutor.map()</code> can be used to map a task onto a sequence of inputs, compute the tasks in parallel, and gather the results. Importantly, a <code>TaskFuture</code> can also be passed as input to another tasks. Doing so indicates to the <code>WorkflowExecutor</code> that there is a dependency between those two tasks.</p>"},{"location":"guides/creating-workflows/#registering-a-workflow","title":"Registering a Workflow","text":"<p>To make WEBS aware of the workflow, we need to modify <code>REGISTERED_WORKFLOWS</code> in <code>webs/workflows.py</code>. <code>REGISTERED_WORKFLOWS</code> is a dictionary mapping the workflow name (e.g., <code>'foobar'</code>) to it's <code>Workflow</code> implementation. For this workflow, this would look like: <pre><code>REGISTERED_WORKFLOWS = {\n    ...\n    'foobar': 'webs.wf.foobar.workflow.FoobarWorkflow',\n}\n</code></pre> If this step is skipped, the workflow will not be accessible from the CLI.</p>"},{"location":"guides/creating-workflows/#running-a-workflow","title":"Running a Workflow","text":"<p>Once a workflow is created and registered within WEBS, the workflow is available within the CLI. <pre><code>python -m webs.run foobar --help\n</code></pre> Using <code>foobar</code> as the first positional argument indicates we want to execute the <code>foobar</code> workflow, and <code>--help</code> will print all of the required and optional arguments of the workflow. The arguments will be separated into sections, such as for arguments specific to the <code>foobar</code> workflow or for executor arguments.</p> <p>The following command will execute the workflow to print \"Hello, World!\" three times. We specify the <code>thread-pool</code> executor because this will allow our printing to show up in the main process. <pre><code>$ python -m webs.run foobar --message 'Hello, World!' --repeat 3 --executor thread-pool\nRUN   (webs.run) :: Starting workflow (name=foobar)\n...\nWORK  (webs.wf.foobar.workflow) :: Hello, World!\nWORK  (webs.wf.foobar.workflow) :: Hello, World!\nWORK  (webs.wf.foobar.workflow) :: Hello, World!\nRUN   (webs.run) :: Finished workflow (name=foobar, runtime=0.00s)\n</code></pre></p>"}]}